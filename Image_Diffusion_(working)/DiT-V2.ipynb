{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vkxG8UA55CDs",
        "boKbD65w2odj",
        "XYdPvNIg2q6C",
        "aH_B8CT82uLR",
        "YuPSPusZ2v3N",
        "5C5kiQ_k2x7F",
        "udhjg0J-9pK0",
        "gV_U-JJrL-jC",
        "7wfr3bbCSZkk"
      ],
      "authorship_tag": "ABX9TyP4Rft2EL7JdziXn6akadNo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durml91/MMath-Project/blob/duo-branch/Image_Diffusion_(working)/DiT-V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**DiT implementation**"
      ],
      "metadata": {
        "id": "YJW-hYHrRghA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports and Installs"
      ],
      "metadata": {
        "id": "CwbREoSGRkg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to install new version of Jax for package compatibility"
      ],
      "metadata": {
        "id": "9JBPitYdRoFm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB7c1s9yAoVp"
      },
      "outputs": [],
      "source": [
        "!pip install jaxlib==0.4.2+cuda11.cudnn82 -f  https://storage.googleapis.com/jax-releases/jax_cuda_releases.html # [cuda]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install packages"
      ],
      "metadata": {
        "id": "53kc_s37RsQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffrax\n",
        "!pip install equinox\n",
        "!pip install einops\n",
        "!pip install optax"
      ],
      "metadata": {
        "id": "ZAhw7yT6AvfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "DGZx1OUJRuUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import array\n",
        "import functools as ft\n",
        "import gzip\n",
        "import os\n",
        "import struct\n",
        "import urllib.request\n",
        "\n",
        "import diffrax as dfx  # https://github.com/patrick-kidger/diffrax\n",
        "import einops  # https://github.com/arogozhnikov/einops\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr\n",
        "import matplotlib.pyplot as plt\n",
        "import optax  # https://github.com/deepmind/optax\n",
        "\n",
        "import equinox as eqx"
      ],
      "metadata": {
        "id": "E1AYxrBfAxC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate rng key**"
      ],
      "metadata": {
        "id": "1Bw9Y9YTRwKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key = jr.PRNGKey(2023)"
      ],
      "metadata": {
        "id": "7CDF6Fp0CL4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup for saving model parameters - only run once!"
      ],
      "metadata": {
        "id": "o-FpEm9rR4Iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFbT8W33BUPS",
        "outputId": "06ac6304-c303-40cb-d7ff-72228010875e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data loader and data shuffler"
      ],
      "metadata": {
        "id": "RiTiRLb7XjXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mnist():\n",
        "    filename = \"train-images-idx3-ubyte.gz\"\n",
        "    url_dir = \"https://storage.googleapis.com/cvdf-datasets/mnist\"\n",
        "    target_dir = os.getcwd() + \"/data/mnist\"\n",
        "    url = f\"{url_dir}/{filename}\"\n",
        "    target = f\"{target_dir}/{filename}\"\n",
        "\n",
        "    if not os.path.exists(target):\n",
        "        os.makedirs(target_dir, exist_ok=True)\n",
        "        urllib.request.urlretrieve(url, target)\n",
        "        print(f\"Downloaded {url} to {target}\")\n",
        "\n",
        "    with gzip.open(target, \"rb\") as fh:\n",
        "        _, batch, rows, cols = struct.unpack(\">IIII\", fh.read(16))\n",
        "        shape = (batch, 1, rows, cols)\n",
        "        return jnp.array(array.array(\"B\", fh.read()), dtype=jnp.uint8).reshape(shape)\n",
        "\n",
        "\n",
        "def dataloader(data, batch_size, *, key):\n",
        "    dataset_size = data.shape[0]\n",
        "    indices = jnp.arange(dataset_size)\n",
        "    while True:\n",
        "        perm = jr.permutation(key, indices)\n",
        "        (key,) = jr.split(key, 1)\n",
        "        start = 0\n",
        "        end = batch_size\n",
        "        while end < dataset_size:\n",
        "            batch_perm = perm[start:end]\n",
        "            yield data[batch_perm]\n",
        "            start = end\n",
        "            end = start + batch_size"
      ],
      "metadata": {
        "id": "Rgx0ctSJXlT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DiT Model"
      ],
      "metadata": {
        "id": "MxSYycHl2mpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utility functions**"
      ],
      "metadata": {
        "id": "toAYtcuUWfh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@eqx.filter_jit\n",
        "def modulate(x, shift, scale):\n",
        "    scale = jnp.expand_dims(scale, axis=0) \n",
        "    shift = jnp.expand_dims(shift, axis=0)  \n",
        "    c = (1 + scale) + shift\n",
        "    return x * c"
      ],
      "metadata": {
        "id": "UrDibia_WiyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@eqx.filter_jit\n",
        "def get_2d_sincos_pos_embed(n_embd, grid_size):\n",
        "\n",
        "    grid_h = jnp.arange(grid_size, dtype=float)\n",
        "    grid_w = jnp.arange(grid_size, dtype=float)\n",
        "    grid = jnp.meshgrid(grid_w, grid_h)\n",
        "    grid = jnp.stack(grid, axis=0)\n",
        "\n",
        "    #grid = einops.rearrange(grid, \" 2 d f -> 2 1 d f\")\n",
        "    grid = jnp.reshape(grid, (2, 1, grid_size, grid_size))\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(n_embd, grid)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "##################################################################################\n",
        "\n",
        "@eqx.filter_jit\n",
        "def get_2d_sincos_pos_embed_from_grid(n_embd, grid):\n",
        "    assert n_embd % 2 == 0\n",
        "\n",
        "\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(n_embd // 2, grid[0]) # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(n_embd // 2, grid[1]) # (H*W, D/2)\n",
        "\n",
        "    emb = jnp.concatenate([emb_h, emb_w], axis=1)  #(H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "##################################################################################\n",
        "\n",
        "@eqx.filter_jit\n",
        "def get_1d_sincos_pos_embed_from_grid(n_embd, pos):\n",
        "    assert n_embd % 2 == 0\n",
        "    omega = jnp.arange(n_embd // 2, dtype=float)\n",
        "    omega /= n_embd / 2\n",
        "    omega = 1. / 10000*omega #(D/2)\n",
        "\n",
        "    pos = jnp.array(pos)\n",
        "    #print(pos.shape)\n",
        "    #pos = einops.rearrange(pos, \"a -> a 0\")\n",
        "    out = jnp.outer(pos, omega)\n",
        "\n",
        "    emb_sin = jnp.sin(out)\n",
        "    emb_cos = jnp.cos(out)\n",
        "\n",
        "    emb = jnp.concatenate([emb_sin, emb_cos], axis=1)\n",
        "    return emb"
      ],
      "metadata": {
        "id": "EkoixIOpWs0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NN modules**"
      ],
      "metadata": {
        "id": "gnBO4_x7WcSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "##############                ###############\n",
        "##############    DiT model   ###############\n",
        "##############                ###############\n",
        "#############################################\n",
        "\n",
        "\"\"\"Diffusion models meet Transformers!\"\"\"\n",
        "\n",
        "\n",
        "###########   Time embedding    #############\n",
        "\n",
        "\n",
        "\n",
        "###### Define silu activation ######\n",
        "\n",
        "from typing import Callable\n",
        "\n",
        "class Lambda1(eqx.Module):\n",
        "    fn: Callable\n",
        "    \n",
        "    def __call__(self, x, *, key=None):\n",
        "        return self.fn(x)\n",
        "\n",
        "###### Time embedding ######\n",
        "\n",
        "class TimeStepEmbedder(eqx.Module):\n",
        "    mlp: eqx.nn.Sequential\n",
        "    frequency_embedding_size: int\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        frequency_embedding_size,   #set as 256\n",
        "        key\n",
        "    ):\n",
        "        l1key, l2key = jr.split(key, 2)\n",
        "        self.mlp = eqx.nn.Sequential([\n",
        "            eqx.nn.Linear(frequency_embedding_size, hidden_size, key=l1key),\n",
        "            Lambda1(jax.nn.silu),\n",
        "            eqx.nn.Linear(hidden_size, hidden_size, key=l2key)\n",
        "        ])\n",
        "        self.frequency_embedding_size = frequency_embedding_size\n",
        "\n",
        "    def __call__(self, t, max_period=10000):\n",
        "        dim = self.frequency_embedding_size\n",
        "        half = dim // 2\n",
        "        freqs = jnp.exp(\n",
        "            -jnp.log(max_period) * jnp.arange(0, half, dtype=float) / half\n",
        "        )\n",
        "        args = t[:, None].astype(float) * freqs[None]\n",
        "        embedding = jnp.concatenate([jnp.cos(args), jnp.sin(args)], axis=-1)\n",
        "        if dim % 2:\n",
        "            embedding = jnp.concatenate([embedding, jnp.zeros_like(embedding[:, :1])], axis=-1)\n",
        "        t_freq = embedding\n",
        "        t_emb = jax.vmap(self.mlp)(t_freq)\n",
        "        return t_emb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########   Multi-Head Attention   #########\n",
        "\n",
        "\n",
        "class MultiAtt(eqx.Module):\n",
        "    c_attn: eqx.nn.Linear\n",
        "    c_proj: eqx.nn.Linear\n",
        "    n_head: int\n",
        "\n",
        "    def __init__(self, n_embd, n_head, key):\n",
        "        assert n_embd % n_head ==0\n",
        "        atkey, prkey = jr.split(key, 2)\n",
        "        self.c_attn = eqx.nn.Linear(n_embd, 3 * n_embd, key=atkey)\n",
        "        self.c_proj = eqx.nn.Linear(n_embd, n_embd, key=prkey)\n",
        "        self.n_head = n_head\n",
        "        #self.mha = eqx.nn.MultiheadAttention(n_head, )\n",
        "\n",
        "    def __call__(self, x, mask=None):\n",
        "        B, T, C = x.shape\n",
        "        qkv = jax.vmap(jax.vmap(self.c_attn))(x)\n",
        "        qkv = qkv.reshape(B, T, self.n_head, -1)\n",
        "        qkv = jnp.transpose(qkv, (0, 2, 1, 3)) #[batch, number of heads, T or seq_length, embed_dim or C(channels)]\n",
        "        q, k, v = jnp.array_split(qkv, 3, axis=-1)\n",
        "        new_k = jnp.swapaxes(k, -2,-1) *(1.0 / jnp.sqrt(k.shape[-1]))\n",
        "        att = jax.lax.batch_matmul(q, new_k)\n",
        "        if mask is not None:\n",
        "            attn = jnp.where(mask == 0, -9e15, att)\n",
        "        attention = jax.nn.softmax(att, axis=-1)\n",
        "        values = jax.lax.batch_matmul(attention, v)\n",
        "        values = jnp.transpose(values, (0,2,1,3)) # [batch, T, n_heads, C]\n",
        "        values = values.reshape(B, T, C)\n",
        "        o = jax.vmap(jax.vmap(self.c_proj))(values)\n",
        "        return o   #, attention\n",
        "\n",
        "\n",
        "##############    DiT Block     ##############\n",
        "\n",
        "\n",
        "class DitBlock(eqx.Module):\n",
        "    norm1: eqx.nn.LayerNorm\n",
        "    norm2: eqx.nn.LayerNorm\n",
        "    attn: eqx.Module\n",
        "    Mlp: eqx.nn.MLP\n",
        "    adaLN_modulation: eqx.nn.Sequential\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        n_head,\n",
        "        mlp_ratio,   # = 4.0\n",
        "        key,\n",
        "    ):\n",
        "        mkey, adakey = jr.split(key, 2)\n",
        "        self.norm1 = eqx.nn.LayerNorm(hidden_size, eps = 1e-06, elementwise_affine=False)\n",
        "        self.attn = MultiAtt(hidden_size, n_head=n_head, key=key)\n",
        "        self.norm2 = eqx.nn.LayerNorm(hidden_size, eps = 1e-06, elementwise_affine=False)\n",
        "        mlp_hidden_size = int(hidden_size * mlp_ratio)\n",
        "        self.Mlp = eqx.nn.MLP(hidden_size, hidden_size, mlp_hidden_size, 1, key=mkey)\n",
        "        self.adaLN_modulation = eqx.nn.Sequential([\n",
        "            Lambda1(jax.nn.silu),\n",
        "            eqx.nn.Linear(hidden_size, 6 * hidden_size, key=adakey)\n",
        "        ])\n",
        "\n",
        "    def __call__(self, x, c):\n",
        "        temp = jax.vmap(self.adaLN_modulation)(c)\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = jnp.array_split(temp, 6, axis=1)\n",
        "        gate_msa = jnp.expand_dims(gate_msa, axis=0)\n",
        "        a = jax.vmap(self.norm1)(x)\n",
        "        tem = modulate(a, shift_msa, scale_msa)\n",
        "        x = x + gate_msa * self.attn(tem)\n",
        "        gate_mlp = jnp.expand_dims(gate_mlp, axis=0)\n",
        "        b = jax.vmap(self.norm2)(x)\n",
        "        tems = modulate(self.norm2(x), shift_mlp, scale_mlp)     \n",
        "        x = x + gate_mlp * jax.vmap(jax.vmap(self.Mlp))(tems)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#################   Final Layer   ################\n",
        "\n",
        "\n",
        "\n",
        "class FinalLayer(eqx.Module):\n",
        "    norm_final: eqx.nn.LayerNorm\n",
        "    linear: eqx.nn.Linear\n",
        "    adaLN_modulation: eqx.nn.Linear\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        patch_size,\n",
        "        out_channels,\n",
        "        key\n",
        "    ):\n",
        "        lkey, adakey = jr.split(key, 2)\n",
        "        self.norm_final = eqx.nn.LayerNorm(hidden_size, eps=1e-6, elementwise_affine=False)\n",
        "        self.linear = eqx.nn.Linear(hidden_size, patch_size * patch_size * out_channels, key=lkey)\n",
        "        self.adaLN_modulation = eqx.nn.Linear(hidden_size, 2 * hidden_size, key=adakey)\n",
        "\n",
        "    def __call__(self, x, c):\n",
        "        c = jax.nn.silu(c)\n",
        "        temp = jax.vmap(self.adaLN_modulation)(c)\n",
        "        shift, scale = jnp.array_split(temp, 2, axis=1)\n",
        "        x = modulate(self.norm_final(x), shift, scale)\n",
        "        x = jax.vmap(jax.vmap(self.linear))(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "###########   Patch embedding   ##############\n",
        "\n",
        "class PatchEmbed(eqx.Module):\n",
        "    num_patches:int\n",
        "    proj: eqx.nn.Conv2d\n",
        "    patch_size: int\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size,\n",
        "        patch_size,\n",
        "        in_chans,\n",
        "        n_embd,\n",
        "        key\n",
        "    ):\n",
        "        patkey, _ = jr.split(key,2)\n",
        "        self.patch_size = patch_size\n",
        "        dg = img_size // self.patch_size\n",
        "        self.num_patches = dg ** 2\n",
        "        self.proj = eqx.nn.Conv2d(in_chans, n_embd, self.patch_size, self.patch_size, key=patkey)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = jnp.array(x, dtype=float)\n",
        "        x = jax.vmap(self.proj)(x)\n",
        "        x = einops.rearrange(x, \"B C H W -> B (H W) C\")\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "###########   Parameter module    ##########\n",
        "\n",
        "class Params(eqx.Module):\n",
        "    param: jnp.ndarray\n",
        "\n",
        "    def __init__(self, num_patches, hidden_size):\n",
        "        self.param = jnp.zeros((1, num_patches, hidden_size), dtype = float)\n",
        "\n",
        "    def __call__(self):\n",
        "        return self.param\n",
        "\n",
        "\n",
        "##########    DiT   ##########\n",
        "\n",
        "\n",
        "class DiT(eqx.Module):\n",
        "    in_channels: int\n",
        "    out_channels: int\n",
        "    patch_size: int\n",
        "    n_head: int\n",
        "\n",
        "    x_embedder: eqx.Module\n",
        "    t_embedder: eqx.Module\n",
        "    pos_embed: eqx.Module\n",
        "    blocks: list\n",
        "    final_layer: eqx.Module\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=28,\n",
        "        patch_size=4,\n",
        "        in_channels=1,\n",
        "        hidden_size=384,\n",
        "        depth=4,\n",
        "        n_head=6,\n",
        "        mlp_ratio=4.0,\n",
        "        frequency_embedding_size=256,\n",
        "        *,\n",
        "        key=key,\n",
        "        \n",
        "    ):\n",
        "        xkey, tkey, flkey, *dbkeys = jr.split(key, 3 + depth)\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = in_channels\n",
        "        self.patch_size = patch_size\n",
        "        self.n_head = n_head\n",
        "        self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, key=xkey)\n",
        "        self.t_embedder = TimeStepEmbedder(hidden_size, frequency_embedding_size, key=tkey)\n",
        "        num_patches = self.x_embedder.num_patches\n",
        "        self.pos_embed = Params(num_patches, hidden_size)\n",
        "        self.blocks = [\n",
        "            DitBlock(\n",
        "                hidden_size, n_head, mlp_ratio, key = key\n",
        "            )\n",
        "            for dbkey in dbkeys                                   #_ in range(depth)           #*bkeys = jr.split(key, num_blocks)\n",
        "        ]\n",
        "        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels, key=flkey)\n",
        "    \n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"\n",
        "        x: (N, T, patch_size ** 2 * C)\n",
        "        imgs: (N, H, W, C)\n",
        "        \"\"\"\n",
        "        c = self.out_channels      \n",
        "        p = self.x_embedder.patch_size \n",
        "        h = w = int(x.shape[1] ** 0.5)    \n",
        "        x = jnp.reshape(x, (x.shape[0], h, w, p, p, c))\n",
        "        x = einops.rearrange(x, \"n h w p q c->n c h p w q\")\n",
        "        imgs = jnp.reshape(x, (x.shape[0], c, h * p, h * p))\n",
        "        return imgs\n",
        "    \n",
        "    def __call__(self, x, t):\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed().shape[-1], int(self.x_embedder.num_patches ** 0.5))\n",
        "        \n",
        "        \n",
        "        \"\"\"\n",
        "        x: (N, C, H, W)\n",
        "        t: (N, )\n",
        "        \"\"\"\n",
        "        \n",
        "        t = jnp.array([t], dtype=int)\n",
        "        x = self.x_embedder(x) + pos_embed  # (N, T, D), where T = H * W / patch_size ** 2\n",
        "        t = self.t_embedder(t)   # (N, D)\n",
        "        for block in self.blocks:\n",
        "            x = block(x,t)    # (N, T, D)\n",
        "        x = self.final_layer(x, t)     # (N, T, patch_size ** 2 * out_channels) - N is the batch_size, T is the number of patches, \n",
        "        x = self.unpatchify(x)        # (N, out_channels, H, W)\n",
        "        x = jnp.squeeze(x, axis=0)\n",
        "        return x"
      ],
      "metadata": {
        "id": "rLBy2RgjSNc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debugging functions"
      ],
      "metadata": {
        "id": "bxL33vh_yc7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DiT()"
      ],
      "metadata": {
        "id": "lpe7zwvuSjyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "F9lt_YfeUxk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = mnist()"
      ],
      "metadata": {
        "id": "thzcy2UYSayZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eF9e4o-VShOm",
        "outputId": "e4c53621-d66a-4549-8bdd-225cd8b3e400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eg = data[0]\n",
        "eg = eg[None,:,:,:]"
      ],
      "metadata": {
        "id": "JbF3Bm69U0GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eg.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PHJrvR1YJz1",
        "outputId": "a3f816d0-c3d6-4a15-a3ed-289f74002ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = 2"
      ],
      "metadata": {
        "id": "gr80hL8tU7Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(eg, t).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOItbatMU_wZ",
        "outputId": "973d2077-e910-43e2-dfba-31b697cbcb91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss functions"
      ],
      "metadata": {
        "id": "rDm2oF452jQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def single_loss_fn(model, weight, int_beta, data, t, key):\n",
        "    mean = data * jnp.exp(-0.5 * int_beta(t))\n",
        "    var = jnp.maximum(1 - jnp.exp(-int_beta(t)), 1e-5)\n",
        "    std = jnp.sqrt(var)\n",
        "    noise = jr.normal(key, data.shape)\n",
        "    y = mean + std * noise\n",
        "    y = jnp.expand_dims(y, axis=0)\n",
        "    pred = model(y,t)\n",
        "    return weight(t) * jnp.mean((pred + noise / std) ** 2)\n",
        "\n",
        "\n",
        "def batch_loss_fn(model, weight, int_beta, data, t1, key):\n",
        "    batch_size = data.shape[0]\n",
        "    tkey, losskey = jr.split(key)\n",
        "    losskey = jr.split(losskey, batch_size)\n",
        "    # Low-discrepancy sampling over t to reduce variance\n",
        "    t = jr.uniform(tkey, (batch_size,), minval=0, maxval=t1 / batch_size)\n",
        "    t = t + (t1 / batch_size) * jnp.arange(batch_size)\n",
        "    loss_fn = ft.partial(single_loss_fn, model, weight, int_beta)\n",
        "    loss_fn = jax.vmap(loss_fn)\n",
        "    return jnp.mean(loss_fn(data, t, losskey))"
      ],
      "metadata": {
        "id": "Xf-iL2jpBEB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update function"
      ],
      "metadata": {
        "id": "kUh2huWlXtxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@eqx.filter_jit\n",
        "def make_step(model, weight, int_beta, data, t1, key, opt_state, opt_update):\n",
        "    loss_fn = eqx.filter_value_and_grad(batch_loss_fn)\n",
        "    loss, grads = loss_fn(model, weight, int_beta, data, t1, key)\n",
        "    updates, opt_state = opt_update(grads, opt_state)\n",
        "    model = eqx.apply_updates(model, updates)\n",
        "    key = jr.split(key, 1)[0]\n",
        "    return loss, model, key, opt_state"
      ],
      "metadata": {
        "id": "J0sVh0_nBKOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampler"
      ],
      "metadata": {
        "id": "RRsIwypJjORC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@eqx.filter_jit\n",
        "def single_sample_fn(model, int_beta, data_shape, dt0, t1, key):\n",
        "    def drift(t, y, args):\n",
        "        _, beta = jax.jvp(int_beta, (t,), (jnp.ones_like(t),))\n",
        "        y = jnp.expand_dims(y, axis=0)\n",
        "        c = -0.5 * beta * (y + model(y,t))\n",
        "        #print(c.shape)\n",
        "        c = jnp.squeeze(c, axis=0)\n",
        "        return c\n",
        "\n",
        "    term = dfx.ODETerm(drift)\n",
        "    solver = dfx.Tsit5()\n",
        "    t0 = 0\n",
        "    y1 = jr.normal(key, data_shape)\n",
        "    # reverse time, solve from t1 to t0\n",
        "    sol = dfx.diffeqsolve(term, solver, t1, t0, -dt0, y1)\n",
        "    return sol.ys[0]"
      ],
      "metadata": {
        "id": "P86wJHrRjSAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "m-BSAfb1Xxk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Future main training loop"
      ],
      "metadata": {
        "id": "IsrCLvajo59T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(\n",
        "    # Model hyperparameters\n",
        "    input_size=28,\n",
        "    patch_size=4,\n",
        "    in_channels=1,\n",
        "    hidden_size=384,\n",
        "    depth=4,\n",
        "    n_head=6,\n",
        "    mlp_ratio=4.0,\n",
        "    frequency_embedding_size=256,\n",
        "    t1=10.0,\n",
        "    # Optimisation hyperparameters\n",
        "    num_steps=1_000_000,\n",
        "    lr=3e-4,\n",
        "    batch_size=256,\n",
        "    print_every=5_000,\n",
        "    # Sampling hyperparameters\n",
        "    dt0=0.1,\n",
        "    sample_size=10,\n",
        "    # Seed\n",
        "    seed=2023,\n",
        "):\n",
        "    key = jr.PRNGKey(seed)\n",
        "    model_key, train_key, loader_key, sample_key = jr.split(key, 4)\n",
        "    data = mnist()\n",
        "    data_mean = jnp.mean(data)\n",
        "    data_std = jnp.std(data)\n",
        "    data_max = jnp.max(data)\n",
        "    data_min = jnp.min(data)\n",
        "    #data_shape = data.shape[1:]\n",
        "    data = (data - data_mean) / data_std\n",
        "\n",
        "    model = DiT(\n",
        "        input_size,\n",
        "        patch_size,\n",
        "        in_channels,\n",
        "        hidden_size,\n",
        "        depth,\n",
        "        n_head,\n",
        "        mlp_ratio,\n",
        "        frequency_embedding_size,\n",
        "        key=model_key,\n",
        "    )\n",
        "    int_beta = lambda t: t  # Try experimenting with other options here!\n",
        "    weight = lambda t: 1 - jnp.exp(\n",
        "        -int_beta(t)\n",
        "    )  # Just chosen to upweight the region near t=0.\n",
        "\n",
        "    opt = optax.adabelief(lr)\n",
        "    # Optax will update the floating-point JAX arrays in the model.\n",
        "    opt_state = opt.init(eqx.filter(model, eqx.is_inexact_array))\n",
        "\n",
        "    total_value = 0\n",
        "    total_size = 0\n",
        "    for step, data in zip(\n",
        "        range(num_steps), dataloader(data, batch_size, key=loader_key)\n",
        "    ):\n",
        "        value, model, train_key, opt_state = make_step(\n",
        "            model, weight, int_beta, data, t1, train_key, opt_state, opt.update\n",
        "        )\n",
        "        total_value += value.item()\n",
        "        total_size += 1\n",
        "        if (step % print_every) == 0 or step == num_steps - 1:\n",
        "            print(f\"Step={step} Loss={total_value / total_size}\")\n",
        "            total_value = 0\n",
        "            total_size = 0\n",
        "\n",
        "    sample_key = jr.split(sample_key, sample_size**2)\n",
        "    sample_fn = ft.partial(single_sample_fn, model, int_beta, data_shape, dt0, t1)\n",
        "    sample = jax.vmap(sample_fn)(sample_key)\n",
        "    sample = data_mean + data_std * sample\n",
        "    sample = jnp.clip(sample, data_min, data_max)\n",
        "    sample = einops.rearrange(\n",
        "        sample, \"(n1 n2) 1 h w -> (n1 h) (n2 w)\", n1=sample_size, n2=sample_size\n",
        "    )\n",
        "    plt.imshow(sample, cmap=\"Greys\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0fBuMTg5bqEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Temporary manual training loop"
      ],
      "metadata": {
        "id": "5hPwC8pOo-Qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t1=10.0\n",
        "# Optimisation hyperparameters\n",
        "num_steps=500_000\n",
        "lr=3e-4\n",
        "batch_size=256\n",
        "print_every=5_000"
      ],
      "metadata": {
        "id": "3DiMj1uH0l1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = mnist()\n",
        "data_mean = jnp.mean(data)\n",
        "data_std = jnp.std(data)\n",
        "data_max = jnp.max(data)\n",
        "data_min = jnp.min(data)\n",
        "data_shape = data.shape[1:]\n",
        "data = (data - data_mean) / data_std"
      ],
      "metadata": {
        "id": "5iqUlNfy0sOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader_key, train_key, model_key, sample_key = jr.split(key, 4)"
      ],
      "metadata": {
        "id": "ZZtfxUuk1RqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DiT(key = model_key)"
      ],
      "metadata": {
        "id": "5xC1QPWW0xmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int_beta = lambda t: t  # Try experimenting with other options here!\n",
        "weight = lambda t: 1 - jnp.exp(-int_beta(t))  "
      ],
      "metadata": {
        "id": "sfqWMuv_01gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = optax.adabelief(lr)\n",
        "# Optax will update the floating-point JAX arrays in the model.\n",
        "opt_state = opt.init(eqx.filter(model, eqx.is_inexact_array))"
      ],
      "metadata": {
        "id": "0gN_Ft5J05yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_value = 0\n",
        "total_size = 0\n",
        "for step, data in zip(range(num_steps), dataloader(data, batch_size, key=loader_key)): \n",
        "    value, model, train_key, opt_state = make_step(model, weight, int_beta, data, t1, train_key, opt_state, opt.update)\n",
        "    total_value += value.item()\n",
        "    total_size += 1\n",
        "    if (step % print_every) == 0 or step == num_steps - 1:\n",
        "        print(f\"Step={step} Loss={total_value / total_size}\")\n",
        "        total_value = 0\n",
        "        total_size = 0"
      ],
      "metadata": {
        "id": "HR_WBH2ABNSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save model**"
      ],
      "metadata": {
        "id": "WrfT_WIepFAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eqx.tree_serialise_leaves(\"DiT_mnist.eqx\", model)\n",
        "shutil.copy('/content/DiT_mnist.eqx','/content/gdrive/MyDrive/Colab_Notebooks')"
      ],
      "metadata": {
        "id": "6P4YFNU9niOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling"
      ],
      "metadata": {
        "id": "ECNuSDaYgde2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_size = 2\n",
        "dt0 = 0.05"
      ],
      "metadata": {
        "id": "qBbPr-o6gxB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_key = jr.split(sample_key, sample_size**2)\n",
        "sample_fn = ft.partial(single_sample_fn, model, int_beta, data_shape, dt0, t1)\n",
        "sample = jax.vmap(sample_fn)(sample_key)\n",
        "sample = data_mean + data_std * sample\n",
        "sample = jnp.clip(sample, data_min, data_max)\n",
        "sample = einops.rearrange(sample, \"(n1 n2) 1 h w -> (n1 h) (n2 w)\", n1=sample_size, n2=sample_size)\n",
        "plt.imshow(sample, cmap=\"Greys\")\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sas4H60VgWfQ",
        "outputId": "2fea6640-204a-4f95-f6d7-0e3bafc86561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}