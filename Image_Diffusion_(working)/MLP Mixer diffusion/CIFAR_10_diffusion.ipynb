{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1pfDsaTE6nXyxC9X5V88I2mtulZDJwaYV",
      "authorship_tag": "ABX9TyMOfe7jyYZFequ67Ts/nz5F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durml91/MMath-Project/blob/duo-branch/Image_Diffusion_(working)/CIFAR_10_diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "!pip install diffrax\n",
        "!pip install optax\n",
        "!pip install equinox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAnDp7yPcn3D",
        "outputId": "659a1d3c-4de8-4251-b6da-21f9babdd2a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting diffrax\n",
            "  Downloading diffrax-0.2.2-py3-none-any.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.1/138.1 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jax>=0.3.4 in /usr/local/lib/python3.8/dist-packages (from diffrax) (0.3.25)\n",
            "Collecting equinox>=0.9.1\n",
            "  Downloading equinox-0.9.2-py3-none-any.whl (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jaxtyping>=0.2.5\n",
            "  Downloading jaxtyping-0.2.11-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->diffrax) (1.7.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->diffrax) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->diffrax) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->diffrax) (4.4.0)\n",
            "Collecting typeguard>=2.13.3\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, jaxtyping, equinox, diffrax\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 2.7.1\n",
            "    Uninstalling typeguard-2.7.1:\n",
            "      Successfully uninstalled typeguard-2.7.1\n",
            "Successfully installed diffrax-0.2.2 equinox-0.9.2 jaxtyping-0.2.11 typeguard-2.13.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optax\n",
            "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.8/dist-packages (from optax) (0.3.25)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.8/dist-packages (from optax) (0.3.25+cuda11.cudnn805)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.8/dist-packages (from optax) (4.4.0)\n",
            "Collecting chex>=0.1.5\n",
            "  Downloading chex-0.1.6-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 KB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from optax) (1.21.6)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax) (0.12.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.1.55->optax) (1.7.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Installing collected packages: chex, optax\n",
            "Successfully installed chex-0.1.6 optax-0.1.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: equinox in /usr/local/lib/python3.8/dist-packages (0.9.2)\n",
            "Requirement already satisfied: jaxtyping>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from equinox) (0.2.11)\n",
            "Requirement already satisfied: jax>=0.3.4 in /usr/local/lib/python3.8/dist-packages (from equinox) (0.3.25)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->equinox) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->equinox) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->equinox) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->equinox) (4.4.0)\n",
            "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.8/dist-packages (from jaxtyping>=0.2.5->equinox) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import array\n",
        "import functools as ft\n",
        "import gzip\n",
        "import os\n",
        "import struct\n",
        "import urllib.request\n",
        "\n",
        "import diffrax as dfx  # https://github.com/patrick-kidger/diffrax\n",
        "import einops  # https://github.com/arogozhnikov/einops\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr\n",
        "import matplotlib.pyplot as plt\n",
        "import optax  # https://github.com/deepmind/optax\n",
        "\n",
        "import equinox as eqx"
      ],
      "metadata": {
        "id": "xSYJIA33cgIh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil"
      ],
      "metadata": {
        "id": "eapOABvqouMo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQn7H7QrowZ0",
        "outputId": "bfeb3beb-e4e8-4dc6-f10a-0ece68440b65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cifar():\n",
        "  from tensorflow.keras.datasets import cifar10\n",
        "  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "  set1 = jnp.array(x_train)\n",
        "  set2 = jnp.array(x_test)\n",
        "\n",
        "  data = jnp.concatenate((set1, set2))\n",
        "\n",
        "  data_reag = einops.rearrange(data, \"n h w c -> n c h w\")\n",
        "  return data_reag"
      ],
      "metadata": {
        "id": "iBJhJK5teUdz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MixerBlock(eqx.Module):\n",
        "    patch_mixer: eqx.nn.MLP\n",
        "    hidden_mixer: eqx.nn.MLP\n",
        "    norm1: eqx.nn.LayerNorm\n",
        "    norm2: eqx.nn.LayerNorm\n",
        "\n",
        "    def __init__(\n",
        "        self, num_patches, hidden_size, mix_patch_size, mix_hidden_size, *, key\n",
        "    ):\n",
        "        tkey, ckey = jr.split(key, 2)\n",
        "        self.patch_mixer = eqx.nn.MLP(\n",
        "            num_patches, num_patches, mix_patch_size, depth=1, key=tkey\n",
        "        )\n",
        "        self.hidden_mixer = eqx.nn.MLP(\n",
        "            hidden_size, hidden_size, mix_hidden_size, depth=1, key=ckey\n",
        "        )\n",
        "        self.norm1 = eqx.nn.LayerNorm((hidden_size, num_patches))\n",
        "        self.norm2 = eqx.nn.LayerNorm((num_patches, hidden_size))\n",
        "\n",
        "    def __call__(self, y):\n",
        "        y = y + jax.vmap(self.patch_mixer)(self.norm1(y))\n",
        "        y = einops.rearrange(y, \"c p -> p c\")\n",
        "        y = y + jax.vmap(self.hidden_mixer)(self.norm2(y))\n",
        "        y = einops.rearrange(y, \"p c -> c p\")\n",
        "        return y\n",
        "\n",
        "\n",
        "class Mixer2d(eqx.Module):\n",
        "    conv_in: eqx.nn.Conv2d\n",
        "    conv_out: eqx.nn.ConvTranspose2d\n",
        "    blocks: list\n",
        "    norm: eqx.nn.LayerNorm\n",
        "    t1: float\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size,\n",
        "        patch_size,\n",
        "        hidden_size,\n",
        "        mix_patch_size,\n",
        "        mix_hidden_size,\n",
        "        num_blocks,\n",
        "        t1,\n",
        "        *,\n",
        "        key,\n",
        "    ):\n",
        "        input_size, height, width = img_size\n",
        "        assert (height % patch_size) == 0\n",
        "        assert (width % patch_size) == 0\n",
        "        num_patches = (height // patch_size) * (width // patch_size)\n",
        "        inkey, outkey, *bkeys = jr.split(key, 2 + num_blocks)\n",
        "\n",
        "        self.conv_in = eqx.nn.Conv2d(\n",
        "            input_size + 1, hidden_size, patch_size, stride=patch_size, key=inkey\n",
        "        )\n",
        "        self.conv_out = eqx.nn.ConvTranspose2d(\n",
        "            hidden_size, input_size, patch_size, stride=patch_size, key=outkey\n",
        "        )\n",
        "        self.blocks = [\n",
        "            MixerBlock(\n",
        "                num_patches, hidden_size, mix_patch_size, mix_hidden_size, key=bkey\n",
        "            )\n",
        "            for bkey in bkeys\n",
        "        ]\n",
        "        self.norm = eqx.nn.LayerNorm((hidden_size, num_patches))\n",
        "        self.t1 = t1\n",
        "\n",
        "    def __call__(self, t, y):\n",
        "        t = t / self.t1\n",
        "        _, height, width = y.shape\n",
        "        t = einops.repeat(t, \"-> 1 h w\", h=height, w=width)\n",
        "        y = jnp.concatenate([y, t])\n",
        "        y = self.conv_in(y)\n",
        "        _, patch_height, patch_width = y.shape\n",
        "        y = einops.rearrange(y, \"c h w -> c (h w)\")\n",
        "        for block in self.blocks:\n",
        "            y = block(y)\n",
        "        y = self.norm(y)\n",
        "        y = einops.rearrange(y, \"c (h w) -> c h w\", h=patch_height, w=patch_width)\n",
        "        return self.conv_out(y)"
      ],
      "metadata": {
        "id": "JuD5K1f1ejQe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def single_loss_fn(model, weight, int_beta, data, t, key):\n",
        "    mean = data * jnp.exp(-0.5 * int_beta(t))\n",
        "    var = jnp.maximum(1 - jnp.exp(-int_beta(t)), 1e-5)\n",
        "    std = jnp.sqrt(var)\n",
        "    noise = jr.normal(key, data.shape)\n",
        "    y = mean + std * noise\n",
        "    pred = model(t, y)\n",
        "    return weight(t) * jnp.mean((pred + noise / std) ** 2)\n",
        "\n",
        "\n",
        "def batch_loss_fn(model, weight, int_beta, data, t1, key):\n",
        "    batch_size = data.shape[0]\n",
        "    tkey, losskey = jr.split(key)\n",
        "    losskey = jr.split(losskey, batch_size)\n",
        "    # Low-discrepancy sampling over t to reduce variance\n",
        "    t = jr.uniform(tkey, (batch_size,), minval=0, maxval=t1 / batch_size)\n",
        "    t = t + (t1 / batch_size) * jnp.arange(batch_size)\n",
        "    loss_fn = ft.partial(single_loss_fn, model, weight, int_beta)\n",
        "    loss_fn = jax.vmap(loss_fn)\n",
        "    return jnp.mean(loss_fn(data, t, losskey))\n",
        "\n",
        "\n",
        "@eqx.filter_jit\n",
        "def single_sample_fn(model, int_beta, data_shape, dt0, t1, key):\n",
        "    def drift(t, y, args):\n",
        "        _, beta = jax.jvp(int_beta, (t,), (jnp.ones_like(t),))\n",
        "        return -0.5 * beta * (y + model(t, y))\n",
        "\n",
        "    term = dfx.ODETerm(drift)\n",
        "    solver = dfx.Tsit5()\n",
        "    t0 = 0\n",
        "    y1 = jr.normal(key, data_shape)\n",
        "    # reverse time, solve from t1 to t0\n",
        "    sol = dfx.diffeqsolve(term, solver, t1, t0, -dt0, y1, adjoint=dfx.NoAdjoint())\n",
        "    return sol.ys[0]"
      ],
      "metadata": {
        "id": "wzjN1sw1equL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataloader(data, batch_size, *, key):\n",
        "    dataset_size = data.shape[0]\n",
        "    indices = jnp.arange(dataset_size)\n",
        "    while True:\n",
        "        perm = jr.permutation(key, indices)\n",
        "        (key,) = jr.split(key, 1)\n",
        "        start = 0\n",
        "        end = batch_size\n",
        "        while end < dataset_size:\n",
        "            batch_perm = perm[start:end]\n",
        "            yield data[batch_perm]\n",
        "            start = end\n",
        "            end = start + batch_size"
      ],
      "metadata": {
        "id": "3Pk6JXCXetYH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@eqx.filter_jit\n",
        "def make_step(model, weight, int_beta, data, t1, key, opt_state, opt_update):\n",
        "    loss_fn = eqx.filter_value_and_grad(batch_loss_fn)\n",
        "    loss, grads = loss_fn(model, weight, int_beta, data, t1, key)\n",
        "    updates, opt_state = opt_update(grads, opt_state)\n",
        "    model = eqx.apply_updates(model, updates)\n",
        "    key = jr.split(key, 1)[0]\n",
        "    return loss, model, key, opt_state"
      ],
      "metadata": {
        "id": "vOOMS9d2fJu7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(\n",
        "    # Model hyperparameters\n",
        "    patch_size=4,\n",
        "    hidden_size=64,\n",
        "    mix_patch_size=512,\n",
        "    mix_hidden_size=512,\n",
        "    num_blocks=4,\n",
        "    t1=10.0,\n",
        "    # Optimisation hyperparameters\n",
        "    num_steps=1_000_000,\n",
        "    lr=3e-4,\n",
        "    batch_size=256,\n",
        "    print_every=10_000,\n",
        "    # Sampling hyperparameters\n",
        "    dt0=0.1,\n",
        "    #sample_size=4,\n",
        "    # Seed\n",
        "    seed=2023,\n",
        "):\n",
        "    key = jr.PRNGKey(seed)\n",
        "    model_key, train_key, loader_key, sample_key = jr.split(key, 4)\n",
        "    data = cifar()\n",
        "    data_mean = jnp.mean(data)\n",
        "    data_std = jnp.std(data)\n",
        "    data_max = jnp.max(data)\n",
        "    data_min = jnp.min(data)\n",
        "    data_shape = data.shape[1:]\n",
        "    data = (data - data_mean) / data_std\n",
        "\n",
        "    model = Mixer2d(\n",
        "        data_shape,\n",
        "        patch_size,\n",
        "        hidden_size,\n",
        "        mix_patch_size,\n",
        "        mix_hidden_size,\n",
        "        num_blocks,\n",
        "        t1,\n",
        "        key=model_key,\n",
        "    )\n",
        "    int_beta = lambda t: t  # Try experimenting with other options here!\n",
        "    weight = lambda t: 1 - jnp.exp(\n",
        "        -int_beta(t)\n",
        "    )  # Just chosen to upweight the region near t=0.\n",
        "\n",
        "    opt = optax.adabelief(lr)\n",
        "    # Optax will update the floating-point JAX arrays in the model.\n",
        "    opt_state = opt.init(eqx.filter(model, eqx.is_inexact_array))\n",
        "\n",
        "    total_value = 0\n",
        "    total_size = 0\n",
        "    for step, data in zip(\n",
        "        range(num_steps), dataloader(data, batch_size, key=loader_key)\n",
        "    ):\n",
        "        value, model, train_key, opt_state = make_step(\n",
        "            model, weight, int_beta, data, t1, train_key, opt_state, opt.update\n",
        "        )\n",
        "        total_value += value.item()\n",
        "        total_size += 1\n",
        "        if (step % print_every) == 0 or step == num_steps - 1:\n",
        "            print(f\"Step={step} Loss={total_value / total_size}\")\n",
        "            total_value = 0\n",
        "            total_size = 0\n",
        "\n",
        "    # sample_key = jr.split(sample_key, sample_size**2)\n",
        "    # sample_fn = ft.partial(single_sample_fn, model, int_beta, data_shape, dt0, t1)\n",
        "    # sample = jax.vmap(sample_fn)(sample_key)\n",
        "    # sample = data_mean + data_std * sample\n",
        "    # sample = jnp.clip(sample, data_min, data_max)\n",
        "    # sample = einops.rearrange(sample, \"(n1 n2) 3 h w -> (3 n1 h) (n2 w)\", n1=sample_size, n2=sample_size)\n",
        "    \n",
        "    \n",
        "    eqx.tree_serialise_leaves(\"cifar10_model.eqx\", model)\n",
        "    shutil.copy('/content/cifar10_model.eqx','/content/gdrive/MyDrive/Colab_Notebooks')\n",
        "\n",
        "\n",
        "    sample = single_sample_fn( model, int_beta, data_shape, dt0, t1, sample_key)\n",
        "    sample = data_mean + data_std * sample\n",
        "    sample = jnp.clip(sample, data_min, data_max)\n",
        "    sample = einops.rearrange(sample, \"c h w -> h w c\") # tranpose\n",
        "    sample = jnp.array(sample, dtype=int)\n",
        "    print(sample)\n",
        "    plt.imshow(sample)\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "aW92qzaJevuj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C7ysRfpxeymV",
        "outputId": "d9110c14-d396-43fb-dbda-55965bb1d99b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n",
            "Step=0 Loss=1.0112404823303223\n",
            "Step=10000 Loss=0.035394261854700744\n",
            "Step=20000 Loss=0.025169736861251294\n",
            "Step=30000 Loss=0.02426148052737117\n",
            "Step=40000 Loss=0.023534108498506248\n",
            "Step=50000 Loss=0.02292249101176858\n",
            "Step=60000 Loss=0.022441766785643996\n",
            "Step=70000 Loss=0.02207370957341045\n",
            "Step=80000 Loss=0.02174152689278126\n",
            "Step=90000 Loss=0.021514976149983704\n",
            "Step=100000 Loss=0.021335032659210263\n",
            "Step=110000 Loss=0.021177873464673758\n",
            "Step=120000 Loss=0.021051311974786224\n",
            "Step=130000 Loss=0.02091251601036638\n",
            "Step=140000 Loss=0.020782361959852277\n",
            "Step=150000 Loss=0.02069324722029269\n",
            "Step=160000 Loss=0.020606496958807112\n",
            "Step=170000 Loss=0.020514849337004124\n",
            "Step=180000 Loss=0.020451261303946377\n",
            "Step=190000 Loss=0.020374410583265126\n",
            "Step=200000 Loss=0.020324559670127928\n",
            "Step=210000 Loss=0.020264538280665875\n",
            "Step=220000 Loss=0.02020052242372185\n",
            "Step=230000 Loss=0.020142714905552566\n",
            "Step=240000 Loss=0.02008366367649287\n",
            "Step=250000 Loss=0.02005077879689634\n",
            "Step=260000 Loss=0.019976908668875696\n",
            "Step=270000 Loss=0.01995568104684353\n",
            "Step=280000 Loss=0.019899897362105547\n",
            "Step=290000 Loss=0.01986789531763643\n",
            "Step=300000 Loss=0.019830192382633685\n",
            "Step=310000 Loss=0.01978922428544611\n",
            "Step=320000 Loss=0.01976399864219129\n",
            "Step=330000 Loss=0.019714744712784886\n",
            "Step=340000 Loss=0.019685048204660417\n",
            "Step=350000 Loss=0.019670400022156538\n",
            "Step=360000 Loss=0.019639876564219595\n",
            "Step=370000 Loss=0.019592069374769927\n",
            "Step=380000 Loss=0.019568784081935884\n",
            "Step=390000 Loss=0.01956122266445309\n",
            "Step=400000 Loss=0.019514472077600657\n",
            "Step=410000 Loss=0.019495271209068597\n",
            "Step=420000 Loss=0.019465868000872435\n",
            "Step=430000 Loss=0.01944419674295932\n",
            "Step=440000 Loss=0.01941955879535526\n",
            "Step=450000 Loss=0.01941360585652292\n",
            "Step=460000 Loss=0.01938072493970394\n",
            "Step=470000 Loss=0.019365555614233018\n",
            "Step=480000 Loss=0.019340323332324626\n",
            "Step=490000 Loss=0.01932330519948155\n",
            "Step=500000 Loss=0.019302644618973137\n",
            "Step=510000 Loss=0.01927092543374747\n",
            "Step=520000 Loss=0.019265202456712722\n",
            "Step=530000 Loss=0.019235483385436236\n",
            "Step=540000 Loss=0.019228251817636193\n",
            "Step=550000 Loss=0.019220427083969115\n",
            "Step=560000 Loss=0.019205426657199858\n",
            "Step=570000 Loss=0.019178187995776533\n",
            "Step=580000 Loss=0.019180780165456237\n",
            "Step=590000 Loss=0.019158619432151316\n",
            "Step=600000 Loss=0.01913993806540966\n",
            "Step=610000 Loss=0.019143125890009105\n",
            "Step=620000 Loss=0.019103416168130934\n",
            "Step=630000 Loss=0.019093986148945988\n",
            "Step=640000 Loss=0.019085108437016607\n",
            "Step=650000 Loss=0.019068384276703\n",
            "Step=660000 Loss=0.019039615619555115\n",
            "Step=670000 Loss=0.01903898647390306\n",
            "Step=680000 Loss=0.01901637941878289\n",
            "Step=690000 Loss=0.019004070683941245\n",
            "Step=700000 Loss=0.018990621852874756\n",
            "Step=710000 Loss=0.01899031711779535\n",
            "Step=720000 Loss=0.018958172101713716\n",
            "Step=730000 Loss=0.018964450890384614\n",
            "Step=740000 Loss=0.018939964995067567\n",
            "Step=750000 Loss=0.018923662277497352\n",
            "Step=760000 Loss=0.018921085370332002\n",
            "Step=770000 Loss=0.018896938589960336\n",
            "Step=780000 Loss=0.018871357426419853\n",
            "Step=790000 Loss=0.018862253729626535\n",
            "Step=800000 Loss=0.018878112073801457\n",
            "Step=810000 Loss=0.01886080008149147\n",
            "Step=820000 Loss=0.01884662449937314\n",
            "Step=830000 Loss=0.018820486560836434\n",
            "Step=840000 Loss=0.018817569209262728\n",
            "Step=850000 Loss=0.018808495851233602\n",
            "Step=860000 Loss=0.018800414365530014\n",
            "Step=870000 Loss=0.018771968749538064\n",
            "Step=880000 Loss=0.018767373403254895\n",
            "Step=890000 Loss=0.0187711740963161\n",
            "Step=900000 Loss=0.018766752954758704\n",
            "Step=910000 Loss=0.018743407260067763\n",
            "Step=920000 Loss=0.018748749587126076\n",
            "Step=930000 Loss=0.01873586557097733\n",
            "Step=940000 Loss=0.01872479632999748\n",
            "Step=950000 Loss=0.018709232883527875\n",
            "Step=960000 Loss=0.018721002362295985\n",
            "Step=970000 Loss=0.01870049288813025\n",
            "Step=980000 Loss=0.01869302197508514\n",
            "Step=990000 Loss=0.018674338422715663\n",
            "Step=999999 Loss=0.018673059736492217\n",
            "[[[ 69  87  80]\n",
            "  [ 48  70  86]\n",
            "  [124 143 147]\n",
            "  ...\n",
            "  [ 33  37  24]\n",
            "  [ 40  35   8]\n",
            "  [ 41  36  28]]\n",
            "\n",
            " [[100 104 105]\n",
            "  [ 68 109  98]\n",
            "  [124 130 144]\n",
            "  ...\n",
            "  [ 30  21  24]\n",
            "  [ 65  53  50]\n",
            "  [ 61  69  53]]\n",
            "\n",
            " [[ 85  90  86]\n",
            "  [ 53  73  67]\n",
            "  [152 168 173]\n",
            "  ...\n",
            "  [ 29  30  26]\n",
            "  [ 72  66  51]\n",
            "  [ 92  95  75]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[124 173 218]\n",
            "  [133 199 232]\n",
            "  [123 188 231]\n",
            "  ...\n",
            "  [ 99 144 202]\n",
            "  [102 154 222]\n",
            "  [ 87 139 194]]\n",
            "\n",
            " [[143 180 235]\n",
            "  [127 202 241]\n",
            "  [131 190 239]\n",
            "  ...\n",
            "  [ 87 120 188]\n",
            "  [103 148 195]\n",
            "  [ 89 152 208]]\n",
            "\n",
            " [[142 187 228]\n",
            "  [139 182 229]\n",
            "  [126 184 221]\n",
            "  ...\n",
            "  [102 138 191]\n",
            "  [ 99 144 198]\n",
            "  [ 95 142 195]]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}