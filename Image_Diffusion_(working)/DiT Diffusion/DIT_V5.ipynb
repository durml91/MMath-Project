{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMufoiBL6UNCAq8jAsDJ6u0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durml91/MMath-Project/blob/duo-branch/Image_Diffusion_(working)/DiT%20Diffusion/DIT_V5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqYPElSabF4Y",
        "outputId": "0c773779-1533-4ecc-e399-f79dd5cf41c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting diffrax\n",
            "  Downloading diffrax-0.3.1-py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jax>=0.4.3 in /usr/local/lib/python3.9/dist-packages (from diffrax) (0.4.4)\n",
            "Collecting equinox>=0.10.0\n",
            "  Downloading equinox-0.10.1-py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.7/108.7 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jaxtyping>=0.2.12\n",
            "  Downloading jaxtyping-0.2.14-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.4.3->diffrax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.4.3->diffrax) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from jax>=0.4.3->diffrax) (1.22.4)\n",
            "Collecting typeguard>=2.13.3\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.12->equinox>=0.10.0->diffrax) (4.5.0)\n",
            "Installing collected packages: typeguard, jaxtyping, equinox, diffrax\n",
            "Successfully installed diffrax-0.3.1 equinox-0.10.1 jaxtyping-0.2.14 typeguard-2.13.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: equinox in /usr/local/lib/python3.9/dist-packages (0.10.1)\n",
            "Requirement already satisfied: jaxtyping>=0.2.12 in /usr/local/lib/python3.9/dist-packages (from equinox) (0.2.14)\n",
            "Requirement already satisfied: jax>=0.4.3 in /usr/local/lib/python3.9/dist-packages (from equinox) (0.4.4)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.4.3->equinox) (1.10.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.4.3->equinox) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from jax>=0.4.3->equinox) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.12->equinox) (4.5.0)\n",
            "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.12->equinox) (2.13.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optax\n",
            "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.9/dist-packages (from optax) (0.4.4)\n",
            "Collecting chex>=0.1.5\n",
            "  Downloading chex-0.1.6-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from optax) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.9/dist-packages (from optax) (4.5.0)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.9/dist-packages (from optax) (0.4.4+cuda11.cudnn82)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from chex>=0.1.5->optax) (0.12.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.9/dist-packages (from chex>=0.1.5->optax) (0.1.8)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (1.10.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Installing collected packages: chex, optax\n",
            "Successfully installed chex-0.1.6 optax-0.1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install diffrax\n",
        "!pip install equinox\n",
        "!pip install einops\n",
        "!pip install optax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import array\n",
        "import functools as ft\n",
        "import gzip\n",
        "import os\n",
        "import struct\n",
        "import urllib.request\n",
        "\n",
        "import diffrax as dfx  # https://github.com/patrick-kidger/diffrax\n",
        "import einops  # https://github.com/arogozhnikov/einops\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr\n",
        "import matplotlib.pyplot as plt\n",
        "import optax  # https://github.com/deepmind/optax\n",
        "\n",
        "import equinox as eqx"
      ],
      "metadata": {
        "id": "nkebHYG1bU4C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = jr.PRNGKey(2023)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H11C7UeVbZoD",
        "outputId": "f151b396-22e0-4568-8e08-2888dd6838b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = 10.0"
      ],
      "metadata": {
        "id": "H517RV7n9RIy"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cifar():\n",
        "  from tensorflow.keras.datasets import cifar10\n",
        "  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "  \n",
        "  set1 = jnp.array(x_train)\n",
        "  set2 = jnp.array(x_test)\n",
        "\n",
        "  lab1 = jnp.array(y_train)\n",
        "  lab2 = jnp.array(y_test)\n",
        "\n",
        "  im = jnp.concatenate((set1, set2))\n",
        "  lab = jnp.concatenate((lab1, lab2))\n",
        "  im = einops.rearrange(im, \"n h w c -> n c h w\")\n",
        " \n",
        "\n",
        "  \n",
        "  return im, lab"
      ],
      "metadata": {
        "id": "shnETeYLbZuj"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@eqx.filter_jit\n",
        "def modulate(x, shift, scale):\n",
        "    scale = jnp.expand_dims(scale, axis=1)       #scale is the value you mutliply your array by\n",
        "    shift = jnp.expand_dims(shift, axis=1)       #shift is the value you move your array b                     \n",
        "    return x * (1 + scale) + shift #1+scale to allow for zero scaling\n",
        "\n",
        "    #e.g. by inputting (x, 0 ,0) you end up with x returned\n",
        "    #this is just a very intuitive function!"
      ],
      "metadata": {
        "id": "woD2CdzWbdil"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@eqx.filter_jit\n",
        "def get_2d_sincos_pos_embed(n_embd, grid_size):\n",
        "\n",
        "    \"\"\"grid_size: int value of grid height and width - we denote by (H, W)\n",
        "       return pos_embed of dim (grid_size*grid_size, n_embd)\"\"\"\n",
        "    \n",
        "\n",
        "    grid_h = jnp.arange(grid_size, dtype=float)\n",
        "    grid_w = jnp.arange(grid_size, dtype=float)\n",
        "    grid = jnp.meshgrid(grid_w, grid_h)\n",
        "    grid = jnp.stack(grid, axis=0)\n",
        "\n",
        "    grid = jnp.reshape(grid, (2, 1, grid_size, grid_size))\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(n_embd, grid)\n",
        "    return pos_embed\n",
        "\n",
        "#we basically end up with each patch embedding having a fixed n_embd dimensional sin/cos embedding vector - this is fixed! doesn't change based on sample but allows the NN to understand the \"spatial\" representation of the patches\n",
        "##################################################################################\n",
        "\n",
        "@eqx.filter_jit\n",
        "def get_2d_sincos_pos_embed_from_grid(n_embd, grid):\n",
        "    assert n_embd % 2 == 0\n",
        "\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(n_embd // 2, grid[0]) # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(n_embd // 2, grid[1]) # (H*W, D/2)\n",
        "\n",
        "    emb = jnp.concatenate([emb_h, emb_w], axis=1)  #(H*W, D)\n",
        "    return emb\n",
        "\n",
        "#if we have a grid - think two rows of however many columns - we take each row and encode and get an positional embedding for each element in the row\n",
        "\n",
        "##################################################################################\n",
        "\n",
        "@eqx.filter_jit\n",
        "def get_1d_sincos_pos_embed_from_grid(n_embd, pos):\n",
        "    \"\"\"n_embd is the output dimension for each position (D,)\n",
        "       pos is an array of positions to be encoded of size (M,)\n",
        "       size is of pos (M,D)\"\"\"\n",
        "    \n",
        "    assert n_embd % 2 == 0                \n",
        "    omega = jnp.arange(n_embd // 2, dtype=float)   #generates an array from 0 to (n_embd // 2) - 1 of integer values    \n",
        "    omega /= n_embd / 2         #divide by n_embd / 2 - can be thought of as normalising the values from to 0 to 1 i.e. uniform values\n",
        "    omega = 1. / 10000**omega #(D/2) - so n_embd is D  \n",
        "   \n",
        "    pos = jnp.array(pos)\n",
        "    out = jnp.outer(pos, omega)   #so we have pos^T * omega to make a matrix of dim (M, D/2)\n",
        "    emb_sin = jnp.sin(out)   #dim (M, D/2)\n",
        "    emb_cos = jnp.cos(out)   #dim (M, D/2)\n",
        "\n",
        "    emb = jnp.concatenate([emb_sin, emb_cos], axis=1)  #dim (M, D) - join sin and cos column wise -> <-\n",
        "    return emb\n",
        "\n",
        "\n",
        "    #if we think of the input as a sequence of words and the respective n_embedding, then we are simply adding a positional embedding across each word in the sequence "
      ],
      "metadata": {
        "id": "ZNYIT0Znbfn1"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "##############                ###############\n",
        "##############    DiT model   ###############\n",
        "##############                ###############\n",
        "#############################################\n",
        "\n",
        "\"\"\"Diffusion models meet Transformers!\"\"\"\n",
        "\n",
        "\n",
        "###########   Time embedding    #############\n",
        "\n",
        "\n",
        "\n",
        "###### Define silu activation ######\n",
        "\n",
        "from typing import Callable\n",
        "\n",
        "class Lambda1(eqx.Module):\n",
        "    fn: Callable\n",
        "    \n",
        "    def __call__(self, x, *, key=None):\n",
        "        return self.fn(x)\n",
        "\n",
        "###### Time embedding ######\n",
        "\n",
        "class TimeStepEmbedder(eqx.Module):\n",
        "    mlp: eqx.nn.Sequential\n",
        "    frequency_embedding_size: int\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        frequency_embedding_size,   #set as 256\n",
        "        key\n",
        "    ):\n",
        "        l1key, l2key = jr.split(key, 2)\n",
        "        self.mlp = eqx.nn.Sequential([\n",
        "            eqx.nn.Linear(frequency_embedding_size, hidden_size, key=l1key),\n",
        "            Lambda1(jax.nn.silu),\n",
        "            eqx.nn.Linear(hidden_size, hidden_size, key=l2key)\n",
        "        ])\n",
        "        self.frequency_embedding_size = frequency_embedding_size\n",
        "\n",
        "    def __call__(self, t, max_period=10000):\n",
        "        dim = self.frequency_embedding_size\n",
        "        half = dim // 2\n",
        "        freqs = jnp.exp(\n",
        "            -jnp.log(max_period) * jnp.arange(0, half, dtype=float) / half\n",
        "        )\n",
        "        args = t[:, None].astype(float) * freqs[None]\n",
        "        embedding = jnp.concatenate([jnp.cos(args), jnp.sin(args)], axis=-1)\n",
        "        if dim % 2:\n",
        "            embedding = jnp.concatenate([embedding, jnp.zeros_like(embedding[:, :1])], axis=-1)\n",
        "        t_freq = embedding\n",
        "        t_emb = jax.vmap(self.mlp)(t_freq)\n",
        "        return t_emb\n",
        "\n",
        "###########  Label Embedding   #########\n",
        "\n",
        "class LabelEmbedder(eqx.Module):\n",
        "  embedding_table: eqx.nn.Embedding\n",
        "  num_classes: int\n",
        "  dropout_prob: float\n",
        "\n",
        "  def __init__(self, num_classes, hidden_size, dropout_prob, key):\n",
        "      key1 = jr.split(key, 1)[0]\n",
        "      self.embedding_table = eqx.nn.Embedding(num_classes, hidden_size, key=key1)\n",
        "      self.num_classes = num_classes\n",
        "      self.dropout_prob = dropout_prob\n",
        "\n",
        "  def __call__(self, labels):\n",
        "      use_dropout = self.dropout_prob\n",
        "      embeddings = self.embedding_table(labels)\n",
        "      return embeddings\n",
        "\n",
        "\n",
        "##########   Multi-Head Attention   #########\n",
        "\n",
        "\n",
        "class MHA(eqx.Module):\n",
        "    num_heads: int\n",
        "    head_dim: int\n",
        "    scale: int\n",
        "\n",
        "    qkv: eqx.nn.Linear\n",
        "    o_proj: eqx.nn.Linear\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self, dim, key, num_heads=8, ):\n",
        "        assert dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        \n",
        "        key1, key2 = jr.split(key, 2)\n",
        "        self.qkv = eqx.nn.Linear(dim, dim *3, key=key1)\n",
        "        self.o_proj = eqx.nn.Linear(dim, dim, key=key2)\n",
        "\n",
        "    def __call__(self, x, mask=None):\n",
        "        B, N, C = x.shape\n",
        "        qkv = jax.vmap(jax.vmap(self.qkv))(x)\n",
        "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.transpose(2, 0, 3, 1, 4)\n",
        "\n",
        "        q, k, v =jnp.moveaxis(qkv, 0, 0)\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn_logits = jnp.matmul(q, jnp.swapaxes(k, -2, -1))\n",
        "        if mask is not None:\n",
        "            attn_logits = jnp.where(mask == 0, -9e15, attn_logits)\n",
        "        attention = jax.nn.softmax(attn_logits, axis=-1)\n",
        "        values = jnp.matmul(attention, v)\n",
        "        values = einops.rearrange(values, \"B C N H -> B N (C H)\")\n",
        "\n",
        "        values = jax.vmap(jax.vmap(self.o_proj))(values)\n",
        "        return values\n",
        "\n",
        "\n",
        "##############    DiT Block     ##############\n",
        "\n",
        "\n",
        "class DitBlock(eqx.Module):\n",
        "    norm1: eqx.nn.LayerNorm\n",
        "    norm2: eqx.nn.LayerNorm\n",
        "    attn: eqx.Module\n",
        "    Mlp: eqx.nn.Sequential\n",
        "    adaLN_modulation: eqx.nn.Sequential\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        n_head,\n",
        "        mlp_ratio,   # = 4.0\n",
        "        key,\n",
        "    ):\n",
        "        m1key, m2key, adakey, attkey = jr.split(key, 4)\n",
        "        self.norm1 = eqx.nn.LayerNorm(hidden_size, eps = 1e-06, elementwise_affine=False)\n",
        "        self.attn = MHA(hidden_size, key=attkey, num_heads=n_head,)\n",
        "        self.norm2 = eqx.nn.LayerNorm(hidden_size, eps = 1e-06, elementwise_affine=False)\n",
        "        mlp_hidden_size = int(hidden_size * mlp_ratio)\n",
        "        self.Mlp = eqx.nn.Sequential([\n",
        "            eqx.nn.Linear(hidden_size, mlp_hidden_size, key=m1key),\n",
        "            Lambda1(jax.nn.gelu),\n",
        "            eqx.nn.Linear(mlp_hidden_size, hidden_size, key=m2key) ])\n",
        "        self.adaLN_modulation = eqx.nn.Sequential([\n",
        "            Lambda1(jax.nn.silu),\n",
        "            eqx.nn.Linear(hidden_size, 6 * hidden_size, key=adakey)\n",
        "        ])\n",
        "\n",
        "    def __call__(self, x, t):\n",
        "\n",
        "        temp = jax.vmap(self.adaLN_modulation)(t)\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = jnp.array_split(temp, 6, axis=1)\n",
        "        \n",
        "        \n",
        "        gate_msa = jnp.expand_dims(gate_msa, axis=1)\n",
        "        a = jax.vmap(self.norm1)(x)\n",
        "        tem = modulate(a, shift_msa, scale_msa)\n",
        "        x = x + gate_msa * self.attn(tem)\n",
        "        \n",
        "        \n",
        "        gate_mlp = jnp.expand_dims(gate_mlp, axis=1)\n",
        "        b = jax.vmap(self.norm2)(x)\n",
        "        tems = modulate(b, shift_mlp, scale_mlp)     \n",
        "        x = x + gate_mlp * jax.vmap(jax.vmap(self.Mlp))(tems)\n",
        "        \n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "#################   Final Layer   ################\n",
        "\n",
        "\n",
        "\n",
        "class FinalLayer(eqx.Module):\n",
        "    norm_final: eqx.nn.LayerNorm\n",
        "    linear: eqx.nn.Linear\n",
        "    adaLN_modulation: eqx.nn.Sequential\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        patch_size,\n",
        "        out_channels,\n",
        "        key\n",
        "    ):\n",
        "        lkey, adakey = jr.split(key, 2)\n",
        "        self.norm_final = eqx.nn.LayerNorm(hidden_size, eps=1e-6, elementwise_affine=False)\n",
        "        self.linear = eqx.nn.Linear(hidden_size, patch_size * patch_size * out_channels, key=lkey)\n",
        "        self.adaLN_modulation = eqx.nn.Sequential([\n",
        "            Lambda1(jax.nn.silu),\n",
        "            eqx.nn.Linear(hidden_size, 2 * hidden_size, key=adakey)\n",
        "        ])\n",
        "\n",
        "    def __call__(self, x, t):\n",
        "        temp = jax.vmap(self.adaLN_modulation)(t)\n",
        "        shift, scale = jnp.array_split(temp, 2, axis=1)\n",
        "        x = modulate(jax.vmap(self.norm_final)(x), shift, scale)\n",
        "        x = jax.vmap(jax.vmap(self.linear))(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "###########   Patch embedding   ##############\n",
        "\n",
        "class PatchEmbed(eqx.Module):\n",
        "    num_patches:int\n",
        "    proj: eqx.nn.Conv2d\n",
        "    patch_size: int\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size,\n",
        "        patch_size,\n",
        "        in_chans,\n",
        "        n_embd,\n",
        "        key\n",
        "    ):\n",
        "        patkey, _ = jr.split(key,2)\n",
        "        self.patch_size = patch_size\n",
        "        dg = img_size // self.patch_size\n",
        "        self.num_patches = dg ** 2\n",
        "        self.proj = eqx.nn.Conv2d(in_chans, n_embd, self.patch_size, self.patch_size, key=patkey)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = jnp.array(x, dtype=float)\n",
        "        x = jax.vmap(self.proj)(x)\n",
        "        x = einops.rearrange(x, \"B C H W -> B (H W) C\")\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "###########   Parameter module    ##########\n",
        "\n",
        "class Params(eqx.Module):\n",
        "    param: jnp.ndarray\n",
        "\n",
        "    def __init__(self, num_patches, hidden_size):\n",
        "        self.param = jnp.zeros((1, num_patches, hidden_size), dtype = float)\n",
        "\n",
        "    def __call__(self):\n",
        "        return self.param\n",
        "\n",
        "\n",
        "##########    DiT   ##########\n",
        "\n",
        "\n",
        "class DiT(eqx.Module):\n",
        "    in_channels: int\n",
        "    out_channels: int\n",
        "    patch_size: int\n",
        "    n_head: int\n",
        "    t1: float\n",
        "\n",
        "    x_embedder: eqx.Module\n",
        "    t_embedder: eqx.Module\n",
        "    y_embedder: eqx.Module\n",
        "    pos_embed: eqx.Module\n",
        "    blocks: list\n",
        "    final_layer: eqx.Module\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=32,\n",
        "        patch_size=4,\n",
        "        in_channels=3,\n",
        "        hidden_size=128,\n",
        "        depth=2,  \n",
        "        n_head=4,  \n",
        "        mlp_ratio=4.0,  #fixed\n",
        "        frequency_embedding_size=256,   #fixed\n",
        "        class_dropout_prob=0.1,\n",
        "        num_classes=10,\n",
        "        *,\n",
        "        key=key,\n",
        "        \n",
        "    ):\n",
        "        xkey, tkey, flkey, embkey, *dbkeys = jr.split(key, 4 + depth)\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = in_channels\n",
        "        self.patch_size = patch_size\n",
        "        self.n_head = n_head\n",
        "        self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, key=xkey)\n",
        "        self.t_embedder = TimeStepEmbedder(hidden_size, frequency_embedding_size, key=tkey)\n",
        "        self.y_embedder = LabelEmbedder(num_classes, hidden_size, class_dropout_prob, key=embkey)\n",
        "        num_patches = self.x_embedder.num_patches\n",
        "        self.pos_embed = Params(num_patches, hidden_size)\n",
        "        self.blocks = [\n",
        "            DitBlock(\n",
        "                hidden_size, n_head, mlp_ratio, key = key\n",
        "            )\n",
        "            for dbkey in dbkeys                                   #_ in range(depth)           #*bkeys = jr.split(key, num_blocks)\n",
        "        ]\n",
        "        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels, key=flkey)\n",
        "\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed().shape[-1], int(self.x_embedder.num_patches ** 0.5))\n",
        "        pos_embed = jnp.array(pos_embed, dtype=float)\n",
        "        pos_embed = jnp.expand_dims(pos_embed, axis=0)\n",
        "        self.pos_embed = pos_embed.copy()\n",
        "\n",
        "        self.t1 = t1\n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"\n",
        "        x: (N, T, patch_size ** 2 * C)\n",
        "        imgs: (N, H, W, C)\n",
        "        \"\"\"\n",
        "        c = self.out_channels      \n",
        "        p = self.x_embedder.patch_size \n",
        "        h = w = int(x.shape[1] ** 0.5)    \n",
        "        x = jnp.reshape(x, (x.shape[0], h, w, p, p, c))\n",
        "        x = einops.rearrange(x, \"n h w p q c->n c h p w q\")\n",
        "        imgs = jnp.reshape(x, (x.shape[0], c, h * p, h * p))\n",
        "        return imgs\n",
        "    \n",
        "    def __call__(self, x, t, y):\n",
        "        #pos_embed = get_2d_sincos_pos_embed(self.pos_embed().shape[-1], int(self.x_embedder.num_patches ** 0.5))\n",
        "        \n",
        "        \n",
        "        \"\"\"\n",
        "        x: (N, C, H, W)\n",
        "        t: (N, )\n",
        "        \"\"\"\n",
        "        #t = jnp.array([t], dtype=int)\n",
        "        print(self.pos_embed)\n",
        "        t = t/self.t1\n",
        "        x = self.x_embedder(x) + self.pos_embed # (N, T, D), where T = H * W / patch_size ** 2\n",
        "        t = self.t_embedder(t)   # (N, D)\n",
        "\n",
        "        y = self.y_embedder(y)\n",
        "\n",
        "        t = t+y\n",
        "  \n",
        "        for block in self.blocks:\n",
        "            x = block(x,t)    # (N, T, D)\n",
        "        x = self.final_layer(x, t)     # (N, T, patch_size ** 2 * out_channels) - N is the batch_size, T is the number of patches, \n",
        "        x = self.unpatchify(x)        # (N, out_channels, H, W)\n",
        "        #x = jnp.squeeze(x, axis=0)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cetdJR2tbhLX"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DiT()"
      ],
      "metadata": {
        "id": "Wz3bY0xl_Fak"
      },
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = cifar()"
      ],
      "metadata": {
        "id": "1dNGRhMaALMZ"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eg = images[0:1]"
      ],
      "metadata": {
        "id": "9bZybNcECwtE"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eg.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBa0TZFUCzwW",
        "outputId": "faa30433-fed3-4aae-ba04-ad7634c8f37e"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3, 32, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "egl = labels[0:1]"
      ],
      "metadata": {
        "id": "CCahLZCMC10w"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "egl=jnp.squeeze(egl, axis=1)"
      ],
      "metadata": {
        "id": "-Ux3duoHGeCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t= jnp.array([5], dtype=float)"
      ],
      "metadata": {
        "id": "7h_1bzoODBZI"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(eg, t, egl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfAGH4fOC-_T",
        "outputId": "2756ac3f-2f7b-4893-eb31-c63667f9f38d"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 0.          0.          0.         ...  1.          1.\n",
            "    1.        ]\n",
            "  [ 0.84147096  0.68156135  0.53316844 ...  1.          1.\n",
            "    1.        ]\n",
            "  [ 0.9092974   0.99748     0.9021307  ...  1.          1.\n",
            "    1.        ]\n",
            "  ...\n",
            "  [-0.9589243  -0.5711271   0.32393527 ...  0.9999986   0.9999992\n",
            "    0.9999996 ]\n",
            "  [-0.2794155  -0.97739613 -0.23036751 ...  0.9999986   0.9999992\n",
            "    0.9999996 ]\n",
            "  [ 0.6569866  -0.8593135  -0.7137213  ...  0.9999986   0.9999992\n",
            "    0.9999996 ]]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[[[-0.34200123,  0.2966537 , -0.12871987, ...,  0.2676334 ,\n",
              "           0.40355986, -0.01633361],\n",
              "         [-0.08845013,  0.36638242, -0.11904924, ...,  0.20065154,\n",
              "          -0.37602353, -0.4027549 ],\n",
              "         [ 1.1493251 ,  0.04737014,  0.44141626, ...,  0.45272228,\n",
              "           0.21076703, -0.29202688],\n",
              "         ...,\n",
              "         [-0.5765127 ,  0.158071  , -0.5851695 , ...,  0.2035378 ,\n",
              "          -0.31120777, -0.07466888],\n",
              "         [ 1.9556712 ,  0.7470492 ,  0.2461924 , ...,  0.33781418,\n",
              "           0.76516306,  0.39535636],\n",
              "         [ 0.61308324,  0.82563716,  0.00530211, ...,  0.63807684,\n",
              "          -0.26639882,  0.6060345 ]],\n",
              "\n",
              "        [[ 0.30808845, -0.19852664,  0.31034914, ..., -0.42339617,\n",
              "           0.3716297 ,  0.05619001],\n",
              "         [ 0.14791155, -0.45064726, -0.4066413 , ..., -0.8313336 ,\n",
              "          -0.6323137 , -0.05502474],\n",
              "         [-0.39554676,  0.08149413,  0.140227  , ...,  0.33878013,\n",
              "           0.4687193 , -0.5533159 ],\n",
              "         ...,\n",
              "         [ 0.49349555, -1.372494  , -0.9543904 , ..., -0.61244994,\n",
              "          -0.47309014,  0.21614885],\n",
              "         [ 0.75802577,  0.24478875,  0.43369445, ...,  0.38557673,\n",
              "           0.20062207, -0.81469154],\n",
              "         [ 1.2401179 ,  1.1042689 ,  0.048702  , ...,  0.38587528,\n",
              "          -0.44524089,  0.345091  ]],\n",
              "\n",
              "        [[-0.01130517, -0.11776909, -0.07804798, ..., -0.3442057 ,\n",
              "          -0.01358473,  0.7564861 ],\n",
              "         [-0.16602296,  0.547506  , -0.62929785, ...,  0.50914985,\n",
              "          -1.3389558 ,  0.57822245],\n",
              "         [-0.08285464,  0.2693169 , -0.46514446, ..., -0.12795481,\n",
              "          -0.76278317,  0.12316229],\n",
              "         ...,\n",
              "         [-0.6491386 ,  0.5847462 , -1.3690126 , ...,  0.23141381,\n",
              "          -1.1828363 ,  0.6799002 ],\n",
              "         [-0.60668993, -0.12812778, -0.71364456, ..., -0.26175022,\n",
              "          -0.8292363 ,  0.11379801],\n",
              "         [ 0.44979405, -0.9030671 , -0.5812328 , ..., -0.20665672,\n",
              "          -0.32832614,  0.11370691]]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 264
        }
      ]
    }
  ]
}