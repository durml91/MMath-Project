{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8pXy9biEcmI1Vy8s1Knq4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durml91/MMath-Project/blob/duo-branch/Image_Diffusion_(working)/DiT%20Diffusion/DIT_V5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqYPElSabF4Y",
        "outputId": "0c773779-1533-4ecc-e399-f79dd5cf41c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting diffrax\n",
            "  Downloading diffrax-0.3.1-py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jax>=0.4.3 in /usr/local/lib/python3.9/dist-packages (from diffrax) (0.4.4)\n",
            "Collecting equinox>=0.10.0\n",
            "  Downloading equinox-0.10.1-py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.7/108.7 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jaxtyping>=0.2.12\n",
            "  Downloading jaxtyping-0.2.14-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.4.3->diffrax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.4.3->diffrax) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from jax>=0.4.3->diffrax) (1.22.4)\n",
            "Collecting typeguard>=2.13.3\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.12->equinox>=0.10.0->diffrax) (4.5.0)\n",
            "Installing collected packages: typeguard, jaxtyping, equinox, diffrax\n",
            "Successfully installed diffrax-0.3.1 equinox-0.10.1 jaxtyping-0.2.14 typeguard-2.13.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: equinox in /usr/local/lib/python3.9/dist-packages (0.10.1)\n",
            "Requirement already satisfied: jaxtyping>=0.2.12 in /usr/local/lib/python3.9/dist-packages (from equinox) (0.2.14)\n",
            "Requirement already satisfied: jax>=0.4.3 in /usr/local/lib/python3.9/dist-packages (from equinox) (0.4.4)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.4.3->equinox) (1.10.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.4.3->equinox) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from jax>=0.4.3->equinox) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.12->equinox) (4.5.0)\n",
            "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.12->equinox) (2.13.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optax\n",
            "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.9/dist-packages (from optax) (0.4.4)\n",
            "Collecting chex>=0.1.5\n",
            "  Downloading chex-0.1.6-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from optax) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.9/dist-packages (from optax) (4.5.0)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.9/dist-packages (from optax) (0.4.4+cuda11.cudnn82)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from chex>=0.1.5->optax) (0.12.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.9/dist-packages (from chex>=0.1.5->optax) (0.1.8)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (1.10.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Installing collected packages: chex, optax\n",
            "Successfully installed chex-0.1.6 optax-0.1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install diffrax\n",
        "!pip install equinox\n",
        "!pip install einops\n",
        "!pip install optax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import array\n",
        "import functools as ft\n",
        "import gzip\n",
        "import os\n",
        "import struct\n",
        "import urllib.request\n",
        "\n",
        "import diffrax as dfx  # https://github.com/patrick-kidger/diffrax\n",
        "import einops  # https://github.com/arogozhnikov/einops\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr\n",
        "import matplotlib.pyplot as plt\n",
        "import optax  # https://github.com/deepmind/optax\n",
        "\n",
        "import equinox as eqx"
      ],
      "metadata": {
        "id": "nkebHYG1bU4C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = jr.PRNGKey(2023)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H11C7UeVbZoD",
        "outputId": "f151b396-22e0-4568-8e08-2888dd6838b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cifar():\n",
        "  from tensorflow.keras.datasets import cifar10\n",
        "  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "  \n",
        "  set1 = jnp.array(x_train)\n",
        "  set2 = jnp.array(x_test)\n",
        "\n",
        "  data = jnp.concatenate((set1, set2))\n",
        "\n",
        "  data_reag = einops.rearrange(data, \"n h w c -> n c h w\")\n",
        "  return data_reag"
      ],
      "metadata": {
        "id": "shnETeYLbZuj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@eqx.filter_jit\n",
        "def modulate(x, shift, scale):\n",
        "    scale = jnp.expand_dims(scale, axis=1)       #scale is the value you mutliply your array by\n",
        "    shift = jnp.expand_dims(shift, axis=1)       #shift is the value you move your array b                     \n",
        "    return x * (1 + scale) + shift #1+scale to allow for zero scaling\n",
        "\n",
        "    #e.g. by inputting (x, 0 ,0) you end up with x returned\n",
        "    #this is just a very intuitive function!"
      ],
      "metadata": {
        "id": "woD2CdzWbdil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@eqx.filter_jit\n",
        "def get_2d_sincos_pos_embed(n_embd, grid_size):\n",
        "\n",
        "    \"\"\"grid_size: int value of grid height and width - we denote by (H, W)\n",
        "       return pos_embed of dim (grid_size*grid_size, n_embd)\"\"\"\n",
        "    \n",
        "\n",
        "    grid_h = jnp.arange(grid_size, dtype=float)\n",
        "    grid_w = jnp.arange(grid_size, dtype=float)\n",
        "    grid = jnp.meshgrid(grid_w, grid_h)\n",
        "    grid = jnp.stack(grid, axis=0)\n",
        "\n",
        "    grid = jnp.reshape(grid, (2, 1, grid_size, grid_size))\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(n_embd, grid)\n",
        "    return pos_embed\n",
        "\n",
        "#we basically end up with each patch embedding having a fixed n_embd dimensional sin/cos embedding vector - this is fixed! doesn't change based on sample but allows the NN to understand the \"spatial\" representation of the patches\n",
        "##################################################################################\n",
        "\n",
        "@eqx.filter_jit\n",
        "def get_2d_sincos_pos_embed_from_grid(n_embd, grid):\n",
        "    assert n_embd % 2 == 0\n",
        "\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(n_embd // 2, grid[0]) # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(n_embd // 2, grid[1]) # (H*W, D/2)\n",
        "\n",
        "    emb = jnp.concatenate([emb_h, emb_w], axis=1)  #(H*W, D)\n",
        "    return emb\n",
        "\n",
        "#if we have a grid - think two rows of however many columns - we take each row and encode and get an positional embedding for each element in the row\n",
        "\n",
        "##################################################################################\n",
        "\n",
        "@eqx.filter_jit\n",
        "def get_1d_sincos_pos_embed_from_grid(n_embd, pos):\n",
        "    \"\"\"n_embd is the output dimension for each position (D,)\n",
        "       pos is an array of positions to be encoded of size (M,)\n",
        "       size is of pos (M,D)\"\"\"\n",
        "    \n",
        "    assert n_embd % 2 == 0                \n",
        "    omega = jnp.arange(n_embd // 2, dtype=float)   #generates an array from 0 to (n_embd // 2) - 1 of integer values    \n",
        "    omega /= n_embd / 2         #divide by n_embd / 2 - can be thought of as normalising the values from to 0 to 1 i.e. uniform values\n",
        "    omega = 1. / 10000**omega #(D/2) - so n_embd is D  \n",
        "   \n",
        "    pos = jnp.array(pos)\n",
        "    out = jnp.outer(pos, omega)   #so we have pos^T * omega to make a matrix of dim (M, D/2)\n",
        "    emb_sin = jnp.sin(out)   #dim (M, D/2)\n",
        "    emb_cos = jnp.cos(out)   #dim (M, D/2)\n",
        "\n",
        "    emb = jnp.concatenate([emb_sin, emb_cos], axis=1)  #dim (M, D) - join sin and cos column wise -> <-\n",
        "    return emb\n",
        "\n",
        "\n",
        "    #if we think of the input as a sequence of words and the respective n_embedding, then we are simply adding a positional embedding across each word in the sequence "
      ],
      "metadata": {
        "id": "ZNYIT0Znbfn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "##############                ###############\n",
        "##############    DiT model   ###############\n",
        "##############                ###############\n",
        "#############################################\n",
        "\n",
        "\"\"\"Diffusion models meet Transformers!\"\"\"\n",
        "\n",
        "\n",
        "###########   Time embedding    #############\n",
        "\n",
        "\n",
        "\n",
        "###### Define silu activation ######\n",
        "\n",
        "from typing import Callable\n",
        "\n",
        "class Lambda1(eqx.Module):\n",
        "    fn: Callable\n",
        "    \n",
        "    def __call__(self, x, *, key=None):\n",
        "        return self.fn(x)\n",
        "\n",
        "###### Time embedding ######\n",
        "\n",
        "class TimeStepEmbedder(eqx.Module):\n",
        "    mlp: eqx.nn.Sequential\n",
        "    frequency_embedding_size: int\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        frequency_embedding_size,   #set as 256\n",
        "        key\n",
        "    ):\n",
        "        l1key, l2key = jr.split(key, 2)\n",
        "        self.mlp = eqx.nn.Sequential([\n",
        "            eqx.nn.Linear(frequency_embedding_size, hidden_size, key=l1key),\n",
        "            Lambda1(jax.nn.silu),\n",
        "            eqx.nn.Linear(hidden_size, hidden_size, key=l2key)\n",
        "        ])\n",
        "        self.frequency_embedding_size = frequency_embedding_size\n",
        "\n",
        "    def __call__(self, t, max_period=10000):\n",
        "        dim = self.frequency_embedding_size\n",
        "        half = dim // 2\n",
        "        freqs = jnp.exp(\n",
        "            -jnp.log(max_period) * jnp.arange(0, half, dtype=float) / half\n",
        "        )\n",
        "        args = t[:, None].astype(float) * freqs[None]\n",
        "        embedding = jnp.concatenate([jnp.cos(args), jnp.sin(args)], axis=-1)\n",
        "        if dim % 2:\n",
        "            embedding = jnp.concatenate([embedding, jnp.zeros_like(embedding[:, :1])], axis=-1)\n",
        "        t_freq = embedding\n",
        "        t_emb = jax.vmap(self.mlp)(t_freq)\n",
        "        return t_emb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########   Multi-Head Attention   #########\n",
        "\n",
        "\n",
        "class MultiAtt(eqx.Module):\n",
        "    n_embed: int\n",
        "    n_head: int\n",
        "    mha: eqx.nn.MultiheadAttention\n",
        "\n",
        "    def __init__(self, n_embed, n_head, key):\n",
        "        self.n_embed = n_embed\n",
        "        self.n_head = n_head\n",
        "        assert self.n_embed % self.n_head ==0\n",
        "        atkey, prkey, mhakey = jr.split(key, 3)\n",
        "        self.mha = eqx.nn.MultiheadAttention(num_heads=self.n_head,\n",
        "                                             query_size = self.n_embed,\n",
        "                                             key_size=self.n_embed,\n",
        "                                             value_size=self.n_embed,\n",
        "                                             output_size=self.n_embed,\n",
        "                                             use_query_bias=False,\n",
        "                                             use_key_bias=False,\n",
        "                                             use_value_bias=False,\n",
        "                                             use_output_bias=False,\n",
        "                                             key=mhakey)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        values = jax.vmap(self.mha)(x, x, x)\n",
        "        return values\n",
        "\n",
        "\n",
        "##############    DiT Block     ##############\n",
        "\n",
        "\n",
        "class DitBlock(eqx.Module):\n",
        "    norm1: eqx.nn.LayerNorm\n",
        "    norm2: eqx.nn.LayerNorm\n",
        "    attn: eqx.Module\n",
        "    Mlp: eqx.nn.MLP\n",
        "    adaLN_modulation: eqx.nn.Sequential\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        n_head,\n",
        "        mlp_ratio,   # = 4.0\n",
        "        key,\n",
        "    ):\n",
        "        mkey, adakey = jr.split(key, 2)\n",
        "        self.norm1 = eqx.nn.LayerNorm(hidden_size, eps = 1e-06, elementwise_affine=False)\n",
        "        self.attn = MultiAtt(hidden_size, n_head=n_head, key=key)\n",
        "        self.norm2 = eqx.nn.LayerNorm(hidden_size, eps = 1e-06, elementwise_affine=False)\n",
        "        mlp_hidden_size = int(hidden_size * mlp_ratio)\n",
        "        self.Mlp = eqx.nn.MLP(hidden_size, hidden_size, mlp_hidden_size, 1, key=mkey)\n",
        "        self.adaLN_modulation = eqx.nn.Sequential([\n",
        "            Lambda1(jax.nn.silu),\n",
        "            eqx.nn.Linear(hidden_size, 6 * hidden_size, key=adakey)\n",
        "        ])\n",
        "\n",
        "    def __call__(self, x, t):\n",
        "        temp = jax.vmap(self.adaLN_modulation)(t)\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = jnp.array_split(temp, 6, axis=1)\n",
        "        \n",
        "        \n",
        "        gate_msa = jnp.expand_dims(gate_msa, axis=1)\n",
        "        a = jax.vmap(self.norm1)(x)\n",
        "        tem = modulate(a, shift_msa, scale_msa)\n",
        "        x = x + gate_msa * self.attn(tem)\n",
        "        \n",
        "        \n",
        "        gate_mlp = jnp.expand_dims(gate_mlp, axis=1)\n",
        "        b = jax.vmap(self.norm2)(x)\n",
        "        tems = modulate(b, shift_mlp, scale_mlp)     \n",
        "        x = x + gate_mlp * jax.vmap(jax.vmap(self.Mlp))(tems)\n",
        "        \n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "#################   Final Layer   ################\n",
        "\n",
        "\n",
        "\n",
        "class FinalLayer(eqx.Module):\n",
        "    norm_final: eqx.nn.LayerNorm\n",
        "    linear: eqx.nn.Linear\n",
        "    #adaLN_modulation: eqx.nn.Linear\n",
        "    adaLN_modulation: eqx.nn.Sequential\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        patch_size,\n",
        "        out_channels,\n",
        "        key\n",
        "    ):\n",
        "        lkey, adakey = jr.split(key, 2)\n",
        "        self.norm_final = eqx.nn.LayerNorm(hidden_size, eps=1e-6, elementwise_affine=False)\n",
        "        self.linear = eqx.nn.Linear(hidden_size, patch_size * patch_size * out_channels, key=lkey)\n",
        "        #self.adaLN_modulation = eqx.nn.Linear(hidden_size, 2 * hidden_size, key=adakey)\n",
        "        self.adaLN_modulation = eqx.nn.Sequential([\n",
        "            Lambda1(jax.nn.silu),\n",
        "            eqx.nn.Linear(hidden_size, 2 * hidden_size, key=adakey)\n",
        "        ])\n",
        "\n",
        "    def __call__(self, x, t):\n",
        "        #t = jax.nn.silu(t)\n",
        "        temp = jax.vmap(self.adaLN_modulation)(t)\n",
        "        shift, scale = jnp.array_split(temp, 2, axis=1)\n",
        "        x = modulate(self.norm_final(x), shift, scale)\n",
        "        x = jax.vmap(jax.vmap(self.linear))(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "###########   Patch embedding   ##############\n",
        "\n",
        "class PatchEmbed(eqx.Module):\n",
        "    num_patches:int\n",
        "    proj: eqx.nn.Conv2d\n",
        "    patch_size: int\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size,\n",
        "        patch_size,\n",
        "        in_chans,\n",
        "        n_embd,\n",
        "        key\n",
        "    ):\n",
        "        patkey, _ = jr.split(key,2)\n",
        "        self.patch_size = patch_size\n",
        "        dg = img_size // self.patch_size\n",
        "        self.num_patches = dg ** 2\n",
        "        self.proj = eqx.nn.Conv2d(in_chans, n_embd, self.patch_size, self.patch_size, key=patkey)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = jnp.array(x, dtype=float)\n",
        "        x = jax.vmap(self.proj)(x)\n",
        "        x = einops.rearrange(x, \"B C H W -> B (H W) C\")\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "###########   Parameter module    ##########\n",
        "\n",
        "class Params(eqx.Module):\n",
        "    param: jnp.ndarray\n",
        "\n",
        "    def __init__(self, num_patches, hidden_size):\n",
        "        self.param = jnp.zeros((1, num_patches, hidden_size), dtype = float)\n",
        "\n",
        "    def __call__(self):\n",
        "        return self.param\n",
        "\n",
        "\n",
        "##########    DiT   ##########\n",
        "\n",
        "\n",
        "class DiT(eqx.Module):\n",
        "    in_channels: int\n",
        "    out_channels: int\n",
        "    patch_size: int\n",
        "    n_head: int\n",
        "\n",
        "    x_embedder: eqx.Module\n",
        "    t_embedder: eqx.Module\n",
        "    pos_embed: eqx.Module\n",
        "    blocks: list\n",
        "    final_layer: eqx.Module\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=28,\n",
        "        patch_size=4,\n",
        "        in_channels=1,\n",
        "        hidden_size=384,\n",
        "        depth=4,  \n",
        "        n_head=6,  \n",
        "        mlp_ratio=4.0,  #fixed\n",
        "        frequency_embedding_size=256,   #fixed\n",
        "        *,\n",
        "        key=key,\n",
        "        \n",
        "    ):\n",
        "        xkey, tkey, flkey, *dbkeys = jr.split(key, 3 + depth)\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = in_channels\n",
        "        self.patch_size = patch_size\n",
        "        self.n_head = n_head\n",
        "        self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, key=xkey)\n",
        "        self.t_embedder = TimeStepEmbedder(hidden_size, frequency_embedding_size, key=tkey)\n",
        "        num_patches = self.x_embedder.num_patches\n",
        "        self.pos_embed = Params(num_patches, hidden_size)\n",
        "        self.blocks = [\n",
        "            DitBlock(\n",
        "                hidden_size, n_head, mlp_ratio, key = key\n",
        "            )\n",
        "            for dbkey in dbkeys                                   #_ in range(depth)           #*bkeys = jr.split(key, num_blocks)\n",
        "        ]\n",
        "        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels, key=flkey)\n",
        "\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed().shape[-1], int(self.x_embedder.num_patches ** 0.5))\n",
        "        pos_embed = jnp.array(pos_embed, dtype=float)\n",
        "        pos_embed = jnp.expand_dims(pos_embed, axis=0)\n",
        "        self.pos_embed = pos_embed.copy()\n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"\n",
        "        x: (N, T, patch_size ** 2 * C)\n",
        "        imgs: (N, H, W, C)\n",
        "        \"\"\"\n",
        "        c = self.out_channels      \n",
        "        p = self.x_embedder.patch_size \n",
        "        h = w = int(x.shape[1] ** 0.5)    \n",
        "        x = jnp.reshape(x, (x.shape[0], h, w, p, p, c))\n",
        "        x = einops.rearrange(x, \"n h w p q c->n c h p w q\")\n",
        "        imgs = jnp.reshape(x, (x.shape[0], c, h * p, h * p))\n",
        "        return imgs\n",
        "    \n",
        "    def __call__(self, x, t):\n",
        "        #pos_embed = get_2d_sincos_pos_embed(self.pos_embed().shape[-1], int(self.x_embedder.num_patches ** 0.5))\n",
        "        \n",
        "        \n",
        "        \"\"\"\n",
        "        x: (N, C, H, W)\n",
        "        t: (N, )\n",
        "        \"\"\"\n",
        "        #t = jnp.array([t], dtype=int)\n",
        "\n",
        "        x = self.x_embedder(x) + self.pos_embed # (N, T, D), where T = H * W / patch_size ** 2\n",
        "        t = self.t_embedder(t)   # (N, D)\n",
        "        for block in self.blocks:\n",
        "            x = block(x,t)    # (N, T, D)\n",
        "        x = self.final_layer(x, t)     # (N, T, patch_size ** 2 * out_channels) - N is the batch_size, T is the number of patches, \n",
        "        x = self.unpatchify(x)        # (N, out_channels, H, W)\n",
        "        #x = jnp.squeeze(x, axis=0)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cetdJR2tbhLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MHA(eqx.Module):\n",
        "    num_heads: int\n",
        "    head_dim: int\n",
        "    scale: int\n",
        "\n",
        "    qkv: eqx.nn.Linear\n",
        "    o_proj: eqx.nn.Linear\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self, dim, key, num_heads=8, qkv_bias=False, qk_norm=False, ):\n",
        "        assert dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        \n",
        "        key1, key2 = jr.split(key, 2)\n",
        "        self.qkv = eqx.nn.Linear(dim, dim *3, key=key1)\n",
        "        self.o_proj = eqx.nn.Linear(dim, dim, key=key2)\n",
        "\n",
        "    def __call__(self, x, mask=None):\n",
        "        B, N, C = x.shape\n",
        "        qkv = jax.vmap(jax.vmap(self.qkv))(x)\n",
        "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.transpose(2, 0, 3, 1, 4)\n",
        "\n",
        "        q, k, v =jnp.moveaxis(qkv, 0, 0)\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn_logits = jnp.matmul(q, jnp.swapaxes(k, -2, -1))\n",
        "        if mask is not None:\n",
        "            attn_logits = jnp.where(mask == 0, -9e15, attn_logits)\n",
        "        attention = jax.nn.softmax(attn_logits, axis=-1)\n",
        "        values = jnp.matmul(attention, v)\n",
        "        values = einops.rearrange(values, \"B C N H -> B N (C H)\")\n",
        "\n",
        "        values = jax.vmap(jax.vmap(self.o_proj))(values)\n",
        "        return values"
      ],
      "metadata": {
        "id": "5YGMUxmGbhuN"
      },
      "execution_count": 114,
      "outputs": []
    }
  ]
}