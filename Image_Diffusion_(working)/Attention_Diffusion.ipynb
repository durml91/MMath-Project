{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlWL2Vbn9rHJZAlO+Dk+GY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durml91/MMath-Project/blob/duo-branch/Image_Diffusion_(working)/Attention_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB7c1s9yAoVp",
        "outputId": "8989576a-4151-400d-f333-455db34d9e48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Collecting jaxlib==0.4.2+cuda11.cudnn82\n",
            "  Downloading https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.4.2%2Bcuda11.cudnn82-cp38-cp38-manylinux2014_x86_64.whl (164.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.4/164.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jaxlib==0.4.2+cuda11.cudnn82) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jaxlib==0.4.2+cuda11.cudnn82) (1.7.3)\n",
            "Installing collected packages: jaxlib\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.3.25+cuda11.cudnn805\n",
            "    Uninstalling jaxlib-0.3.25+cuda11.cudnn805:\n",
            "      Successfully uninstalled jaxlib-0.3.25+cuda11.cudnn805\n",
            "Successfully installed jaxlib-0.4.2+cuda11.cudnn82\n"
          ]
        }
      ],
      "source": [
        "!pip install jaxlib==0.4.2+cuda11.cudnn82 -f  https://storage.googleapis.com/jax-releases/jax_cuda_releases.html # [cuda]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffrax\n",
        "!pip install equinox\n",
        "!pip install einops\n",
        "!pip install optax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAhw7yT6AvfB",
        "outputId": "6d4a74ac-ba2b-4ee7-b0e5-737dd8fed0cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting diffrax\n",
            "  Downloading diffrax-0.3.1-py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jax>=0.4.3\n",
            "  Downloading jax-0.4.4.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting equinox>=0.10.0\n",
            "  Downloading equinox-0.10.1-py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.7/108.7 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jaxtyping>=0.2.12\n",
            "  Downloading jaxtyping-0.2.12-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jax>=0.4.3->diffrax) (1.22.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.4.3->diffrax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.4.3->diffrax) (1.7.3)\n",
            "Collecting typeguard>=2.13.3\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.8/dist-packages (from jaxtyping>=0.2.12->equinox>=0.10.0->diffrax) (4.5.0)\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.4-py3-none-any.whl size=1403844 sha256=fc39d3411f42499ff2440ab261dc46bcf1abdb29b0478ab219eb2b8d8a3ae11f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/cd/9b/750eb95db5b18b776ae59f55ae22b91a01e3703f3fb07eaa13\n",
            "Successfully built jax\n",
            "Installing collected packages: typeguard, jaxtyping, jax, equinox, diffrax\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 2.7.1\n",
            "    Uninstalling typeguard-2.7.1:\n",
            "      Successfully uninstalled typeguard-2.7.1\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.3.25\n",
            "    Uninstalling jax-0.3.25:\n",
            "      Successfully uninstalled jax-0.3.25\n",
            "Successfully installed diffrax-0.3.1 equinox-0.10.1 jax-0.4.4 jaxtyping-0.2.12 typeguard-2.13.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: equinox in /usr/local/lib/python3.8/dist-packages (0.10.1)\n",
            "Requirement already satisfied: jaxtyping>=0.2.12 in /usr/local/lib/python3.8/dist-packages (from equinox) (0.2.12)\n",
            "Requirement already satisfied: jax>=0.4.3 in /usr/local/lib/python3.8/dist-packages (from equinox) (0.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jax>=0.4.3->equinox) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.4.3->equinox) (1.7.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.4.3->equinox) (3.3.0)\n",
            "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.8/dist-packages (from jaxtyping>=0.2.12->equinox) (2.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.8/dist-packages (from jaxtyping>=0.2.12->equinox) (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optax\n",
            "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chex>=0.1.5\n",
            "  Downloading chex-0.1.6-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.8/dist-packages (from optax) (4.5.0)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.8/dist-packages (from optax) (0.4.2+cuda11.cudnn82)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from optax) (1.22.4)\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.8/dist-packages (from optax) (0.4.4)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax) (0.12.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.1.55->optax) (1.7.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Installing collected packages: chex, optax\n",
            "Successfully installed chex-0.1.6 optax-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import array\n",
        "import functools as ft\n",
        "import gzip\n",
        "import os\n",
        "import struct\n",
        "import urllib.request\n",
        "\n",
        "import diffrax as dfx  # https://github.com/patrick-kidger/diffrax\n",
        "import einops  # https://github.com/arogozhnikov/einops\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr\n",
        "import matplotlib.pyplot as plt\n",
        "import optax  # https://github.com/deepmind/optax\n",
        "\n",
        "import equinox as eqx"
      ],
      "metadata": {
        "id": "E1AYxrBfAxC6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFbT8W33BUPS",
        "outputId": "06ac6304-c303-40cb-d7ff-72228010875e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil"
      ],
      "metadata": {
        "id": "nrnW1wkMCJae"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class qkv(eqx.Module):\n",
        "#     c_attn: eqx.nn.Linear\n",
        "\n",
        "\n",
        "#     def __init__(self, key):\n",
        "#         self.c_attn = eqx.nn.Linear(64, 3*64, key=key)\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         #x = einops.rearrange(x, \"a b -> b a\")\n",
        "        \n",
        "#         #T,C = x.shape\n",
        "#         print(\"hi\")\n",
        "#         qkv = jax.vmap(self.c_attn)(x)\n",
        "#         #qkv = qkv.transpose(1,0)\n",
        "#         #print(qkv.shape)\n",
        "#         q,k,v = jnp.array_split(qkv, 3, axis=-1)\n",
        "#         print(q.shape,k.shape,v.shape)\n",
        "#         # q = einops.rearrange(q, \"a b->b a\")\n",
        "#         # k = einops.rearrange(k, \"a b->b a\")\n",
        "#         # v = einops.rearrange(v, \"a b->b a\")\n",
        "\n",
        "        \n",
        "        \n",
        "#         # #print(x.shape)\n",
        "#         # W_q = self.wq(x)\n",
        "#         # #print(W_q.shape)\n",
        "#         # W_k = self.wk(x)\n",
        "#         # W_v = self.wv(x)\n",
        "#         # Q = jnp.matmul(x, W_q)\n",
        "#         # K = jnp.matmul(x, W_k)\n",
        "#         # V = jnp.matmul(x, W_v)\n",
        "\n",
        "#         return q,k,v\n",
        "\n",
        "\n",
        "# class Attblock(eqx.Module):\n",
        "#     matrix: eqx.Module\n",
        "\n",
        "#     c_proj: eqx.nn.Linear\n",
        "    \n",
        "#     att: eqx.nn.MultiheadAttention\n",
        "\n",
        "#     def __init__(self, key):\n",
        "\n",
        "#         self.att = eqx.nn.MultiheadAttention(7, query_size=64, key=key)\n",
        "#         self.matrix = qkv(key=key)\n",
        "#         self.c_proj = eqx.nn.Linear(64,64, key=key)\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         Q,K,V = self.matrix(x)\n",
        "        \n",
        "#         at=self.att(Q,K,V)\n",
        "#         at = self.c_proj(at)\n",
        "#         return at"
      ],
      "metadata": {
        "id": "PwkZyojpwC-4"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class qkv(eqx.Module):\n",
        "#     c_attn: eqx.nn.Linear\n",
        "\n",
        "\n",
        "#     def __init__(self, key):\n",
        "#         self.c_attn = eqx.nn.Linear(66, 3*66, key=key)\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         x = einops.rearrange(x, \"a b -> b a\")\n",
        "        \n",
        "#         #T,C = x.shape\n",
        "\n",
        "#         qkv = jax.vmap(self.c_attn)(x)\n",
        "#         #qkv = qkv.transpose(1,0)\n",
        "#         #print(qkv.shape)\n",
        "#         q,k,v = jnp.array_split(qkv, 3, axis=-1)\n",
        "#         print(q.shape,k.shape,v.shape)\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "#         # #print(x.shape)\n",
        "#         # W_q = self.wq(x)\n",
        "#         # #print(W_q.shape)\n",
        "#         # W_k = self.wk(x)\n",
        "#         # W_v = self.wv(x)\n",
        "#         # Q = jnp.matmul(x, W_q)\n",
        "#         # K = jnp.matmul(x, W_k)\n",
        "#         # V = jnp.matmul(x, W_v)\n",
        "\n",
        "#         return q,k,v\n",
        "\n",
        "\n",
        "# class Attblock(eqx.Module):\n",
        "#     matrix: eqx.Module\n",
        "\n",
        "#     c_proj: eqx.nn.Linear\n",
        "    \n",
        "#     att: eqx.nn.MultiheadAttention\n",
        "\n",
        "#     def __init__(self, key):\n",
        "\n",
        "#         self.att = eqx.nn.MultiheadAttention(7, query_size=66, key=key)\n",
        "#         self.matrix = qkv(key=key)\n",
        "#         self.c_proj = eqx.nn.Linear(66,66, key=key)\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         Q,K,V = self.matrix(x)\n",
        "        \n",
        "#         at=self.att(Q,K,V)\n",
        "#         at = self.c_proj(at)\n",
        "#         return at\n",
        "\n",
        "\n",
        "\n",
        "# class SinusoidalEmbedding(eqx.Module):\n",
        "#     d:int = 28\n",
        "\n",
        "\n",
        "#     def __init__(self, d):\n",
        "#         self.d=d\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#       half_dim = self.d//2\n",
        "#       emb=jnp.log(10000) / (half_dim - 1)\n",
        "#       emb = jnp.exp(jnp.arange(half_dim)* -emb)\n",
        "#       emb = x[:,None] * emb[None,:]\n",
        "#       emb = jnp.concatenate([jnp.sin(emb), jnp.cos(emb)], -1)\n",
        "#       return emb\n",
        "\n",
        "\n",
        "# class TimeEmbed(eqx.Module):\n",
        "#     dim: int =28\n",
        "#     embed: eqx.nn.Embedding\n",
        "#     sin: eqx.Module\n",
        "    \n",
        "#     def __init__(self, key):\n",
        "#         self.embed = eqx.nn.Embedding(1000, 28, key=key)\n",
        "#         self.sin = SinusoidalEmbedding(28)\n",
        "  \n",
        "#     def __call__(self, x):\n",
        "#         se = self.sin(x)\n",
        "#         se = (se*10000).astype(int)\n",
        "#         emb = self.embed(se)\n",
        "#         return emb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class MixerBlock(eqx.Module):\n",
        "#     patch_mixer: eqx.nn.MLP\n",
        "#     hidden_mixer: eqx.nn.MLP\n",
        "#     norm1: eqx.nn.LayerNorm\n",
        "#     norm2: eqx.nn.LayerNorm\n",
        "\n",
        "#     Att1: eqx.Module\n",
        "#     Att2: eqx.Module\n",
        "#     gnorm1: eqx.nn.GroupNorm\n",
        "#     gnorm2: eqx.nn.GroupNorm\n",
        "\n",
        "\n",
        "#     def __init__(\n",
        "#         self, num_patches, hidden_size, mix_patch_size, mix_hidden_size, *, key\n",
        "#     ):\n",
        "#         tkey, ckey,a1key, a2key = jr.split(key, 4)\n",
        "#         self.patch_mixer = eqx.nn.MLP(\n",
        "#             num_patches, num_patches, mix_patch_size, depth=1, key=tkey\n",
        "#         )\n",
        "#         self.hidden_mixer = eqx.nn.MLP(\n",
        "#             hidden_size, hidden_size, mix_hidden_size, depth=1, key=ckey\n",
        "#         )\n",
        "#         self.norm1 = eqx.nn.LayerNorm((hidden_size, num_patches))\n",
        "#         self.norm2 = eqx.nn.LayerNorm((num_patches, hidden_size))\n",
        "\n",
        "#         self.Att1 = Attblock(key = a1key)\n",
        "#         self.Att2 = Attblock(key = a2key)\n",
        "\n",
        "#         self.gnorm1 = eqx.nn.GroupNorm(4,32)\n",
        "#         self.gnorm2 = eqx.nn.GroupNorm(4,32)\n",
        "\n",
        "#     def __call__(self, y):\n",
        "#         y = y + jax.vmap(self.patch_mixer)(self.norm1(y))\n",
        "#         unatt = self.Att1(y)\n",
        "#         gnormatt = self.gnorm1(unatt)\n",
        "#         y = y + gnormatt\n",
        "        \n",
        "#         y = einops.rearrange(y, \"c p -> p c\")\n",
        "#         y = y + jax.vmap(self.hidden_mixer)(self.norm2(y))\n",
        "#         y = einops.rearrange(y, \"p c -> c p\")\n",
        "        \n",
        "#         unatt = self.Att2(y)\n",
        "#         gnormatt = self.gnorm2(unatt)\n",
        "#         y = y + gnormatt\n",
        "#         return y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class Mixer2d(eqx.Module):\n",
        "#     conv_in: eqx.nn.Conv2d\n",
        "#     conv_out: eqx.nn.ConvTranspose2d\n",
        "#     blocks: list\n",
        "#     norm: eqx.nn.LayerNorm\n",
        "#     t1: float\n",
        "#     timemb: eqx.Module\n",
        "\n",
        "\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         img_size,\n",
        "#         patch_size,\n",
        "#         hidden_size,\n",
        "#         mix_patch_size,\n",
        "#         mix_hidden_size,\n",
        "#         num_blocks,\n",
        "#         t1,\n",
        "#         *,\n",
        "#         key,\n",
        "#     ):\n",
        "#         input_size, height, width = img_size\n",
        "#         assert (height % patch_size) == 0\n",
        "#         assert (width % patch_size) == 0\n",
        "#         num_patches = (height // patch_size) * (width // patch_size)\n",
        "        \n",
        "#         inkey, outkey,tkey, *bkeys = jr.split(key, 3 + num_blocks)\n",
        "\n",
        "#         self.conv_in = eqx.nn.Conv2d(\n",
        "#             input_size + 1, hidden_size, patch_size, stride=patch_size, key=inkey\n",
        "#         )\n",
        "#         self.conv_out = eqx.nn.ConvTranspose2d(\n",
        "#             hidden_size, input_size, patch_size, stride=patch_size, key=outkey\n",
        "#         )\n",
        "#         self.blocks = [\n",
        "#             MixerBlock(\n",
        "#                 num_patches, hidden_size, mix_patch_size, mix_hidden_size, key=bkey\n",
        "#             )\n",
        "#             for bkey in bkeys\n",
        "#         ]\n",
        "#         self.norm = eqx.nn.LayerNorm((hidden_size, num_patches))\n",
        "#         self.t1 = t1\n",
        "#         self.timemb = TimeEmbed(key=tkey)\n",
        "\n",
        "#     def __call__(self, t, y):\n",
        "#         t = t / self.t1\n",
        "#         t = einops.repeat(t, \"-> 1\")\n",
        "#         t = jnp.array([t], dtype=int)\n",
        "#         t = self.timemb(t)\n",
        "#         t = einops.rearrange(t, \" c 1 h w-> c h w\")\n",
        "#         y = jnp.concatenate([y, t])\n",
        "#         y = self.conv_in(y)\n",
        "#         _, patch_height, patch_width = y.shape\n",
        "#         y = einops.rearrange(y, \"c h w -> c (h w)\")\n",
        "#         for block in self.blocks:\n",
        "#             y = block(y)\n",
        "#         y = self.norm(y)\n",
        "#         y = einops.rearrange(y, \"c (h w) -> c h w\", h=patch_height, w=patch_width)\n",
        "#         return self.conv_out(y)"
      ],
      "metadata": {
        "id": "MP-0jr4XA8Y5"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class qkv(eqx.Module):\n",
        "#     wk: eqx.nn.Linear\n",
        "#     wq: eqx.nn.Linear\n",
        "#     wv: eqx.nn.Linear\n",
        "\n",
        "#     def __init__(self, key):\n",
        "#         self.wq = eqx.nn.Linear(49, 128, key=key)\n",
        "#         self.wk = eqx.nn.Linear(49, 128, key=key)\n",
        "#         self.wv = eqx.nn.Linear(49, 128, key=key)\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         W_q = jax.vmap(self.wq)(x)\n",
        "#         W_k = jax.vmap(self.wk)(x)\n",
        "#         W_v = jax.vmap(self.wv)(x)\n",
        "        \n",
        "#         print(W_q.shape, x.shape)\n",
        "#         Q = jax.lax.batch_matmul(x, W_q)\n",
        "#         K = jax.lax.batch_matmul(x, W_k)\n",
        "#         V = jax.lax.batch_matmul(x, W_v)\n",
        "#         return Q,K,V\n",
        "\n",
        "\n",
        "# class Attblock(eqx.Module):\n",
        "#     matrix: eqx.Module\n",
        "    \n",
        "#     att: eqx.nn.MultiheadAttention\n",
        "\n",
        "#     def __init__(self, key):\n",
        "\n",
        "#         self.att = eqx.nn.MultiheadAttention(7, query_size=49, key=key)\n",
        "#         self.matrix = qkv(key=key)\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         Q,K,V = self.matrix(x)\n",
        "        \n",
        "#         at=self.att(Q,K,V)\n",
        "#         return at\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class MixerBlock(eqx.Module):\n",
        "#     patch_mixer: eqx.nn.MLP\n",
        "#     hidden_mixer: eqx.nn.MLP\n",
        "#     norm1: eqx.nn.LayerNorm\n",
        "#     norm2: eqx.nn.LayerNorm\n",
        "\n",
        "#     Att1: eqx.Module\n",
        "#     Att2: eqx.Module\n",
        "#     gnorm1: eqx.nn.GroupNorm\n",
        "#     gnorm2: eqx.nn.GroupNorm\n",
        "\n",
        "\n",
        "#     def __init__(\n",
        "#         self, num_patches, hidden_size, mix_patch_size, mix_hidden_size, *, key\n",
        "#     ):\n",
        "#         tkey, ckey,a1key, a2key = jr.split(key, 4)\n",
        "#         self.patch_mixer = eqx.nn.MLP(\n",
        "#             num_patches, num_patches, mix_patch_size, depth=1, key=tkey\n",
        "#         )\n",
        "#         self.hidden_mixer = eqx.nn.MLP(\n",
        "#             hidden_size, hidden_size, mix_hidden_size, depth=1, key=ckey\n",
        "#         )\n",
        "#         self.norm1 = eqx.nn.LayerNorm((hidden_size, num_patches))\n",
        "#         self.norm2 = eqx.nn.LayerNorm((num_patches, hidden_size))\n",
        "\n",
        "#         self.Att1 = Attblock(key = a1key)\n",
        "#         self.Att2 = Attblock(key = a2key)\n",
        "\n",
        "#         self.gnorm1 = eqx.nn.GroupNorm(4,32)\n",
        "#         self.gnorm2 = eqx.nn.GroupNorm(4,32)\n",
        "\n",
        "#     def __call__(self, y):\n",
        "#         y = y + jax.vmap(self.patch_mixer)(self.norm1(y))\n",
        "#         unatt = self.Att1(y)\n",
        "#         gnormatt = self.gnorm1(unatt)\n",
        "#         y = y + gnormatt\n",
        "        \n",
        "#         y = einops.rearrange(y, \"c p -> p c\")\n",
        "#         y = y + jax.vmap(self.hidden_mixer)(self.norm2(y))\n",
        "#         y = einops.rearrange(y, \"p c -> c p\")\n",
        "        \n",
        "#         unatt = self.Att2(y)\n",
        "#         gnormatt = self.gnorm2(unatt)\n",
        "#         y = y + gnormatt\n",
        "#         return y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class Mixer2d(eqx.Module):\n",
        "#     conv_in: eqx.nn.Conv2d\n",
        "#     conv_out: eqx.nn.ConvTranspose2d\n",
        "#     blocks: list\n",
        "#     norm: eqx.nn.LayerNorm\n",
        "#     t1: float\n",
        "#     tim_emb: eqx.nn.Embedding\n",
        "\n",
        "\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         img_size,\n",
        "#         patch_size,\n",
        "#         hidden_size,\n",
        "#         mix_patch_size,\n",
        "#         mix_hidden_size,\n",
        "#         num_blocks,\n",
        "#         t1,\n",
        "#         *,\n",
        "#         key,\n",
        "#     ):\n",
        "#         input_size, height, width = img_size\n",
        "#         assert (height % patch_size) == 0\n",
        "#         assert (width % patch_size) == 0\n",
        "#         num_patches = (height // patch_size) * (width // patch_size)\n",
        "        \n",
        "#         inkey, outkey,tkey, *bkeys = jr.split(key, 3 + num_blocks)\n",
        "\n",
        "#         self.conv_in = eqx.nn.Conv2d(\n",
        "#             input_size + 1, hidden_size, patch_size, stride=patch_size, key=inkey\n",
        "#         )\n",
        "#         self.conv_out = eqx.nn.ConvTranspose2d(\n",
        "#             hidden_size, input_size, patch_size, stride=patch_size, key=outkey\n",
        "#         )\n",
        "#         self.blocks = [\n",
        "#             MixerBlock(\n",
        "#                 num_patches, hidden_size, mix_patch_size, mix_hidden_size, key=bkey\n",
        "#             )\n",
        "#             for bkey in bkeys\n",
        "#         ]\n",
        "#         self.norm = eqx.nn.LayerNorm((hidden_size, num_patches))\n",
        "#         self.t1 = t1\n",
        "#         self.tim_emb = eqx.nn.Embedding(28, 28, key=tkey)\n",
        "\n",
        "#     def __call__(self, t, y):\n",
        "#         t = t / self.t1\n",
        "#         t = einops.repeat(t, \"-> 1\")\n",
        "#         t = jnp.array([t], dtype=int)\n",
        "#         t = self.tim_emb(t)\n",
        "        \n",
        "#         #_, height, width = y.shape\n",
        "#         t = einops.repeat(t, \"c h w-> c (28 h) w\")\n",
        "#         y = jnp.concatenate([y, t])\n",
        "#         y = self.conv_in(y)\n",
        "#         _, patch_height, patch_width = y.shape\n",
        "#         y = einops.rearrange(y, \"c h w -> c (h w)\")\n",
        "#         for block in self.blocks:\n",
        "#             y = block(y)\n",
        "#         y = self.norm(y)\n",
        "#         y = einops.rearrange(y, \"c (h w) -> c h w\", h=patch_height, w=patch_width)\n",
        "#         return self.conv_out(y)"
      ],
      "metadata": {
        "id": "PWQ1ceeiD5BH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "MxSYycHl2mpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time embedding"
      ],
      "metadata": {
        "id": "boKbD65w2odj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable"
      ],
      "metadata": {
        "id": "hLcneMNhBhII"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = jr.PRNGKey(2023)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CDF6Fp0CL4J",
        "outputId": "146ad69c-a731-4991-c19f-fb4cdc93de0b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dit\n",
        "\n",
        "#Embedding Layer of Timesteps\n",
        "\n",
        "class Lambda1(eqx.Module):\n",
        "    fn: Callable\n",
        "\n",
        "    def __call__(self , x, *, key=None):\n",
        "        return self.fn(x)\n",
        "\n",
        "\n",
        "\n",
        "class TimeStepEmbedder(eqx.Module):\n",
        "    mlp: eqx.nn.Sequential\n",
        "    frequency_embedding_size: int\n",
        "    \n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        frequency_embedding_size,   #set as 256\n",
        "        key\n",
        "    ):\n",
        "\n",
        "        self.mlp = eqx.nn.Sequential([\n",
        "            eqx.nn.Linear(frequency_embedding_size, hidden_size, key=key),\n",
        "            Lambda1(jax.nn.silu),\n",
        "            eqx.nn.Linear(hidden_size, hidden_size, key=key)\n",
        "        ])\n",
        "        self.frequency_embedding_size = frequency_embedding_size\n",
        "\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        t,\n",
        "        max_period=10000\n",
        "    ):\n",
        "        dim = self.frequency_embedding_size\n",
        "        half = dim // 2\n",
        "        freqs = jnp.exp(\n",
        "            -jnp.log(max_period) * jnp.arange(0, half, dtype=float) / half\n",
        "        )\n",
        "        args = t[:, None].astype(float) * freqs[None]\n",
        "        embedding = jnp.concatenate([jnp.cos(args), jnp.sin(args)], axis=-1)\n",
        "        if dim % 2:\n",
        "            embedding = jnp.concatenate([embedding, jnp.zeros_like(embedding[:, :1])], axis=-1)\n",
        "        t_freq = embedding\n",
        "        t_emb = jax.vmap(self.mlp)(t_freq)\n",
        "        return t_emb      "
      ],
      "metadata": {
        "id": "WY2uZhp89Ylu"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Time embed test\n",
        "TimeStepEmbedder(32, 256, key)(jnp.array([4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znitHQVPFd9R",
        "outputId": "b575bd34-69a5-47fe-b844-2cdb96e42dfa"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[ 0.3052112 ,  0.19230524,  0.14764252,  0.31935525, -0.06504469,\n",
              "        -0.23124412, -0.36936757,  0.02332152,  0.14454725, -0.18957621,\n",
              "         0.20766026,  0.00606484,  0.03791146, -0.04720105, -0.12594688,\n",
              "        -0.15844625, -0.09823439,  0.18520194, -0.12404346, -0.09638146,\n",
              "        -0.04518805,  0.06181648, -0.0097344 , -0.07609019, -0.00168262,\n",
              "         0.21857136, -0.17319855,  0.19038439, -0.12791844, -0.2365416 ,\n",
              "         0.14090869,  0.08538287]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multihead Attention"
      ],
      "metadata": {
        "id": "XYdPvNIg2q6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## attention\n",
        "\n",
        "class MultiAtt(eqx.Module):\n",
        "    c_attn: eqx.nn.Linear\n",
        "    c_proj: eqx.nn.Linear\n",
        "    n_head: int\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_embd,\n",
        "        n_head,\n",
        "        key\n",
        "    ):\n",
        "        assert n_embd % n_head ==0\n",
        "        self.c_attn = eqx.nn.Linear(n_embd, 3 * n_embd, key=key)\n",
        "        self.c_proj = eqx.nn.Linear(n_embd, n_embd, key=key)\n",
        "        self.n_head = n_head\n",
        "        #self.mha = eqx.nn.MultiheadAttention(n_head, )\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        x,\n",
        "        mask=None\n",
        "    ):\n",
        "        B, T, C = x.shape\n",
        "        qkv = jax.vmap(jax.vmap(self.c_attn))(x)\n",
        "        qkv = qkv.reshape(B, T, self.n_head, -1)\n",
        "        qkv = jnp.transpose(qkv, (0, 2, 1, 3)) #[batch, number of heads, T or seq_length, embed_dim or C(channels)]\n",
        "        q, k, v = jnp.array_split(qkv, 3, axis=-1)\n",
        "        new_k = jnp.swapaxes(k, -2,-1) *(1.0 / jnp.sqrt(k.shape[-1]))\n",
        "        att = jax.lax.batch_matmul(q, new_k)\n",
        "        if mask is not None:\n",
        "            attn = jnp.where(mask == 0, -9e15, att)\n",
        "        attention = jax.nn.softmax(att, axis=-1)\n",
        "        values = jax.lax.batch_matmul(attention, v)\n",
        "        values = jnp.transpose(values, (0,2,1,3)) # [batch, T, n_heads, C]\n",
        "        values = values.reshape(B, T, C)\n",
        "        o = jax.vmap(jax.vmap(self.c_proj))(values)\n",
        "        return o, attention"
      ],
      "metadata": {
        "id": "AbYAYrUaKuBf"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = jr.normal(key, (3, 16, 128))\n",
        "print(x.shape)\n",
        "mh_attn = MultiAtt(n_embd=128, n_head=4, key=key)\n",
        "output, attention = mh_attn(x)\n",
        "print(output.shape, attention.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVdzNdX5UcA_",
        "outputId": "332e23ae-117e-47d2-fdb8-54cf1b4cb956"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 16, 128)\n",
            "(3, 16, 128) (3, 4, 16, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### \n",
        "\n",
        "class Lambda2(eqx.Module):\n",
        "    fn: Callable\n",
        "\n",
        "    def __call__(self , x, *, key=None):\n",
        "        return self.fn(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "GO5POK3zsOTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "utility functions"
      ],
      "metadata": {
        "id": "aH_B8CT82uLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modulate(x, shift, scale):\n",
        "    scale = einops.rearrange(scale, \"a -> a 1\")\n",
        "    shift = einops.rearrange(shift, \"a -> a 1\")\n",
        "    \n",
        "    return x * (1 + scale) + shift"
      ],
      "metadata": {
        "id": "7drPPWj5vjY1"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "w = jr.normal(key, (4,8,32))\n",
        "q=jnp.arange(0,8)\n",
        "p=jnp.arange(0,8)"
      ],
      "metadata": {
        "id": "9qKkIRQZxkD6"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DiT block"
      ],
      "metadata": {
        "id": "YuPSPusZ2v3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DitBlock\n",
        "\n",
        "class DitBlock(eqx.Module):\n",
        "    norm1: eqx.nn.LayerNorm\n",
        "    norm2: eqx.nn.LayerNorm\n",
        "    attn: eqx.Module\n",
        "    Mlp: eqx.nn.MLP\n",
        "    adaLN_modulation = eqx.nn.Sequential\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        n_head,\n",
        "        mlp_ratio=4.0\n",
        "        key=key,\n",
        "    ):\n",
        "        self.norm1 = eqx.nn.LayerNorm(hidden_size, eps = 1e-06, elementwise_affine=False)\n",
        "        self.attn = MultiAtt(hidden_size, n_head=n_head, key=key)\n",
        "        self.norm2 = eqx.nn.LayerNorm(hidden_size, eps = 1e-06, elementwise_affine=False)\n",
        "        mlp_hidden_size = int(hidden_size * mlp_ratio)\n",
        "        self.Mlp = eqx.nn.MLP(hidden_size, hidden_size, mlp_hidden_size, 1, key=key)\n",
        "        self.adaLN_modulation = eqx.nn.Sequential([\n",
        "            Lambda1(jax.nn.silu),\n",
        "            eqx.nn.Linear(hidden_size, 6 * hidden_size, key=key)\n",
        "        ])\n",
        "\n",
        "    def __call(\n",
        "        self,\n",
        "        x,\n",
        "        c\n",
        "    ):\n",
        "        temp = self.adaLN_modulation(c)\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = jnp.array_split(temp, 6, axis=1)\n",
        "        gate_msa = einops.rearrange(gate_msa, \"a -> a 1\")\n",
        "        x = x + gate_msa * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))\n",
        "        gate_mlp = einops.rearrange(gate_mlp, \"a -> a 1\")\n",
        "        x = x + gate_mlp * self.Mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "32SnHbSUHTsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jr.normal(key, (2,2,2))"
      ],
      "metadata": {
        "id": "S-bZSVc40fsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eqx.nn.MLP(5,16, 20, 3, jax.nn.gelu, key=key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TggtYnHhtRzC",
        "outputId": "6ff5aa0f-6e4b-47df-c312-72a7b76d0567"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  layers=(\n",
              "    Linear(\n",
              "      weight=f32[20,5],\n",
              "      bias=f32[20],\n",
              "      in_features=5,\n",
              "      out_features=20,\n",
              "      use_bias=True\n",
              "    ),\n",
              "    Linear(\n",
              "      weight=f32[20,20],\n",
              "      bias=f32[20],\n",
              "      in_features=20,\n",
              "      out_features=20,\n",
              "      use_bias=True\n",
              "    ),\n",
              "    Linear(\n",
              "      weight=f32[20,20],\n",
              "      bias=f32[20],\n",
              "      in_features=20,\n",
              "      out_features=20,\n",
              "      use_bias=True\n",
              "    ),\n",
              "    Linear(\n",
              "      weight=f32[16,20],\n",
              "      bias=f32[16],\n",
              "      in_features=20,\n",
              "      out_features=16,\n",
              "      use_bias=True\n",
              "    )\n",
              "  ),\n",
              "  activation=<function gelu>,\n",
              "  final_activation=<function _identity>,\n",
              "  in_size=5,\n",
              "  out_size=16,\n",
              "  width_size=20,\n",
              "  depth=3\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Layer"
      ],
      "metadata": {
        "id": "5C5kiQ_k2x7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## FinalLayer\n",
        "\n",
        "class FinalLayer(eqx.Module):\n",
        "    norm_final: eqx.nn.LayerNorm\n",
        "    linear: eqx.nn.Linear\n",
        "    adaLN_modulation = eqx.nn.Sequential\n",
        "    \n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        patch_size,\n",
        "        out_channels,\n",
        "        key=key\n",
        "    ):\n",
        "\n",
        "        self.norm_final = eqx.nn.LayerNorm(hidden_size, pes=1e-6, elementwise_affine=False)\n",
        "        self.linear = eqx.nn.Linear(hidden_size, patch_size * patch_size * out_channels, key=key)\n",
        "        self.adaLN_modulation = eqx.nn.Sequential([\n",
        "            Lambda1(jax.nn.silu),\n",
        "            eqx.nn.Linear(hidden_size, 2 * hidden_size, key=key)\n",
        "        ])\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        x,\n",
        "        c\n",
        "    ):\n",
        "        temp = self.adaLN_modulation(c)\n",
        "        shift, scale = jnp.array_split(temp, 2, axis=1)\n",
        "        x = modulate(self.norm_final(x), shift, scale)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Dumrxptn2Bno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PatchEmbed"
      ],
      "metadata": {
        "id": "ROGcZLaJ4zOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbed(eqx.Module):\n",
        "    grid_size:int\n",
        "    num_patches:int\n",
        "    patch_size: int\n",
        "    img_size: int\n",
        "    proj: eqx.nn.Conv2d\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size,\n",
        "        patch_size,\n",
        "        in_chans,\n",
        "        n_embd,\n",
        "        norm_layer,\n",
        "        key=key\n",
        "    ):\n",
        "        dg = img_size // patch_size\n",
        "        self.grid_size = (dg, dg)\n",
        "        self.num_patches = self.grid_size ** 2\n",
        "\n",
        "        self.proj = eqx.nn.Conv2d(in_chans, n_embd, patch_size, patch_size, key=key)\n",
        "\n",
        "    def __call(\n",
        "        self,\n",
        "        x\n",
        "    ):\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        x = self.proj(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "zDorl8vT40t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility"
      ],
      "metadata": {
        "id": "udhjg0J-9pK0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g0XXP0eZ9qwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DiT"
      ],
      "metadata": {
        "id": "00hqaJ8r2zq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import repeat\n",
        "import collections.abc\n",
        "import torch"
      ],
      "metadata": {
        "id": "GwAmTqD07Gvo"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# From PyTorch internals\n",
        "def _ntuple(n):\n",
        "    def parse(x):\n",
        "        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n",
        "            return tuple(x)\n",
        "        return tuple(repeat(x, n))\n",
        "    return parse"
      ],
      "metadata": {
        "id": "-2Rb-20y7D2Z"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ntuple(2)(16)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8FMe8Bx7M_x",
        "outputId": "772c7101-bc77-44e3-9c27-49e36a96848c"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rest of code"
      ],
      "metadata": {
        "id": "rDm2oF452jQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def single_loss_fn(model, weight, int_beta, data, t, key):\n",
        "    mean = data * jnp.exp(-0.5 * int_beta(t))\n",
        "    var = jnp.maximum(1 - jnp.exp(-int_beta(t)), 1e-5)\n",
        "    std = jnp.sqrt(var)\n",
        "    noise = jr.normal(key, data.shape)\n",
        "    y = mean + std * noise\n",
        "    pred = model(t, y)\n",
        "    return weight(t) * jnp.mean((pred + noise / std) ** 2)\n",
        "\n",
        "\n",
        "def batch_loss_fn(model, weight, int_beta, data, t1, key):\n",
        "    batch_size = data.shape[0]\n",
        "    tkey, losskey = jr.split(key)\n",
        "    losskey = jr.split(losskey, batch_size)\n",
        "    # Low-discrepancy sampling over t to reduce variance\n",
        "    t = jr.uniform(tkey, (batch_size,), minval=0, maxval=t1 / batch_size)\n",
        "    t = t + (t1 / batch_size) * jnp.arange(batch_size)\n",
        "    loss_fn = ft.partial(single_loss_fn, model, weight, int_beta)\n",
        "    loss_fn = jax.vmap(loss_fn)\n",
        "    return jnp.mean(loss_fn(data, t, losskey))\n",
        "\n",
        "\n",
        "@eqx.filter_jit\n",
        "def single_sample_fn(model, int_beta, data_shape, dt0, t1, key):\n",
        "    def drift(t, y, args):\n",
        "        _, beta = jax.jvp(int_beta, (t,), (jnp.ones_like(t),))\n",
        "        return -0.5 * beta * (y + model(t, y))\n",
        "\n",
        "    term = dfx.ODETerm(drift)\n",
        "    solver = dfx.Tsit5()\n",
        "    t0 = 0\n",
        "    y1 = jr.normal(key, data_shape)\n",
        "    # reverse time, solve from t1 to t0\n",
        "    sol = dfx.diffeqsolve(term, solver, t1, t0, -dt0, y1) #adjoint=dfx.NoAdjoint()\n",
        "    return sol.ys[0]"
      ],
      "metadata": {
        "id": "Xf-iL2jpBEB2"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mnist():\n",
        "    filename = \"train-images-idx3-ubyte.gz\"\n",
        "    url_dir = \"https://storage.googleapis.com/cvdf-datasets/mnist\"\n",
        "    target_dir = os.getcwd() + \"/data/mnist\"\n",
        "    url = f\"{url_dir}/{filename}\"\n",
        "    target = f\"{target_dir}/{filename}\"\n",
        "\n",
        "    if not os.path.exists(target):\n",
        "        os.makedirs(target_dir, exist_ok=True)\n",
        "        urllib.request.urlretrieve(url, target)\n",
        "        print(f\"Downloaded {url} to {target}\")\n",
        "\n",
        "    with gzip.open(target, \"rb\") as fh:\n",
        "        _, batch, rows, cols = struct.unpack(\">IIII\", fh.read(16))\n",
        "        shape = (batch, 1, rows, cols)\n",
        "        return jnp.array(array.array(\"B\", fh.read()), dtype=jnp.uint8).reshape(shape)\n",
        "\n",
        "\n",
        "def dataloader(data, batch_size, *, key):\n",
        "    dataset_size = data.shape[0]\n",
        "    indices = jnp.arange(dataset_size)\n",
        "    while True:\n",
        "        perm = jr.permutation(key, indices)\n",
        "        (key,) = jr.split(key, 1)\n",
        "        start = 0\n",
        "        end = batch_size\n",
        "        while end < dataset_size:\n",
        "            batch_perm = perm[start:end]\n",
        "            yield data[batch_perm]\n",
        "            start = end\n",
        "            end = start + batch_size\n",
        "     "
      ],
      "metadata": {
        "id": "CCAjj75kBIqt"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@eqx.filter_jit\n",
        "def make_step(model, weight, int_beta, data, t1, key, opt_state, opt_update):\n",
        "    loss_fn = eqx.filter_value_and_grad(batch_loss_fn)\n",
        "    loss, grads = loss_fn(model, weight, int_beta, data, t1, key)\n",
        "    updates, opt_state = opt_update(grads, opt_state)\n",
        "    model = eqx.apply_updates(model, updates)\n",
        "    key = jr.split(key, 1)[0]\n",
        "    return loss, model, key, opt_state"
      ],
      "metadata": {
        "id": "J0sVh0_nBKOZ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(\n",
        "    # Model hyperparameters\n",
        "    patch_size=4,\n",
        "    hidden_size=66,\n",
        "    mix_patch_size=512,\n",
        "    mix_hidden_size=512,\n",
        "    num_blocks=4,\n",
        "    t1=10.0,\n",
        "    # Optimisation hyperparameters\n",
        "    num_steps=500_000,\n",
        "    lr=3e-4,\n",
        "    batch_size=256,\n",
        "    print_every=5_000,\n",
        "    # Sampling hyperparameters\n",
        "    dt0=0.1,\n",
        "    sample_size=10,\n",
        "    # Seed\n",
        "    seed=2023,\n",
        "):\n",
        "    key = jr.PRNGKey(seed)\n",
        "    model_key, train_key, loader_key, sample_key = jr.split(key, 4)\n",
        "    data = mnist()\n",
        "    data_mean = jnp.mean(data)\n",
        "    data_std = jnp.std(data)\n",
        "    data_max = jnp.max(data)\n",
        "    data_min = jnp.min(data)\n",
        "    data_shape = data.shape[1:]\n",
        "    data = (data - data_mean) / data_std\n",
        "\n",
        "    model = Mixer2d(\n",
        "        data_shape,\n",
        "        patch_size,\n",
        "        hidden_size,\n",
        "        mix_patch_size,\n",
        "        mix_hidden_size,\n",
        "        num_blocks,\n",
        "        t1,\n",
        "        key=model_key,\n",
        "    )\n",
        "    int_beta = lambda t: t  # Try experimenting with other options here!\n",
        "    weight = lambda t: 1 - jnp.exp(\n",
        "        -int_beta(t)\n",
        "    )  # Just chosen to upweight the region near t=0.\n",
        "\n",
        "    opt = optax.adabelief(lr)\n",
        "    # Optax will update the floating-point JAX arrays in the model.\n",
        "    opt_state = opt.init(eqx.filter(model, eqx.is_inexact_array))\n",
        "\n",
        "    total_value = 0\n",
        "    total_size = 0\n",
        "    for step, data in zip(\n",
        "        range(num_steps), dataloader(data, batch_size, key=loader_key)\n",
        "    ):\n",
        "        value, model, train_key, opt_state = make_step(\n",
        "            model, weight, int_beta, data, t1, train_key, opt_state, opt.update\n",
        "        )\n",
        "        total_value += value.item()\n",
        "        total_size += 1\n",
        "        if (step % print_every) == 0 or step == num_steps - 1:\n",
        "            print(f\"Step={step} Loss={total_value / total_size}\")\n",
        "            total_value = 0\n",
        "            total_size = 0\n",
        "\n",
        "    \n",
        "    eqx.tree_serialise_leaves(\"attmixer_mnist_model.eqx\", model)\n",
        "    shutil.copy('/content/attmixer_mnist_model.eqx','/content/gdrive/MyDrive/Colab_Notebooks')\n",
        "\n",
        "    \n",
        "    \n",
        "    sample_key = jr.split(sample_key, sample_size**2)\n",
        "    sample_fn = ft.partial(single_sample_fn, model, int_beta, data_shape, dt0, t1)\n",
        "    sample = jax.vmap(sample_fn)(sample_key)\n",
        "    sample = data_mean + data_std * sample\n",
        "    sample = jnp.clip(sample, data_min, data_max)\n",
        "    sample = einops.rearrange(\n",
        "        sample, \"(n1 n2) 1 h w -> (n1 h) (n2 w)\", n1=sample_size, n2=sample_size\n",
        "    )\n",
        "    plt.imshow(sample, cmap=\"Greys\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "HR_WBH2ABNSb"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "kaUPnG76BQIt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}