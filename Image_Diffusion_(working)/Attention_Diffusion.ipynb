{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vkxG8UA55CDs",
        "boKbD65w2odj",
        "XYdPvNIg2q6C",
        "aH_B8CT82uLR",
        "YuPSPusZ2v3N",
        "5C5kiQ_k2x7F",
        "udhjg0J-9pK0",
        "gV_U-JJrL-jC",
        "7wfr3bbCSZkk"
      ],
      "authorship_tag": "ABX9TyPJZ1RZDy3gmZKv5aIsh1kl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durml91/MMath-Project/blob/duo-branch/Image_Diffusion_(working)/Attention_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB7c1s9yAoVp",
        "outputId": "d5817a54-ab45-4938-da67-5c5ddfbc61a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Collecting jaxlib==0.4.2+cuda11.cudnn82\n",
            "  Downloading https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.4.2%2Bcuda11.cudnn82-cp38-cp38-manylinux2014_x86_64.whl (164.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.4/164.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jaxlib==0.4.2+cuda11.cudnn82) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jaxlib==0.4.2+cuda11.cudnn82) (1.7.3)\n",
            "Installing collected packages: jaxlib\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.3.25+cuda11.cudnn805\n",
            "    Uninstalling jaxlib-0.3.25+cuda11.cudnn805:\n",
            "      Successfully uninstalled jaxlib-0.3.25+cuda11.cudnn805\n",
            "Successfully installed jaxlib-0.4.2+cuda11.cudnn82\n"
          ]
        }
      ],
      "source": [
        "!pip install jaxlib==0.4.2+cuda11.cudnn82 -f  https://storage.googleapis.com/jax-releases/jax_cuda_releases.html # [cuda]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffrax\n",
        "!pip install equinox\n",
        "!pip install einops\n",
        "!pip install optax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAhw7yT6AvfB",
        "outputId": "3ec393ee-5d4d-4586-aa7d-145c39997f71"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting diffrax\n",
            "  Downloading diffrax-0.3.1-py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting equinox>=0.10.0\n",
            "  Downloading equinox-0.10.1-py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.7/108.7 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jax>=0.4.3\n",
            "  Downloading jax-0.4.4.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jaxtyping>=0.2.12\n",
            "  Downloading jaxtyping-0.2.12-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jax>=0.4.3->diffrax) (1.22.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.4.3->diffrax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.4.3->diffrax) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.8/dist-packages (from jaxtyping>=0.2.12->equinox>=0.10.0->diffrax) (4.5.0)\n",
            "Collecting typeguard>=2.13.3\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.4-py3-none-any.whl size=1403844 sha256=fe15552e6a2b1f6f0b02a31657ed05dff3cb62d70053681fe7fe181c1015f93d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/cd/9b/750eb95db5b18b776ae59f55ae22b91a01e3703f3fb07eaa13\n",
            "Successfully built jax\n",
            "Installing collected packages: typeguard, jaxtyping, jax, equinox, diffrax\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 2.7.1\n",
            "    Uninstalling typeguard-2.7.1:\n",
            "      Successfully uninstalled typeguard-2.7.1\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.3.25\n",
            "    Uninstalling jax-0.3.25:\n",
            "      Successfully uninstalled jax-0.3.25\n",
            "Successfully installed diffrax-0.3.1 equinox-0.10.1 jax-0.4.4 jaxtyping-0.2.12 typeguard-2.13.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: equinox in /usr/local/lib/python3.8/dist-packages (0.10.1)\n",
            "Requirement already satisfied: jaxtyping>=0.2.12 in /usr/local/lib/python3.8/dist-packages (from equinox) (0.2.12)\n",
            "Requirement already satisfied: jax>=0.4.3 in /usr/local/lib/python3.8/dist-packages (from equinox) (0.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jax>=0.4.3->equinox) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.4.3->equinox) (1.7.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.4.3->equinox) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.8/dist-packages (from jaxtyping>=0.2.12->equinox) (4.5.0)\n",
            "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.8/dist-packages (from jaxtyping>=0.2.12->equinox) (2.13.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optax\n",
            "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.8/dist-packages (from optax) (0.4.2+cuda11.cudnn82)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.8/dist-packages (from optax) (4.5.0)\n",
            "Collecting chex>=0.1.5\n",
            "  Downloading chex-0.1.6-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.8/dist-packages (from optax) (0.4.4)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from optax) (1.22.4)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax) (0.12.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax) (0.1.8)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.1.55->optax) (1.7.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Installing collected packages: chex, optax\n",
            "Successfully installed chex-0.1.6 optax-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import array\n",
        "import functools as ft\n",
        "import gzip\n",
        "import os\n",
        "import struct\n",
        "import urllib.request\n",
        "\n",
        "import diffrax as dfx  # https://github.com/patrick-kidger/diffrax\n",
        "import einops  # https://github.com/arogozhnikov/einops\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr\n",
        "import matplotlib.pyplot as plt\n",
        "import optax  # https://github.com/deepmind/optax\n",
        "\n",
        "import equinox as eqx"
      ],
      "metadata": {
        "id": "E1AYxrBfAxC6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = jr.PRNGKey(2023)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CDF6Fp0CL4J",
        "outputId": "54213c71-4e52-409d-e8e8-2797eca00d42"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFbT8W33BUPS",
        "outputId": "06ac6304-c303-40cb-d7ff-72228010875e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil"
      ],
      "metadata": {
        "id": "nrnW1wkMCJae"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Original MLP Mixer + Attention"
      ],
      "metadata": {
        "id": "vkxG8UA55CDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class qkv(eqx.Module):\n",
        "#     c_attn: eqx.nn.Linear\n",
        "\n",
        "\n",
        "#     def __init__(self, key):\n",
        "#         self.c_attn = eqx.nn.Linear(64, 3*64, key=key)\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         #x = einops.rearrange(x, \"a b -> b a\")\n",
        "        \n",
        "#         #T,C = x.shape\n",
        "#         print(\"hi\")\n",
        "#         qkv = jax.vmap(self.c_attn)(x)\n",
        "#         #qkv = qkv.transpose(1,0)\n",
        "#         #print(qkv.shape)\n",
        "#         q,k,v = jnp.array_split(qkv, 3, axis=-1)\n",
        "#         print(q.shape,k.shape,v.shape)\n",
        "#         # q = einops.rearrange(q, \"a b->b a\")\n",
        "#         # k = einops.rearrange(k, \"a b->b a\")\n",
        "#         # v = einops.rearrange(v, \"a b->b a\")\n",
        "\n",
        "        \n",
        "        \n",
        "#         # #print(x.shape)\n",
        "#         # W_q = self.wq(x)\n",
        "#         # #print(W_q.shape)\n",
        "#         # W_k = self.wk(x)\n",
        "#         # W_v = self.wv(x)\n",
        "#         # Q = jnp.matmul(x, W_q)\n",
        "#         # K = jnp.matmul(x, W_k)\n",
        "#         # V = jnp.matmul(x, W_v)\n",
        "\n",
        "#         return q,k,v\n",
        "\n",
        "\n",
        "# class Attblock(eqx.Module):\n",
        "#     matrix: eqx.Module\n",
        "\n",
        "#     c_proj: eqx.nn.Linear\n",
        "    \n",
        "#     att: eqx.nn.MultiheadAttention\n",
        "\n",
        "#     def __init__(self, key):\n",
        "\n",
        "#         self.att = eqx.nn.MultiheadAttention(7, query_size=64, key=key)\n",
        "#         self.matrix = qkv(key=key)\n",
        "#         self.c_proj = eqx.nn.Linear(64,64, key=key)\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         Q,K,V = self.matrix(x)\n",
        "        \n",
        "#         at=self.att(Q,K,V)\n",
        "#         at = self.c_proj(at)\n",
        "#         return at"
      ],
      "metadata": {
        "id": "PwkZyojpwC-4"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class qkv(eqx.Module):\n",
        "#     c_attn: eqx.nn.Linear\n",
        "\n",
        "\n",
        "#     def __init__(self, key):\n",
        "#         self.c_attn = eqx.nn.Linear(66, 3*66, key=key)\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         x = einops.rearrange(x, \"a b -> b a\")\n",
        "        \n",
        "#         #T,C = x.shape\n",
        "\n",
        "#         qkv = jax.vmap(self.c_attn)(x)\n",
        "#         #qkv = qkv.transpose(1,0)\n",
        "#         #print(qkv.shape)\n",
        "#         q,k,v = jnp.array_split(qkv, 3, axis=-1)\n",
        "#         print(q.shape,k.shape,v.shape)\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "#         # #print(x.shape)\n",
        "#         # W_q = self.wq(x)\n",
        "#         # #print(W_q.shape)\n",
        "#         # W_k = self.wk(x)\n",
        "#         # W_v = self.wv(x)\n",
        "#         # Q = jnp.matmul(x, W_q)\n",
        "#         # K = jnp.matmul(x, W_k)\n",
        "#         # V = jnp.matmul(x, W_v)\n",
        "\n",
        "#         return q,k,v\n",
        "\n",
        "\n",
        "# class Attblock(eqx.Module):\n",
        "#     matrix: eqx.Module\n",
        "\n",
        "#     c_proj: eqx.nn.Linear\n",
        "    \n",
        "#     att: eqx.nn.MultiheadAttention\n",
        "\n",
        "#     def __init__(self, key):\n",
        "\n",
        "#         self.att = eqx.nn.MultiheadAttention(7, query_size=66, key=key)\n",
        "#         self.matrix = qkv(key=key)\n",
        "#         self.c_proj = eqx.nn.Linear(66,66, key=key)\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         Q,K,V = self.matrix(x)\n",
        "        \n",
        "#         at=self.att(Q,K,V)\n",
        "#         at = self.c_proj(at)\n",
        "#         return at\n",
        "\n",
        "\n",
        "\n",
        "# class SinusoidalEmbedding(eqx.Module):\n",
        "#     d:int = 28\n",
        "\n",
        "\n",
        "#     def __init__(self, d):\n",
        "#         self.d=d\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#       half_dim = self.d//2\n",
        "#       emb=jnp.log(10000) / (half_dim - 1)\n",
        "#       emb = jnp.exp(jnp.arange(half_dim)* -emb)\n",
        "#       emb = x[:,None] * emb[None,:]\n",
        "#       emb = jnp.concatenate([jnp.sin(emb), jnp.cos(emb)], -1)\n",
        "#       return emb\n",
        "\n",
        "\n",
        "# class TimeEmbed(eqx.Module):\n",
        "#     dim: int =28\n",
        "#     embed: eqx.nn.Embedding\n",
        "#     sin: eqx.Module\n",
        "    \n",
        "#     def __init__(self, key):\n",
        "#         self.embed = eqx.nn.Embedding(1000, 28, key=key)\n",
        "#         self.sin = SinusoidalEmbedding(28)\n",
        "  \n",
        "#     def __call__(self, x):\n",
        "#         se = self.sin(x)\n",
        "#         se = (se*10000).astype(int)\n",
        "#         emb = self.embed(se)\n",
        "#         return emb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class MixerBlock(eqx.Module):\n",
        "#     patch_mixer: eqx.nn.MLP\n",
        "#     hidden_mixer: eqx.nn.MLP\n",
        "#     norm1: eqx.nn.LayerNorm\n",
        "#     norm2: eqx.nn.LayerNorm\n",
        "\n",
        "#     Att1: eqx.Module\n",
        "#     Att2: eqx.Module\n",
        "#     gnorm1: eqx.nn.GroupNorm\n",
        "#     gnorm2: eqx.nn.GroupNorm\n",
        "\n",
        "\n",
        "#     def __init__(\n",
        "#         self, num_patches, hidden_size, mix_patch_size, mix_hidden_size, *, key\n",
        "#     ):\n",
        "#         tkey, ckey,a1key, a2key = jr.split(key, 4)\n",
        "#         self.patch_mixer = eqx.nn.MLP(\n",
        "#             num_patches, num_patches, mix_patch_size, depth=1, key=tkey\n",
        "#         )\n",
        "#         self.hidden_mixer = eqx.nn.MLP(\n",
        "#             hidden_size, hidden_size, mix_hidden_size, depth=1, key=ckey\n",
        "#         )\n",
        "#         self.norm1 = eqx.nn.LayerNorm((hidden_size, num_patches))\n",
        "#         self.norm2 = eqx.nn.LayerNorm((num_patches, hidden_size))\n",
        "\n",
        "#         self.Att1 = Attblock(key = a1key)\n",
        "#         self.Att2 = Attblock(key = a2key)\n",
        "\n",
        "#         self.gnorm1 = eqx.nn.GroupNorm(4,32)\n",
        "#         self.gnorm2 = eqx.nn.GroupNorm(4,32)\n",
        "\n",
        "#     def __call__(self, y):\n",
        "#         y = y + jax.vmap(self.patch_mixer)(self.norm1(y))\n",
        "#         unatt = self.Att1(y)\n",
        "#         gnormatt = self.gnorm1(unatt)\n",
        "#         y = y + gnormatt\n",
        "        \n",
        "#         y = einops.rearrange(y, \"c p -> p c\")\n",
        "#         y = y + jax.vmap(self.hidden_mixer)(self.norm2(y))\n",
        "#         y = einops.rearrange(y, \"p c -> c p\")\n",
        "        \n",
        "#         unatt = self.Att2(y)\n",
        "#         gnormatt = self.gnorm2(unatt)\n",
        "#         y = y + gnormatt\n",
        "#         return y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class Mixer2d(eqx.Module):\n",
        "#     conv_in: eqx.nn.Conv2d\n",
        "#     conv_out: eqx.nn.ConvTranspose2d\n",
        "#     blocks: list\n",
        "#     norm: eqx.nn.LayerNorm\n",
        "#     t1: float\n",
        "#     timemb: eqx.Module\n",
        "\n",
        "\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         img_size,\n",
        "#         patch_size,\n",
        "#         hidden_size,\n",
        "#         mix_patch_size,\n",
        "#         mix_hidden_size,\n",
        "#         num_blocks,\n",
        "#         t1,\n",
        "#         *,\n",
        "#         key,\n",
        "#     ):\n",
        "#         input_size, height, width = img_size\n",
        "#         assert (height % patch_size) == 0\n",
        "#         assert (width % patch_size) == 0\n",
        "#         num_patches = (height // patch_size) * (width // patch_size)\n",
        "        \n",
        "#         inkey, outkey,tkey, *bkeys = jr.split(key, 3 + num_blocks)\n",
        "\n",
        "#         self.conv_in = eqx.nn.Conv2d(\n",
        "#             input_size + 1, hidden_size, patch_size, stride=patch_size, key=inkey\n",
        "#         )\n",
        "#         self.conv_out = eqx.nn.ConvTranspose2d(\n",
        "#             hidden_size, input_size, patch_size, stride=patch_size, key=outkey\n",
        "#         )\n",
        "#         self.blocks = [\n",
        "#             MixerBlock(\n",
        "#                 num_patches, hidden_size, mix_patch_size, mix_hidden_size, key=bkey\n",
        "#             )\n",
        "#             for bkey in bkeys\n",
        "#         ]\n",
        "#         self.norm = eqx.nn.LayerNorm((hidden_size, num_patches))\n",
        "#         self.t1 = t1\n",
        "#         self.timemb = TimeEmbed(key=tkey)\n",
        "\n",
        "#     def __call__(self, t, y):\n",
        "#         t = t / self.t1\n",
        "#         t = einops.repeat(t, \"-> 1\")\n",
        "#         t = jnp.array([t], dtype=int)\n",
        "#         t = self.timemb(t)\n",
        "#         t = einops.rearrange(t, \" c 1 h w-> c h w\")\n",
        "#         y = jnp.concatenate([y, t])\n",
        "#         y = self.conv_in(y)\n",
        "#         _, patch_height, patch_width = y.shape\n",
        "#         y = einops.rearrange(y, \"c h w -> c (h w)\")\n",
        "#         for block in self.blocks:\n",
        "#             y = block(y)\n",
        "#         y = self.norm(y)\n",
        "#         y = einops.rearrange(y, \"c (h w) -> c h w\", h=patch_height, w=patch_width)\n",
        "#         return self.conv_out(y)"
      ],
      "metadata": {
        "id": "MP-0jr4XA8Y5"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class qkv(eqx.Module):\n",
        "#     wk: eqx.nn.Linear\n",
        "#     wq: eqx.nn.Linear\n",
        "#     wv: eqx.nn.Linear\n",
        "\n",
        "#     def __init__(self, key):\n",
        "#         self.wq = eqx.nn.Linear(49, 128, key=key)\n",
        "#         self.wk = eqx.nn.Linear(49, 128, key=key)\n",
        "#         self.wv = eqx.nn.Linear(49, 128, key=key)\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         W_q = jax.vmap(self.wq)(x)\n",
        "#         W_k = jax.vmap(self.wk)(x)\n",
        "#         W_v = jax.vmap(self.wv)(x)\n",
        "        \n",
        "#         print(W_q.shape, x.shape)\n",
        "#         Q = jax.lax.batch_matmul(x, W_q)\n",
        "#         K = jax.lax.batch_matmul(x, W_k)\n",
        "#         V = jax.lax.batch_matmul(x, W_v)\n",
        "#         return Q,K,V\n",
        "\n",
        "\n",
        "# class Attblock(eqx.Module):\n",
        "#     matrix: eqx.Module\n",
        "    \n",
        "#     att: eqx.nn.MultiheadAttention\n",
        "\n",
        "#     def __init__(self, key):\n",
        "\n",
        "#         self.att = eqx.nn.MultiheadAttention(7, query_size=49, key=key)\n",
        "#         self.matrix = qkv(key=key)\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         Q,K,V = self.matrix(x)\n",
        "        \n",
        "#         at=self.att(Q,K,V)\n",
        "#         return at\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class MixerBlock(eqx.Module):\n",
        "#     patch_mixer: eqx.nn.MLP\n",
        "#     hidden_mixer: eqx.nn.MLP\n",
        "#     norm1: eqx.nn.LayerNorm\n",
        "#     norm2: eqx.nn.LayerNorm\n",
        "\n",
        "#     Att1: eqx.Module\n",
        "#     Att2: eqx.Module\n",
        "#     gnorm1: eqx.nn.GroupNorm\n",
        "#     gnorm2: eqx.nn.GroupNorm\n",
        "\n",
        "\n",
        "#     def __init__(\n",
        "#         self, num_patches, hidden_size, mix_patch_size, mix_hidden_size, *, key\n",
        "#     ):\n",
        "#         tkey, ckey,a1key, a2key = jr.split(key, 4)\n",
        "#         self.patch_mixer = eqx.nn.MLP(\n",
        "#             num_patches, num_patches, mix_patch_size, depth=1, key=tkey\n",
        "#         )\n",
        "#         self.hidden_mixer = eqx.nn.MLP(\n",
        "#             hidden_size, hidden_size, mix_hidden_size, depth=1, key=ckey\n",
        "#         )\n",
        "#         self.norm1 = eqx.nn.LayerNorm((hidden_size, num_patches))\n",
        "#         self.norm2 = eqx.nn.LayerNorm((num_patches, hidden_size))\n",
        "\n",
        "#         self.Att1 = Attblock(key = a1key)\n",
        "#         self.Att2 = Attblock(key = a2key)\n",
        "\n",
        "#         self.gnorm1 = eqx.nn.GroupNorm(4,32)\n",
        "#         self.gnorm2 = eqx.nn.GroupNorm(4,32)\n",
        "\n",
        "#     def __call__(self, y):\n",
        "#         y = y + jax.vmap(self.patch_mixer)(self.norm1(y))\n",
        "#         unatt = self.Att1(y)\n",
        "#         gnormatt = self.gnorm1(unatt)\n",
        "#         y = y + gnormatt\n",
        "        \n",
        "#         y = einops.rearrange(y, \"c p -> p c\")\n",
        "#         y = y + jax.vmap(self.hidden_mixer)(self.norm2(y))\n",
        "#         y = einops.rearrange(y, \"p c -> c p\")\n",
        "        \n",
        "#         unatt = self.Att2(y)\n",
        "#         gnormatt = self.gnorm2(unatt)\n",
        "#         y = y + gnormatt\n",
        "#         return y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class Mixer2d(eqx.Module):\n",
        "#     conv_in: eqx.nn.Conv2d\n",
        "#     conv_out: eqx.nn.ConvTranspose2d\n",
        "#     blocks: list\n",
        "#     norm: eqx.nn.LayerNorm\n",
        "#     t1: float\n",
        "#     tim_emb: eqx.nn.Embedding\n",
        "\n",
        "\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         img_size,\n",
        "#         patch_size,\n",
        "#         hidden_size,\n",
        "#         mix_patch_size,\n",
        "#         mix_hidden_size,\n",
        "#         num_blocks,\n",
        "#         t1,\n",
        "#         *,\n",
        "#         key,\n",
        "#     ):\n",
        "#         input_size, height, width = img_size\n",
        "#         assert (height % patch_size) == 0\n",
        "#         assert (width % patch_size) == 0\n",
        "#         num_patches = (height // patch_size) * (width // patch_size)\n",
        "        \n",
        "#         inkey, outkey,tkey, *bkeys = jr.split(key, 3 + num_blocks)\n",
        "\n",
        "#         self.conv_in = eqx.nn.Conv2d(\n",
        "#             input_size + 1, hidden_size, patch_size, stride=patch_size, key=inkey\n",
        "#         )\n",
        "#         self.conv_out = eqx.nn.ConvTranspose2d(\n",
        "#             hidden_size, input_size, patch_size, stride=patch_size, key=outkey\n",
        "#         )\n",
        "#         self.blocks = [\n",
        "#             MixerBlock(\n",
        "#                 num_patches, hidden_size, mix_patch_size, mix_hidden_size, key=bkey\n",
        "#             )\n",
        "#             for bkey in bkeys\n",
        "#         ]\n",
        "#         self.norm = eqx.nn.LayerNorm((hidden_size, num_patches))\n",
        "#         self.t1 = t1\n",
        "#         self.tim_emb = eqx.nn.Embedding(28, 28, key=tkey)\n",
        "\n",
        "#     def __call__(self, t, y):\n",
        "#         t = t / self.t1\n",
        "#         t = einops.repeat(t, \"-> 1\")\n",
        "#         t = jnp.array([t], dtype=int)\n",
        "#         t = self.tim_emb(t)\n",
        "        \n",
        "#         #_, height, width = y.shape\n",
        "#         t = einops.repeat(t, \"c h w-> c (28 h) w\")\n",
        "#         y = jnp.concatenate([y, t])\n",
        "#         y = self.conv_in(y)\n",
        "#         _, patch_height, patch_width = y.shape\n",
        "#         y = einops.rearrange(y, \"c h w -> c (h w)\")\n",
        "#         for block in self.blocks:\n",
        "#             y = block(y)\n",
        "#         y = self.norm(y)\n",
        "#         y = einops.rearrange(y, \"c (h w) -> c h w\", h=patch_height, w=patch_width)\n",
        "#         return self.conv_out(y)"
      ],
      "metadata": {
        "id": "PWQ1ceeiD5BH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DiT Model"
      ],
      "metadata": {
        "id": "MxSYycHl2mpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Time embedding"
      ],
      "metadata": {
        "id": "boKbD65w2odj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable"
      ],
      "metadata": {
        "id": "hLcneMNhBhII"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Embedding Layer of Timesteps\n",
        "\n",
        "class Lambda1(eqx.Module):\n",
        "    fn: Callable\n",
        "\n",
        "    def __call__(self , x, *, key=None):\n",
        "        return self.fn(x)\n",
        "\n",
        "\n",
        "\n",
        "class TimeStepEmbedder(eqx.Module):\n",
        "    mlp: eqx.nn.Sequential\n",
        "    frequency_embedding_size: int\n",
        "    \n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        frequency_embedding_size,   #set as 256\n",
        "        key\n",
        "    ):\n",
        "\n",
        "        self.mlp = eqx.nn.Sequential([\n",
        "            eqx.nn.Linear(frequency_embedding_size, hidden_size, key=key),\n",
        "            Lambda1(jax.nn.silu),\n",
        "            eqx.nn.Linear(hidden_size, hidden_size, key=key)\n",
        "        ])\n",
        "        self.frequency_embedding_size = frequency_embedding_size\n",
        "\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        t,\n",
        "        max_period=10000\n",
        "    ):\n",
        "        dim = self.frequency_embedding_size\n",
        "        half = dim // 2\n",
        "        freqs = jnp.exp(\n",
        "            -jnp.log(max_period) * jnp.arange(0, half, dtype=float) / half\n",
        "        )\n",
        "        args = t[:, None].astype(float) * freqs[None]\n",
        "        embedding = jnp.concatenate([jnp.cos(args), jnp.sin(args)], axis=-1)\n",
        "        if dim % 2:\n",
        "            embedding = jnp.concatenate([embedding, jnp.zeros_like(embedding[:, :1])], axis=-1)\n",
        "        t_freq = embedding\n",
        "        t_emb = jax.vmap(self.mlp)(t_freq)\n",
        "        return t_emb      "
      ],
      "metadata": {
        "id": "WY2uZhp89Ylu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Time embed test\n",
        "TimeStepEmbedder(32, 256, key)(jnp.array([4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znitHQVPFd9R",
        "outputId": "12c2b747-33e7-4429-cda3-c254a2e9796d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[ 0.3052112 ,  0.19230524,  0.14764252,  0.31935525, -0.06504469,\n",
              "        -0.23124412, -0.36936757,  0.02332152,  0.14454725, -0.18957621,\n",
              "         0.20766026,  0.00606484,  0.03791146, -0.04720105, -0.12594688,\n",
              "        -0.15844625, -0.09823439,  0.18520194, -0.12404346, -0.09638146,\n",
              "        -0.04518805,  0.06181648, -0.0097344 , -0.07609019, -0.00168262,\n",
              "         0.21857136, -0.17319855,  0.19038439, -0.12791844, -0.2365416 ,\n",
              "         0.14090869,  0.08538287]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Multihead Attention"
      ],
      "metadata": {
        "id": "XYdPvNIg2q6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## attention\n",
        "\n",
        "class MultiAtt(eqx.Module):\n",
        "    c_attn: eqx.nn.Linear\n",
        "    c_proj: eqx.nn.Linear\n",
        "    n_head: int\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_embd,\n",
        "        n_head,\n",
        "        key\n",
        "    ):\n",
        "        assert n_embd % n_head ==0\n",
        "        self.c_attn = eqx.nn.Linear(n_embd, 3 * n_embd, key=key)\n",
        "        self.c_proj = eqx.nn.Linear(n_embd, n_embd, key=key)\n",
        "        self.n_head = n_head\n",
        "        #self.mha = eqx.nn.MultiheadAttention(n_head, )\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        x,\n",
        "        mask=None\n",
        "    ):\n",
        "        B, T, C = x.shape\n",
        "        qkv = jax.vmap(jax.vmap(self.c_attn))(x)\n",
        "        qkv = qkv.reshape(B, T, self.n_head, -1)\n",
        "        qkv = jnp.transpose(qkv, (0, 2, 1, 3)) #[batch, number of heads, T or seq_length, embed_dim or C(channels)]\n",
        "        q, k, v = jnp.array_split(qkv, 3, axis=-1)\n",
        "        new_k = jnp.swapaxes(k, -2,-1) *(1.0 / jnp.sqrt(k.shape[-1]))\n",
        "        att = jax.lax.batch_matmul(q, new_k)\n",
        "        if mask is not None:\n",
        "            attn = jnp.where(mask == 0, -9e15, att)\n",
        "        attention = jax.nn.softmax(att, axis=-1)\n",
        "        values = jax.lax.batch_matmul(attention, v)\n",
        "        values = jnp.transpose(values, (0,2,1,3)) # [batch, T, n_heads, C]\n",
        "        values = values.reshape(B, T, C)\n",
        "        o = jax.vmap(jax.vmap(self.c_proj))(values)\n",
        "        return o   #, attention"
      ],
      "metadata": {
        "id": "AbYAYrUaKuBf"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = jr.normal(key, (3, 16, 128))\n",
        "print(x.shape)\n",
        "mh_attn = MultiAtt(n_embd=128, n_head=4, key=key)\n",
        "output, attention = mh_attn(x)\n",
        "print(output.shape, attention.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVdzNdX5UcA_",
        "outputId": "8b0756f6-7289-41bc-e333-7fe3d477154a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 16, 128)\n",
            "(3, 16, 128) (3, 4, 16, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Extra misc\n",
        "\n",
        "# class Lambda2(eqx.Module):\n",
        "#     fn: Callable\n",
        "\n",
        "#     def __call__(self , x, *, key=None):\n",
        "#         return self.fn(x)"
      ],
      "metadata": {
        "id": "GO5POK3zsOTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Utility function -modulate"
      ],
      "metadata": {
        "id": "aH_B8CT82uLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modulate(x, shift, scale):\n",
        "    scale = jnp.expand_dims(scale, axis=0)   #\"a -> a 1\"\n",
        "    shift = jnp.expand_dims(shift, axis=0)      #\"a -> a 1\"\n",
        "    \n",
        "    #return x * (1 + scale) + shift\n",
        "    c = (1 + scale) + shift\n",
        "    #print(x.shape, c.shape)\n",
        "    return x * c\n",
        "    #print(x.shape, c.shape)\n",
        "    #return jnp.matmul(x,c)"
      ],
      "metadata": {
        "id": "7drPPWj5vjY1"
      },
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "w = jr.normal(key, (4,8,32))\n",
        "q=jnp.arange(0,8)\n",
        "p=jnp.arange(0,8)\n",
        "modulate(w, q, p)"
      ],
      "metadata": {
        "id": "9qKkIRQZxkD6"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### DiT block"
      ],
      "metadata": {
        "id": "YuPSPusZ2v3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DitBlock\n",
        "\n",
        "class DitBlock(eqx.Module):\n",
        "    norm1: eqx.nn.LayerNorm\n",
        "    norm2: eqx.nn.LayerNorm\n",
        "    attn: eqx.Module\n",
        "    Mlp: eqx.nn.MLP\n",
        "    adaLN_modulation: eqx.nn.Sequential\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        n_head,\n",
        "        mlp_ratio,   # = 4.0\n",
        "        key,\n",
        "    ):\n",
        "        self.norm1 = eqx.nn.LayerNorm(hidden_size, eps = 1e-06, elementwise_affine=False)\n",
        "        self.attn = MultiAtt(hidden_size, n_head=n_head, key=key)\n",
        "        self.norm2 = eqx.nn.LayerNorm(hidden_size, eps = 1e-06, elementwise_affine=False)\n",
        "        mlp_hidden_size = int(hidden_size * mlp_ratio)\n",
        "        self.Mlp = eqx.nn.MLP(hidden_size, hidden_size, mlp_hidden_size, 1, key=key)\n",
        "        self.adaLN_modulation = eqx.nn.Sequential([\n",
        "            Lambda1(jax.nn.silu),\n",
        "            eqx.nn.Linear(hidden_size, 6 * hidden_size, key=key)\n",
        "        ])\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        x,\n",
        "        c\n",
        "    ):\n",
        "        temp = jax.vmap(self.adaLN_modulation)(c)\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = jnp.array_split(temp, 6, axis=1)\n",
        "        #gate_msa = einops.rearrange(gate_msa, \"a -> a 1\")\n",
        "        #print(gate_msa.shape)\n",
        "\n",
        "        gate_msa = jnp.expand_dims(gate_msa, axis=0)\n",
        "        #print(gate_msa.shape)\n",
        "\n",
        "        a = jax.vmap(self.norm1)(x)\n",
        "        #print(a.shape, shift_msa.shape)\n",
        "      \n",
        "        #print(a.shape, shift_msa.shape, scale_msa.shape)\n",
        "        tem = modulate(a, shift_msa, scale_msa)\n",
        "        #print(tem.shape)\n",
        "\n",
        "        x = x + gate_msa * self.attn(tem)\n",
        "        #gate_mlp = einops.rearrange(gate_mlp, \"a -> a 1\")\n",
        "\n",
        "        gate_mlp = jnp.expand_dims(gate_mlp, axis=0)\n",
        "        b = jax.vmap(self.norm2)(x)\n",
        "        tems = modulate(self.norm2(x), shift_mlp, scale_mlp)     \n",
        "        x = x + gate_mlp * jax.vmap(jax.vmap(self.Mlp))(tems)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "32SnHbSUHTsm"
      },
      "execution_count": 313,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DitBlock(128, 16, 4.0, key)"
      ],
      "metadata": {
        "id": "CWMW4rgn6W0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jr.normal(key, (2,2,2))"
      ],
      "metadata": {
        "id": "S-bZSVc40fsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eqx.nn.MLP(5,16, 20, 3, jax.nn.gelu, key=key)"
      ],
      "metadata": {
        "id": "TggtYnHhtRzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Final Layer"
      ],
      "metadata": {
        "id": "5C5kiQ_k2x7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## FinalLayer\n",
        "\n",
        "class FinalLayer(eqx.Module):\n",
        "    norm_final: eqx.nn.LayerNorm\n",
        "    linear: eqx.nn.Linear\n",
        "    adaLN_modulation: eqx.nn.Linear    #eqx.nn.Sequential\n",
        "    \n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        patch_size,\n",
        "        out_channels,\n",
        "        key\n",
        "    ):\n",
        "\n",
        "        self.norm_final = eqx.nn.LayerNorm(hidden_size, eps=1e-6, elementwise_affine=False)\n",
        "        self.linear = eqx.nn.Linear(hidden_size, patch_size * patch_size * out_channels, key=key)\n",
        "        # self.adaLN_modulation = eqx.nn.Sequential([\n",
        "        #     Lambda1(jax.nn.silu),\n",
        "        #     eqx.nn.Linear(hidden_size, 2 * hidden_size, key=key)\n",
        "        # ])\n",
        "\n",
        "        self.adaLN_modulation = eqx.nn.Linear(hidden_size, 2 * hidden_size, key=key)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        x,\n",
        "        c\n",
        "    ):\n",
        "        \n",
        "        c = jax.nn.silu(c)\n",
        "        \n",
        "        temp = jax.vmap(self.adaLN_modulation)(c)\n",
        "        shift, scale = jnp.array_split(temp, 2, axis=1)\n",
        "        x = modulate(self.norm_final(x), shift, scale)\n",
        "        x = jax.vmap(jax.vmap(self.linear))(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Dumrxptn2Bno"
      },
      "execution_count": 349,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FinalLayer(16,2,2,key=key)"
      ],
      "metadata": {
        "id": "4u2Kl5vn89As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### PatchEmbed"
      ],
      "metadata": {
        "id": "ROGcZLaJ4zOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbed(eqx.Module):\n",
        "    num_patches:int\n",
        "    proj: eqx.nn.Conv2d\n",
        "    patch_size: int\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size,\n",
        "        patch_size,\n",
        "        in_chans,\n",
        "        n_embd,\n",
        "        key\n",
        "    ):\n",
        "        self.patch_size = patch_size\n",
        "        dg = img_size // self.patch_size\n",
        "        self.num_patches = dg ** 2\n",
        "        self.proj = eqx.nn.Conv2d(in_chans, n_embd, self.patch_size, self.patch_size, key=key)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        x\n",
        "    ):\n",
        "        B, C, H, W = x.shape\n",
        "        x = jnp.array(x, dtype=float)\n",
        "        x = jax.vmap(self.proj)(x)\n",
        "        x = einops.rearrange(x, \"B C H W -> B (H W) C\")\n",
        "        return x"
      ],
      "metadata": {
        "id": "zDorl8vT40t9"
      },
      "execution_count": 403,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eg = eg.astype(float)"
      ],
      "metadata": {
        "id": "ejmgdJwLbAl3"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PatchEmbed(28, 4, 1, 384, key=key)(eg)"
      ],
      "metadata": {
        "id": "w13qo2_P9XVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Utility functions - positional embedding functions"
      ],
      "metadata": {
        "id": "udhjg0J-9pK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_2d_sincos_pos_embed(n_embd, grid_size):\n",
        "\n",
        "    grid_h = jnp.arange(grid_size, dtype=float)\n",
        "    grid_w = jnp.arange(grid_size, dtype=float)\n",
        "    grid = jnp.meshgrid(grid_w, grid_h)\n",
        "    grid = jnp.stack(grid, axis=0)\n",
        "\n",
        "    #grid = einops.rearrange(grid, \" 2 d f -> 2 1 d f\")\n",
        "    grid = jnp.reshape(grid, (2, 1, grid_size, grid_size))\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(n_embd, grid)\n",
        "    return pos_embed"
      ],
      "metadata": {
        "id": "HJpcuZqpDJ5d"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = get_2d_sincos_pos_embed(16, 8)"
      ],
      "metadata": {
        "id": "DdjPKkI8EsqB"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRv3w_iRGyuW",
        "outputId": "f68ec2be-6001-494c-c31c-f2cc4b59ff68"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_2d_sincos_pos_embed_from_grid(n_embd, grid):\n",
        "    assert n_embd % 2 == 0\n",
        "\n",
        "\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(n_embd // 2, grid[0]) # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(n_embd // 2, grid[1]) # (H*W, D/2)\n",
        "\n",
        "    emb = jnp.concatenate([emb_h, emb_w], axis=1)  #(H*W, D)\n",
        "    return emb"
      ],
      "metadata": {
        "id": "_XPUWSZ_DLan"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = jr.normal(key, (3,3))"
      ],
      "metadata": {
        "id": "0V2Qc7tvDPNL"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = get_2d_sincos_pos_embed_from_grid(16, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0MOWA97DnxP",
        "outputId": "f4da45b0-eb48-47b4-815c-ab8bdf69f1a6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlneY3f0D1r2",
        "outputId": "a551e0fe-1266-4708-adad-65958a803d86"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_1d_sincos_pos_embed_from_grid(n_embd, pos):\n",
        "    assert n_embd % 2 == 0\n",
        "    omega = jnp.arange(n_embd // 2, dtype=float)\n",
        "    omega /= n_embd / 2\n",
        "    omega = 1. / 10000*omega #(D/2)\n",
        "\n",
        "    pos = jnp.array(pos)\n",
        "    #print(pos.shape)\n",
        "    #pos = einops.rearrange(pos, \"a -> a 0\")\n",
        "    out = jnp.outer(pos, omega)\n",
        "\n",
        "    emb_sin = jnp.sin(out)\n",
        "    emb_cos = jnp.cos(out)\n",
        "\n",
        "    emb = jnp.concatenate([emb_sin, emb_cos], axis=1)\n",
        "    return emb"
      ],
      "metadata": {
        "id": "g0XXP0eZ9qwx"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=get_1d_sincos_pos_embed_from_grid(32, [1,2,3,4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lfohy7HmCIC4",
        "outputId": "04fe8bde-fc47-4abf-c1b4-609821c47b06"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbdq96F1DEaB",
        "outputId": "a5a699e2-9a44-4af9-d491-b95ac8275a47"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Parameter class"
      ],
      "metadata": {
        "id": "gV_U-JJrL-jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from typing import NamedTuple"
      ],
      "metadata": {
        "id": "K4c-u3SyKLEW"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Params(eqx.Module):\n",
        "    param: jnp.ndarray\n",
        "\n",
        "    def __init__(self, num_patches, hidden_size):\n",
        "        self.param = jnp.zeros((1, num_patches, hidden_size), dtype = float)\n",
        "\n",
        "    def __call__(self):\n",
        "        return self.param"
      ],
      "metadata": {
        "id": "WAs5fRMSKNdo"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = Params(8, 12)()"
      ],
      "metadata": {
        "id": "OyhB1OuzLUIk"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCeQLO1ELkwh",
        "outputId": "a3bcd642-2e33-40f7-fe93-c482229aa97a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 8, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### DiT"
      ],
      "metadata": {
        "id": "00hqaJ8r2zq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiT(eqx.Module):\n",
        "    in_channels: int\n",
        "    out_channels: int\n",
        "    patch_size: int\n",
        "    n_head: int\n",
        "    \n",
        "    x_embedder: eqx.Module\n",
        "    t_embedder: eqx.Module\n",
        "    pos_embed: eqx.Module\n",
        "    blocks: list\n",
        "    final_layer: eqx.Module\n",
        "    conv_out: eqx.nn.ConvTranspose2d\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=28,\n",
        "        patch_size=4,\n",
        "        in_channels=1,\n",
        "        hidden_size=384,\n",
        "        depth=4,\n",
        "        n_head=6,\n",
        "        mlp_ratio=4.0,\n",
        "        key=key\n",
        "    ):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = in_channels\n",
        "        self.patch_size = patch_size\n",
        "        self.n_head = n_head\n",
        "\n",
        "\n",
        "        self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, key=key)\n",
        "        self.t_embedder = TimeStepEmbedder(hidden_size, frequency_embedding_size=256, key=key)\n",
        "        num_patches = self.x_embedder.num_patches\n",
        "\n",
        "\n",
        "        self.pos_embed = Params(num_patches, hidden_size)\n",
        "\n",
        "        #*bkeys = jr.split(key, num_blocks)\n",
        "\n",
        "        self.blocks = [\n",
        "            DitBlock(\n",
        "                hidden_size, n_head, mlp_ratio, key = key\n",
        "            )\n",
        "            for _ in range(depth)\n",
        "        ]\n",
        "\n",
        "        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels, key=key)\n",
        "        \n",
        "\n",
        "        self.conv_out = eqx.nn.ConvTranspose2d(49, input_size, patch_size, patch_size, key=key)\n",
        "\n",
        "    \n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"\n",
        "        x: (N, T, patch_size ** 2 * C)\n",
        "        imgs: (N, H, W, C)\n",
        "        \"\"\"\n",
        "        c = self.out_channels      \n",
        "        p = self.x_embedder.patch_size \n",
        "        h = w = int(x.shape[1] ** 0.5)    \n",
        "\n",
        "        print(h, x.shape)\n",
        "        assert h * 2 == x.shape\n",
        "\n",
        "        x = jnp.reshape(x, (x.shape[0], h, w, p, p, c))\n",
        "        x = einops.rearrange(x, \"n h w p q c->n c h p w q\")\n",
        "        imgs = jnp.reshape(x, (x.shape[0], c, h * p, h * p))\n",
        "        return imgs\n",
        "    \n",
        "    def __call__(self, x, t):\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed().shape[-1], int(self.x_embedder.num_patches ** 0.5))\n",
        "        \n",
        "        \"\"\"\n",
        "        x: (N, C, H, W)\n",
        "        t: (N, )\n",
        "        \"\"\"\n",
        "        t = jnp.array(t, dtype=int)\n",
        "        x = self.x_embedder(x) + pos_embed  # (N, T, D), where T = H * W / patch_size ** 2\n",
        "        t = self.t_embedder(t)   # (N, D)\n",
        "        for block in self.blocks:\n",
        "            x = block(x,t)    # (N, T, D)\n",
        "\n",
        "        print(x.shape)\n",
        "        x = self.final_layer(x, t)     # (N, T, patch_size ** 2 * out_channels) - N is the batch_size, T is the number of patches, \n",
        "        print(x.shape)\n",
        "        x = einops.rearrange(x, \" n c (h w) -> n c h w\", h=4, w=4)\n",
        "\n",
        "\n",
        "        #x = self.unpatchify(x)        # (N, out_channels, H, W)\n",
        "\n",
        "        x = jax.vmap(self.conv_out)(x)\n",
        "\n",
        "\n",
        "        #x = jnp.squeeze(x, axis=0)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "rNU-ch8N_GMX"
      },
      "execution_count": 433,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Test"
      ],
      "metadata": {
        "id": "bxL33vh_yc7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DiT()"
      ],
      "metadata": {
        "id": "lpe7zwvuSjyM"
      },
      "execution_count": 434,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "F9lt_YfeUxk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = mnist()"
      ],
      "metadata": {
        "id": "thzcy2UYSayZ"
      },
      "execution_count": 381,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eF9e4o-VShOm",
        "outputId": "937334c4-08f4-494c-b036-01e43d7c354b"
      },
      "execution_count": 382,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 382
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eg = data[0]\n",
        "eg = eg[None,:,:,:]"
      ],
      "metadata": {
        "id": "JbF3Bm69U0GN"
      },
      "execution_count": 383,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eg.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PHJrvR1YJz1",
        "outputId": "0ec9d0cd-fddc-497e-ef54-3671c9384eb0"
      },
      "execution_count": 384,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 384
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = jnp.array([2], dtype = float)"
      ],
      "metadata": {
        "id": "gr80hL8tU7Sl"
      },
      "execution_count": 385,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(eg, t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOItbatMU_wZ",
        "outputId": "60792414-bfc6-4292-d39f-15d81e458e37"
      },
      "execution_count": 435,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 49, 384)\n",
            "(1, 49, 16)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[[[-0.15487967, -0.03433783,  0.12822291, ..., -0.09030321,\n",
              "           0.03372749,  0.00213137],\n",
              "         [ 0.06128251, -0.07565106, -0.02559442, ..., -0.03945857,\n",
              "           0.04431188,  0.04640357],\n",
              "         [ 0.19963561, -0.03064599, -0.15983671, ...,  0.00627354,\n",
              "           0.03675549,  0.1321956 ],\n",
              "         ...,\n",
              "         [ 0.13916822,  0.06520283,  0.04045987, ...,  0.03250512,\n",
              "           0.16485381, -0.14930223],\n",
              "         [ 0.095825  ,  0.03038388,  0.03251266, ..., -0.21594419,\n",
              "           0.11069892,  0.17600389],\n",
              "         [-0.03835377, -0.02780291, -0.02675361, ...,  0.08678256,\n",
              "           0.21682394,  0.0822638 ]],\n",
              "\n",
              "        [[-0.20123295, -0.02842518, -0.09395275, ..., -0.14093553,\n",
              "           0.06253438, -0.00186716],\n",
              "         [-0.09244944, -0.19457988,  0.01205789, ...,  0.13205373,\n",
              "           0.00641725, -0.0593353 ],\n",
              "         [ 0.00585786,  0.07106563,  0.05164475, ...,  0.09604876,\n",
              "           0.01921992, -0.03600349],\n",
              "         ...,\n",
              "         [ 0.02757796, -0.09348467, -0.01956007, ...,  0.21296152,\n",
              "           0.13330887, -0.10232779],\n",
              "         [ 0.05647248,  0.03896681,  0.03388151, ...,  0.02293054,\n",
              "          -0.02782279, -0.02678644],\n",
              "         [ 0.06589503,  0.02903847, -0.00348953, ..., -0.01820536,\n",
              "          -0.00318016, -0.04050036]],\n",
              "\n",
              "        [[-0.04610727,  0.22982402,  0.12624274, ..., -0.01605413,\n",
              "          -0.03723947, -0.05912744],\n",
              "         [-0.07441116, -0.11614775,  0.06524165, ..., -0.01053029,\n",
              "           0.0289466 ,  0.04276232],\n",
              "         [ 0.1116498 ,  0.09430075,  0.03323379, ..., -0.12897936,\n",
              "           0.03459373,  0.06107671],\n",
              "         ...,\n",
              "         [-0.00353244,  0.0114793 , -0.03300856, ...,  0.05529963,\n",
              "          -0.1031663 ,  0.14409515],\n",
              "         [ 0.07156058,  0.06932215,  0.01815168, ..., -0.1445298 ,\n",
              "          -0.0471566 , -0.03520609],\n",
              "         [-0.05221661,  0.03594735,  0.03609363, ...,  0.10568348,\n",
              "          -0.17113537, -0.01026791]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 0.06510983, -0.00568091,  0.06134058, ...,  0.06466965,\n",
              "          -0.09786772, -0.03182238],\n",
              "         [ 0.09284592,  0.03927095,  0.11974573, ..., -0.00595494,\n",
              "          -0.03320821,  0.00426784],\n",
              "         [-0.07457063,  0.15920097,  0.22909847, ...,  0.04433008,\n",
              "           0.04564741,  0.00317149],\n",
              "         ...,\n",
              "         [ 0.06022733,  0.04528112,  0.08004062, ..., -0.00617306,\n",
              "           0.05999074,  0.01953352],\n",
              "         [-0.03409858,  0.06331037,  0.05315039, ..., -0.04154031,\n",
              "           0.04249758,  0.10537366],\n",
              "         [-0.00284402, -0.01530036,  0.08360223, ...,  0.03641596,\n",
              "           0.23419628, -0.04534714]],\n",
              "\n",
              "        [[-0.01260901,  0.07457959, -0.09483244, ..., -0.03611067,\n",
              "           0.02391189,  0.01911025],\n",
              "         [-0.07512183, -0.18662457,  0.25778052, ...,  0.05439703,\n",
              "          -0.09222125,  0.04820454],\n",
              "         [-0.06015748,  0.10725363, -0.02841751, ...,  0.05087059,\n",
              "          -0.14394265,  0.08002598],\n",
              "         ...,\n",
              "         [ 0.0387233 , -0.05999359,  0.08773398, ..., -0.03560466,\n",
              "          -0.13341132, -0.0784152 ],\n",
              "         [ 0.07610615, -0.00569974,  0.07451361, ...,  0.16704248,\n",
              "          -0.05334341, -0.11521218],\n",
              "         [ 0.11839969, -0.16577734, -0.10662474, ...,  0.0110612 ,\n",
              "           0.28709468,  0.12655926]],\n",
              "\n",
              "        [[ 0.23153795,  0.0806492 ,  0.02049612, ..., -0.14591321,\n",
              "          -0.10498208,  0.09927207],\n",
              "         [ 0.07720266,  0.12426557,  0.09457749, ...,  0.1332184 ,\n",
              "          -0.0315923 , -0.06693916],\n",
              "         [ 0.14462234,  0.10941429, -0.05534134, ...,  0.03241787,\n",
              "           0.10835549,  0.0099049 ],\n",
              "         ...,\n",
              "         [-0.00169885, -0.03586829,  0.01952201, ...,  0.11512482,\n",
              "           0.171534  , -0.16419053],\n",
              "         [ 0.03946269, -0.03027946, -0.07591252, ..., -0.17738706,\n",
              "           0.13990116,  0.0308101 ],\n",
              "         [ 0.07638018,  0.06224826, -0.04719966, ...,  0.10075162,\n",
              "           0.02085415, -0.02956976]]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 435
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Misc"
      ],
      "metadata": {
        "id": "7wfr3bbCSZkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "WgiK4EUEd_oM"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B, M, T = 1, 49, 384"
      ],
      "metadata": {
        "id": "B7uyNN7LeA4J"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand((1, M, T))\n",
        "b = torch.rand((1, T))"
      ],
      "metadata": {
        "id": "8_ydNd__eIKQ"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mod(x, shift, scale):\n",
        "    print(scale.unsqueeze(1).shape)\n",
        "    print(x.shape)\n",
        "    c = (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
        "    print(c.shape)\n",
        "    print((x * c).shape)\n",
        "    return x * c"
      ],
      "metadata": {
        "id": "OnlMpXztmdcs"
      },
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "er = mod(a , b, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esiv5tjCmgox",
        "outputId": "da80e6be-ae99-4048-fb5f-a2454184ad58"
      },
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 384])\n",
            "torch.Size([1, 49, 384])\n",
            "torch.Size([1, 1, 384])\n",
            "torch.Size([1, 49, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   embed_dim = n_embd\n",
        "#   num_heads = n_head\n",
        "\n",
        "#   don't forget to Jit utility functions"
      ],
      "metadata": {
        "id": "PnScRwgS-FYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import repeat\n",
        "import collections.abc\n",
        "import torch"
      ],
      "metadata": {
        "id": "GwAmTqD07Gvo"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# From PyTorch internals\n",
        "def _ntuple(n):\n",
        "    def parse(x):\n",
        "        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n",
        "            return tuple(x)\n",
        "        return tuple(repeat(x, n))\n",
        "    return parse"
      ],
      "metadata": {
        "id": "-2Rb-20y7D2Z"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ntuple(2)(16)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8FMe8Bx7M_x",
        "outputId": "772c7101-bc77-44e3-9c27-49e36a96848c"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rest of code"
      ],
      "metadata": {
        "id": "rDm2oF452jQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def single_loss_fn(model, weight, int_beta, data, t, key):\n",
        "    mean = data * jnp.exp(-0.5 * int_beta(t))\n",
        "    var = jnp.maximum(1 - jnp.exp(-int_beta(t)), 1e-5)\n",
        "    std = jnp.sqrt(var)\n",
        "    noise = jr.normal(key, data.shape)\n",
        "    y = mean + std * noise\n",
        "    y = jnp.expand_dims(y, axis=0)\n",
        "    pred = model(y,t)\n",
        "    return weight(t) * jnp.mean((pred + noise / std) ** 2)\n",
        "\n",
        "\n",
        "def batch_loss_fn(model, weight, int_beta, data, t1, key):\n",
        "    batch_size = data.shape[0]\n",
        "    tkey, losskey = jr.split(key)\n",
        "    losskey = jr.split(losskey, batch_size)\n",
        "    # Low-discrepancy sampling over t to reduce variance\n",
        "    t = jr.uniform(tkey, (batch_size,), minval=0, maxval=t1 / batch_size)\n",
        "    t = t + (t1 / batch_size) * jnp.arange(batch_size)\n",
        "    loss_fn = ft.partial(single_loss_fn, model, weight, int_beta)\n",
        "    loss_fn = jax.vmap(loss_fn)\n",
        "    return jnp.mean(loss_fn(data, t, losskey))"
      ],
      "metadata": {
        "id": "Xf-iL2jpBEB2"
      },
      "execution_count": 372,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mnist():\n",
        "    filename = \"train-images-idx3-ubyte.gz\"\n",
        "    url_dir = \"https://storage.googleapis.com/cvdf-datasets/mnist\"\n",
        "    target_dir = os.getcwd() + \"/data/mnist\"\n",
        "    url = f\"{url_dir}/{filename}\"\n",
        "    target = f\"{target_dir}/{filename}\"\n",
        "\n",
        "    if not os.path.exists(target):\n",
        "        os.makedirs(target_dir, exist_ok=True)\n",
        "        urllib.request.urlretrieve(url, target)\n",
        "        print(f\"Downloaded {url} to {target}\")\n",
        "\n",
        "    with gzip.open(target, \"rb\") as fh:\n",
        "        _, batch, rows, cols = struct.unpack(\">IIII\", fh.read(16))\n",
        "        shape = (batch, 1, rows, cols)\n",
        "        return jnp.array(array.array(\"B\", fh.read()), dtype=jnp.uint8).reshape(shape)\n",
        "\n",
        "\n",
        "def dataloader(data, batch_size, *, key):\n",
        "    dataset_size = data.shape[0]\n",
        "    indices = jnp.arange(dataset_size)\n",
        "    while True:\n",
        "        perm = jr.permutation(key, indices)\n",
        "        (key,) = jr.split(key, 1)\n",
        "        start = 0\n",
        "        end = batch_size\n",
        "        while end < dataset_size:\n",
        "            batch_perm = perm[start:end]\n",
        "            yield data[batch_perm]\n",
        "            start = end\n",
        "            end = start + batch_size\n",
        "     "
      ],
      "metadata": {
        "id": "CCAjj75kBIqt"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@eqx.filter_jit\n",
        "def make_step(model, weight, int_beta, data, t1, key, opt_state, opt_update):\n",
        "    loss_fn = eqx.filter_value_and_grad(batch_loss_fn)\n",
        "    loss, grads = loss_fn(model, weight, int_beta, data, t1, key)\n",
        "    updates, opt_state = opt_update(grads, opt_state)\n",
        "    model = eqx.apply_updates(model, updates)\n",
        "    key = jr.split(key, 1)[0]\n",
        "    return loss, model, key, opt_state"
      ],
      "metadata": {
        "id": "J0sVh0_nBKOZ"
      },
      "execution_count": 355,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1=10.0\n",
        "# Optimisation hyperparameters\n",
        "num_steps=500_000\n",
        "lr=3e-4\n",
        "batch_size=256\n",
        "print_every=5_000"
      ],
      "metadata": {
        "id": "3DiMj1uH0l1D"
      },
      "execution_count": 357,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = mnist()\n",
        "data_mean = jnp.mean(data)\n",
        "data_std = jnp.std(data)\n",
        "data_max = jnp.max(data)\n",
        "data_min = jnp.min(data)\n",
        "data_shape = data.shape[1:]\n",
        "data = (data - data_mean) / data_std"
      ],
      "metadata": {
        "id": "5iqUlNfy0sOj"
      },
      "execution_count": 376,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader_key, train_key, model_key = jr.split(key, 3)"
      ],
      "metadata": {
        "id": "ZZtfxUuk1RqG"
      },
      "execution_count": 359,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DiT(key = model_key)"
      ],
      "metadata": {
        "id": "5xC1QPWW0xmh"
      },
      "execution_count": 377,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int_beta = lambda t: t  # Try experimenting with other options here!\n",
        "weight = lambda t: 1 - jnp.exp(-int_beta(t))  "
      ],
      "metadata": {
        "id": "sfqWMuv_01gV"
      },
      "execution_count": 361,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = optax.adabelief(lr)\n",
        "# Optax will update the floating-point JAX arrays in the model.\n",
        "opt_state = opt.init(eqx.filter(model, eqx.is_inexact_array))"
      ],
      "metadata": {
        "id": "0gN_Ft5J05yh"
      },
      "execution_count": 378,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_value = 0\n",
        "total_size = 0\n",
        "for step, data in zip(range(num_steps), dataloader(data, batch_size, key=loader_key)): \n",
        "    value, model, train_key, opt_state = make_step(model, weight, int_beta, data, t1, train_key, opt_state, opt.update)\n",
        "    total_value += value.item()\n",
        "    total_size += 1\n",
        "    if (step % print_every) == 0 or step == num_steps - 1:\n",
        "        print(f\"Step={step} Loss={total_value / total_size}\")\n",
        "        total_value = 0\n",
        "        total_size = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HR_WBH2ABNSb",
        "outputId": "10b654dd-4965-4a83-e1aa-1b998b365533"
      },
      "execution_count": 379,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-379-d235e7161b4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtotal_value\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/equinox/jit.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fun_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/equinox/jit.py\u001b[0m in \u001b[0;36m_fun_wrapper\u001b[0;34m(self, is_lower, args, kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mdynamic_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcombine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     outs, out_flat, out_tree, args_flat = _python_pjit_helper(\n\u001b[0m\u001b[1;32m    238\u001b[0m         fun, infer_params_fn, *args, **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, infer_params_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_python_pjit_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_params_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m   args_flat, _, params, in_tree, out_tree, _ = infer_params_fn(\n\u001b[0m\u001b[1;32m    181\u001b[0m       *args, **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36minfer_params\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m           inline=inline, resource_env=None)\n\u001b[0;32m--> 443\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mpjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon_infer_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpjit_info_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36mcommon_infer_params\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m   jaxpr, consts, canonicalized_out_shardings_flat = _pjit_jaxpr(\n\u001b[0m\u001b[1;32m    521\u001b[0m       \u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashable_pytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_shardings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_in_avals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/linear_util.py\u001b[0m in \u001b[0;36mmemoized_fun\u001b[0;34m(fun, *args)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m       \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_pjit_jaxpr\u001b[0;34m(fun, out_shardings_thunk, global_in_avals, out_tree, api_name)\u001b[0m\n\u001b[1;32m    931\u001b[0m                                     event=dispatch.JAXPR_TRACE_EVENT):\n\u001b[0;32m--> 932\u001b[0;31m       jaxpr, global_out_avals, consts = pe.trace_to_jaxpr_dynamic(\n\u001b[0m\u001b[1;32m    933\u001b[0m           fun, global_in_avals, debug_info=pe.debug_info_final(fun, api_name))\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr_dynamic\u001b[0;34m(fun, in_avals, debug_info, keep_inputs)\u001b[0m\n\u001b[1;32m   1984\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjaxpr_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1985\u001b[0;31m     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(\n\u001b[0m\u001b[1;32m   1986\u001b[0m       fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_subjaxpr_dynamic\u001b[0;34m(fun, main, in_avals, keep_inputs, debug_info)\u001b[0m\n\u001b[1;32m   2001\u001b[0m     \u001b[0min_tracers_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_inputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2002\u001b[0;31m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_tracers_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2003\u001b[0m     \u001b[0mout_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/equinox/jit.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(dynamic, static)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashable_combine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mdynamic_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-355-2bc4df1e4696>\u001b[0m in \u001b[0;36mmake_step\u001b[0;34m(model, weight, int_beta, data, t1, key, opt_state, opt_update)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meqx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_value_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_loss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/equinox/ad.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mdiff_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnondiff_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inexact_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfun_value_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnondiff_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mvalue_and_grad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m       \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvjp_py\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_partial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdyn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36m_vjp\u001b[0;34m(fun, has_aux, reduce_axes, *primals)\u001b[0m\n\u001b[1;32m   2793\u001b[0m     \u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fun_nokwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2794\u001b[0;31m     out_primal, out_vjp = ad.vjp(\n\u001b[0m\u001b[1;32m   2795\u001b[0m         flat_fun, primals_flat, reduce_axes=reduce_axes)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/ad.py\u001b[0m in \u001b[0;36mvjp\u001b[0;34m(traceable, primals, has_aux, reduce_axes)\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mout_primals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinearize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/ad.py\u001b[0m in \u001b[0;36mlinearize\u001b[0;34m(traceable, *primals, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0mjvpfun_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvpfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m   \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_to_jaxpr_nounits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvpfun_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m   \u001b[0mout_primals_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tangents_pvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr_nounits\u001b[0;34m(fun, pvals, instantiate)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_to_subjaxpr_nounits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstantiate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m     \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/equinox/ad.py\u001b[0m in \u001b[0;36mfun_value_and_grad\u001b[0;34m(_diff_x, _nondiff_x, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0m_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_diff_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_nondiff_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-372-d40d842f718e>\u001b[0m in \u001b[0;36mbatch_loss_fn\u001b[0;34m(model, weight, int_beta, data, t1, key)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosskey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mvmap_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1769\u001b[0m                   _mapped_axis_size(fun, in_tree, args_flat, in_axes_flat, \"vmap\"))\n\u001b[0;32m-> 1770\u001b[0;31m     out_flat = batching.batch(\n\u001b[0m\u001b[1;32m   1771\u001b[0m         \u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_size_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_axes_flat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-372-d40d842f718e>\u001b[0m in \u001b[0;36msingle_loss_fn\u001b[0;34m(model, weight, int_beta, data, t, key)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-375-48a95cfc84fe>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36msqueeze\u001b[0;34m(a, axis)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArrayLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m   \u001b[0m_check_arraylike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"squeeze\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_squeeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ensure_index_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/util.py\u001b[0m in \u001b[0;36m_check_arraylike\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{} requires ndarray or scalar arguments, got {} at position {}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m: TypeError: squeeze requires ndarray or scalar arguments, got <class 'jax._src.tree_util.Partial'> at position 0.\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-379-d235e7161b4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtotal_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtotal_value\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtotal_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/equinox/jit.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fun_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fun_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/equinox/jit.py\u001b[0m in \u001b[0;36m_fun_wrapper\u001b[0;34m(self, is_lower, args, kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mdynamic_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcombine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/equinox/jit.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(dynamic, static)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashable_combine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_fun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashable_combine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mdynamic_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdynamic_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStatic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatic_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-355-2bc4df1e4696>\u001b[0m in \u001b[0;36mmake_step\u001b[0;34m(model, weight, int_beta, data, t1, key, opt_state, opt_update)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meqx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_value_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_loss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meqx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/equinox/ad.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mdiff_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnondiff_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inexact_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfun_value_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnondiff_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/equinox/ad.py\u001b[0m in \u001b[0;36mfun_value_and_grad\u001b[0;34m(_diff_x, _nondiff_x, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfun_value_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_diff_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_nondiff_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0m_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_diff_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_nondiff_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mdiff_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnondiff_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inexact_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-372-d40d842f718e>\u001b[0m in \u001b[0;36mbatch_loss_fn\u001b[0;34m(model, weight, int_beta, data, t1, key)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_beta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosskey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-372-d40d842f718e>\u001b[0m in \u001b[0;36msingle_loss_fn\u001b[0;34m(model, weight, int_beta, data, t, key)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-375-48a95cfc84fe>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpatchify\u001b[0m        \u001b[0;31m# (N, out_channels, H, W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36msqueeze\u001b[0;34m(a, axis)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_wraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlax_description\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_ARRAY_VIEW_DOC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArrayLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m   \u001b[0m_check_arraylike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"squeeze\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_squeeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ensure_index_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/util.py\u001b[0m in \u001b[0;36m_check_arraylike\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    342\u001b[0m                     if not _arraylike(arg))\n\u001b[1;32m    343\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{} requires ndarray or scalar arguments, got {} at position {}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: squeeze requires ndarray or scalar arguments, got <class 'jax._src.tree_util.Partial'> at position 0."
          ]
        }
      ]
    }
  ]
}