{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1pfDsaTE6nXyxC9X5V88I2mtulZDJwaYV",
      "authorship_tag": "ABX9TyMTMsxFQpukNkVowwpqMGl7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durml91/MMath-Project/blob/duo-branch/Image_Diffusion_(working)/CIFAR_10_diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "!pip install diffrax\n",
        "!pip install optax\n",
        "!pip install equinox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAnDp7yPcn3D",
        "outputId": "fd98a9d3-4f97-4960-e83d-5e62c0f3a2c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting diffrax\n",
            "  Downloading diffrax-0.2.2-py3-none-any.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.1/138.1 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jax>=0.3.4 in /usr/local/lib/python3.8/dist-packages (from diffrax) (0.3.25)\n",
            "Collecting equinox>=0.9.1\n",
            "  Downloading equinox-0.9.2-py3-none-any.whl (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 KB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jaxtyping>=0.2.5\n",
            "  Downloading jaxtyping-0.2.11-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->diffrax) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->diffrax) (4.4.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->diffrax) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->diffrax) (1.21.6)\n",
            "Collecting typeguard>=2.13.3\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, jaxtyping, equinox, diffrax\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 2.7.1\n",
            "    Uninstalling typeguard-2.7.1:\n",
            "      Successfully uninstalled typeguard-2.7.1\n",
            "Successfully installed diffrax-0.2.2 equinox-0.9.2 jaxtyping-0.2.11 typeguard-2.13.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optax\n",
            "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.8/dist-packages (from optax) (4.4.0)\n",
            "Collecting chex>=0.1.5\n",
            "  Downloading chex-0.1.6-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from optax) (1.21.6)\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.8/dist-packages (from optax) (0.3.25)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.8/dist-packages (from optax) (0.3.25+cuda11.cudnn805)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.1.55->optax) (1.7.3)\n",
            "Installing collected packages: chex, optax\n",
            "Successfully installed chex-0.1.6 optax-0.1.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: equinox in /usr/local/lib/python3.8/dist-packages (0.9.2)\n",
            "Requirement already satisfied: jaxtyping>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from equinox) (0.2.11)\n",
            "Requirement already satisfied: jax>=0.3.4 in /usr/local/lib/python3.8/dist-packages (from equinox) (0.3.25)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->equinox) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->equinox) (4.4.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->equinox) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->equinox) (1.21.6)\n",
            "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.8/dist-packages (from jaxtyping>=0.2.5->equinox) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import array\n",
        "import functools as ft\n",
        "import gzip\n",
        "import os\n",
        "import struct\n",
        "import urllib.request\n",
        "\n",
        "import diffrax as dfx  # https://github.com/patrick-kidger/diffrax\n",
        "import einops  # https://github.com/arogozhnikov/einops\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr\n",
        "import matplotlib.pyplot as plt\n",
        "import optax  # https://github.com/deepmind/optax\n",
        "\n",
        "import equinox as eqx"
      ],
      "metadata": {
        "id": "xSYJIA33cgIh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil"
      ],
      "metadata": {
        "id": "eapOABvqouMo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQn7H7QrowZ0",
        "outputId": "0fc3d497-e3ad-449c-9baf-3667d94bf811"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cifar():\n",
        "  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "  set1 = jnp.array(x_train)\n",
        "  set2 = jnp.array(x_test)\n",
        "\n",
        "  data = jnp.concatenate((set1, set2))\n",
        "\n",
        "  data_reag = einops.rearrange(data, \"n h w c -> n c h w\")\n",
        "  return data_reag"
      ],
      "metadata": {
        "id": "iBJhJK5teUdz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MixerBlock(eqx.Module):\n",
        "    patch_mixer: eqx.nn.MLP\n",
        "    hidden_mixer: eqx.nn.MLP\n",
        "    norm1: eqx.nn.LayerNorm\n",
        "    norm2: eqx.nn.LayerNorm\n",
        "\n",
        "    def __init__(\n",
        "        self, num_patches, hidden_size, mix_patch_size, mix_hidden_size, *, key\n",
        "    ):\n",
        "        tkey, ckey = jr.split(key, 2)\n",
        "        self.patch_mixer = eqx.nn.MLP(\n",
        "            num_patches, num_patches, mix_patch_size, depth=1, key=tkey\n",
        "        )\n",
        "        self.hidden_mixer = eqx.nn.MLP(\n",
        "            hidden_size, hidden_size, mix_hidden_size, depth=1, key=ckey\n",
        "        )\n",
        "        self.norm1 = eqx.nn.LayerNorm((hidden_size, num_patches))\n",
        "        self.norm2 = eqx.nn.LayerNorm((num_patches, hidden_size))\n",
        "\n",
        "    def __call__(self, y):\n",
        "        y = y + jax.vmap(self.patch_mixer)(self.norm1(y))\n",
        "        y = einops.rearrange(y, \"c p -> p c\")\n",
        "        y = y + jax.vmap(self.hidden_mixer)(self.norm2(y))\n",
        "        y = einops.rearrange(y, \"p c -> c p\")\n",
        "        return y\n",
        "\n",
        "\n",
        "class Mixer2d(eqx.Module):\n",
        "    conv_in: eqx.nn.Conv2d\n",
        "    conv_out: eqx.nn.ConvTranspose2d\n",
        "    blocks: list\n",
        "    norm: eqx.nn.LayerNorm\n",
        "    t1: float\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size,\n",
        "        patch_size,\n",
        "        hidden_size,\n",
        "        mix_patch_size,\n",
        "        mix_hidden_size,\n",
        "        num_blocks,\n",
        "        t1,\n",
        "        *,\n",
        "        key,\n",
        "    ):\n",
        "        input_size, height, width = img_size\n",
        "        assert (height % patch_size) == 0\n",
        "        assert (width % patch_size) == 0\n",
        "        num_patches = (height // patch_size) * (width // patch_size)\n",
        "        inkey, outkey, *bkeys = jr.split(key, 2 + num_blocks)\n",
        "\n",
        "        self.conv_in = eqx.nn.Conv2d(\n",
        "            input_size + 1, hidden_size, patch_size, stride=patch_size, key=inkey\n",
        "        )\n",
        "        self.conv_out = eqx.nn.ConvTranspose2d(\n",
        "            hidden_size, input_size, patch_size, stride=patch_size, key=outkey\n",
        "        )\n",
        "        self.blocks = [\n",
        "            MixerBlock(\n",
        "                num_patches, hidden_size, mix_patch_size, mix_hidden_size, key=bkey\n",
        "            )\n",
        "            for bkey in bkeys\n",
        "        ]\n",
        "        self.norm = eqx.nn.LayerNorm((hidden_size, num_patches))\n",
        "        self.t1 = t1\n",
        "\n",
        "    def __call__(self, t, y):\n",
        "        t = t / self.t1\n",
        "        _, height, width = y.shape\n",
        "        t = einops.repeat(t, \"-> 1 h w\", h=height, w=width)\n",
        "        y = jnp.concatenate([y, t])\n",
        "        y = self.conv_in(y)\n",
        "        _, patch_height, patch_width = y.shape\n",
        "        y = einops.rearrange(y, \"c h w -> c (h w)\")\n",
        "        for block in self.blocks:\n",
        "            y = block(y)\n",
        "        y = self.norm(y)\n",
        "        y = einops.rearrange(y, \"c (h w) -> c h w\", h=patch_height, w=patch_width)\n",
        "        return self.conv_out(y)"
      ],
      "metadata": {
        "id": "JuD5K1f1ejQe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def single_loss_fn(model, weight, int_beta, data, t, key):\n",
        "    mean = data * jnp.exp(-0.5 * int_beta(t))\n",
        "    var = jnp.maximum(1 - jnp.exp(-int_beta(t)), 1e-5)\n",
        "    std = jnp.sqrt(var)\n",
        "    noise = jr.normal(key, data.shape)\n",
        "    y = mean + std * noise\n",
        "    pred = model(t, y)\n",
        "    return weight(t) * jnp.mean((pred + noise / std) ** 2)\n",
        "\n",
        "\n",
        "def batch_loss_fn(model, weight, int_beta, data, t1, key):\n",
        "    batch_size = data.shape[0]\n",
        "    tkey, losskey = jr.split(key)\n",
        "    losskey = jr.split(losskey, batch_size)\n",
        "    # Low-discrepancy sampling over t to reduce variance\n",
        "    t = jr.uniform(tkey, (batch_size,), minval=0, maxval=t1 / batch_size)\n",
        "    t = t + (t1 / batch_size) * jnp.arange(batch_size)\n",
        "    loss_fn = ft.partial(single_loss_fn, model, weight, int_beta)\n",
        "    loss_fn = jax.vmap(loss_fn)\n",
        "    return jnp.mean(loss_fn(data, t, losskey))\n",
        "\n",
        "\n",
        "@eqx.filter_jit\n",
        "def single_sample_fn(model, int_beta, data_shape, dt0, t1, key):\n",
        "    def drift(t, y, args):\n",
        "        _, beta = jax.jvp(int_beta, (t,), (jnp.ones_like(t),))\n",
        "        return -0.5 * beta * (y + model(t, y))\n",
        "\n",
        "    term = dfx.ODETerm(drift)\n",
        "    solver = dfx.Tsit5()\n",
        "    t0 = 0\n",
        "    y1 = jr.normal(key, data_shape)\n",
        "    # reverse time, solve from t1 to t0\n",
        "    sol = dfx.diffeqsolve(term, solver, t1, t0, -dt0, y1, adjoint=dfx.NoAdjoint())\n",
        "    return sol.ys[0]"
      ],
      "metadata": {
        "id": "wzjN1sw1equL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataloader(data, batch_size, *, key):\n",
        "    dataset_size = data.shape[0]\n",
        "    indices = jnp.arange(dataset_size)\n",
        "    while True:\n",
        "        perm = jr.permutation(key, indices)\n",
        "        (key,) = jr.split(key, 1)\n",
        "        start = 0\n",
        "        end = batch_size\n",
        "        while end < dataset_size:\n",
        "            batch_perm = perm[start:end]\n",
        "            yield data[batch_perm]\n",
        "            start = end\n",
        "            end = start + batch_size"
      ],
      "metadata": {
        "id": "3Pk6JXCXetYH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@eqx.filter_jit\n",
        "def make_step(model, weight, int_beta, data, t1, key, opt_state, opt_update):\n",
        "    loss_fn = eqx.filter_value_and_grad(batch_loss_fn)\n",
        "    loss, grads = loss_fn(model, weight, int_beta, data, t1, key)\n",
        "    updates, opt_state = opt_update(grads, opt_state)\n",
        "    model = eqx.apply_updates(model, updates)\n",
        "    key = jr.split(key, 1)[0]\n",
        "    return loss, model, key, opt_state"
      ],
      "metadata": {
        "id": "vOOMS9d2fJu7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(\n",
        "    # Model hyperparameters\n",
        "    patch_size=4,\n",
        "    hidden_size=64,\n",
        "    mix_patch_size=512,\n",
        "    mix_hidden_size=512,\n",
        "    num_blocks=4,\n",
        "    t1=10.0,\n",
        "    # Optimisation hyperparameters\n",
        "    num_steps=1_000_000,\n",
        "    lr=3e-4,\n",
        "    batch_size=256,\n",
        "    print_every=10_000,\n",
        "    # Sampling hyperparameters\n",
        "    dt0=0.1,\n",
        "    #sample_size=4,\n",
        "    # Seed\n",
        "    seed=2023,\n",
        "):\n",
        "    key = jr.PRNGKey(seed)\n",
        "    model_key, train_key, loader_key, sample_key = jr.split(key, 4)\n",
        "    data = cifar()\n",
        "    data_mean = jnp.mean(data)\n",
        "    data_std = jnp.std(data)\n",
        "    data_max = jnp.max(data)\n",
        "    data_min = jnp.min(data)\n",
        "    data_shape = data.shape[1:]\n",
        "    data = (data - data_mean) / data_std\n",
        "\n",
        "    model = Mixer2d(\n",
        "        data_shape,\n",
        "        patch_size,\n",
        "        hidden_size,\n",
        "        mix_patch_size,\n",
        "        mix_hidden_size,\n",
        "        num_blocks,\n",
        "        t1,\n",
        "        key=model_key,\n",
        "    )\n",
        "    int_beta = lambda t: t  # Try experimenting with other options here!\n",
        "    weight = lambda t: 1 - jnp.exp(\n",
        "        -int_beta(t)\n",
        "    )  # Just chosen to upweight the region near t=0.\n",
        "\n",
        "    opt = optax.adabelief(lr)\n",
        "    # Optax will update the floating-point JAX arrays in the model.\n",
        "    opt_state = opt.init(eqx.filter(model, eqx.is_inexact_array))\n",
        "\n",
        "    total_value = 0\n",
        "    total_size = 0\n",
        "    for step, data in zip(\n",
        "        range(num_steps), dataloader(data, batch_size, key=loader_key)\n",
        "    ):\n",
        "        value, model, train_key, opt_state = make_step(\n",
        "            model, weight, int_beta, data, t1, train_key, opt_state, opt.update\n",
        "        )\n",
        "        total_value += value.item()\n",
        "        total_size += 1\n",
        "        if (step % print_every) == 0 or step == num_steps - 1:\n",
        "            print(f\"Step={step} Loss={total_value / total_size}\")\n",
        "            total_value = 0\n",
        "            total_size = 0\n",
        "\n",
        "    # sample_key = jr.split(sample_key, sample_size**2)\n",
        "    # sample_fn = ft.partial(single_sample_fn, model, int_beta, data_shape, dt0, t1)\n",
        "    # sample = jax.vmap(sample_fn)(sample_key)\n",
        "    # sample = data_mean + data_std * sample\n",
        "    # sample = jnp.clip(sample, data_min, data_max)\n",
        "    # sample = einops.rearrange(sample, \"(n1 n2) 3 h w -> (3 n1 h) (n2 w)\", n1=sample_size, n2=sample_size)\n",
        "    \n",
        "    \n",
        "    eqx.tree_serialise_leaves(\"cifar10_model.eqx\", model)\n",
        "    shutil.copy('/content/cifar10_model.eqx','/content/gdrive/MyDrive/Colab_Notebooks')\n",
        "\n",
        "\n",
        "    sample = single_sample_fn( model, int_beta, data_shape, dt0, t1, sample_key)\n",
        "    sample = data_mean + data_std * sample\n",
        "    sample = jnp.clip(sample, data_min, data_max)\n",
        "    sample = einops.rearrange(sample, \"c h w -> h w c\") # tranpose\n",
        "    sample = jnp.array(sample, dtype=int)\n",
        "    print(sample)\n",
        "    plt.imshow(sample)\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "aW92qzaJevuj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C7ysRfpxeymV",
        "outputId": "acc18c97-313b-4c6c-b4fa-08b98f5c293c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step=0 Loss=1.0178276300430298\n",
            "Step=1 Loss=1.0014079809188843\n",
            "[[[  0 255   0]\n",
            "  [  0   0   0]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [  0 255   0]\n",
            "  [255   0   0]\n",
            "  [  0   0   0]]\n",
            "\n",
            " [[255   0 255]\n",
            "  [  0 255   0]\n",
            "  [255   0 145]\n",
            "  ...\n",
            "  [  0   0   0]\n",
            "  [255   0 255]\n",
            "  [  0 255   0]]\n",
            "\n",
            " [[255   7   0]\n",
            "  [  0   0   0]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [  0   0   0]\n",
            "  [255 255   0]\n",
            "  [255 255   0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[  0   0   0]\n",
            "  [255 255 255]\n",
            "  [  0 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [  0 255 255]\n",
            "  [  0   0   0]]\n",
            "\n",
            " [[255   0 255]\n",
            "  [  0 255 255]\n",
            "  [  0 255 255]\n",
            "  ...\n",
            "  [  0   0   0]\n",
            "  [255 255   0]\n",
            "  [  0 255 255]]\n",
            "\n",
            " [[  0 255   0]\n",
            "  [255   0 255]\n",
            "  [  0 255   0]\n",
            "  ...\n",
            "  [255   0   0]\n",
            "  [  0   0   0]\n",
            "  [  0   0 255]]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAAEYCAYAAACHjumMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAMXUlEQVR4nO3dr8ptaxUG8LnkHFHQZhFEjGIRvASLQTAZtXgDZoPJYjB5AwbxBsRg8xI0aRQUbCaDyAnLsIth7/kMvvE+S5HfL541v/edf9Z+mLDGGePxfD4vgIZP/bdPAPj/JWCAGgED1AgYoEbAADUf3X34uB73PzE98gb5V6r7RQZbXNd1v0c8g8l1xAPSIoNf6yYncuMngy1+HE/hS/cHPP8a9zhxv/fyJum7mZ/o/kLSvRj9xhsOitcxuIy4xvP9q3iDAWoEDFAjYIAaAQPUCBigRsAANQIGqLmtg3mmMpjRD+g/D2uEH/G/94e4xfNXqZYm7DEqNtjt8TxQ/BEeR3xe7w5afTwTrnXyP/Bv79ZzVHe0PSDvkR9J+F6Nbtb+buVD3raHNxigRsAANQIGqBEwQI2AAWoEDFAjYIAaAQPUPG6b7nzmvkzo8a+8wbrh1IGmVo8DjYFi0eGJPULBU276k88hFh2e6AaVnvmBDl+5v9egCC6ex66R2XUNilFTw6kX1NmdmVyk4RTwYgIGqBEwQI2AAWoEDFAjYIAaAQPU3A9eC3Uuk6Y+qT4krTEZbrVtCxRrQ0aLbAs3BvcqLpGvI1/GgdqPI4PVdsUZs/qp7RqD4W5phSNFKMvpbZNGZW/cwxsMUCNggBoBA9QIGKBGwAA1AgaoETBAzW0dzKCAJJpUyiwXiLUyj/g7/4FeLrEnzWCN1Dok1aicaOUSPp/Ndksn8ufBmbxgmFhaITedGayS1tj3Q/pN+F58O17GoEYrPPgPfeoNBqgRMECNgAFqBAxQI2CAGgED1AgYoEbAADX3hXZx2FiWm1LtC41+sW6StC+YSkO8RgO0Dgz6ynukBmD3Jk3G4my36yt5iXWdXV7g++H7/ct0qb8bnMc3l890VAS3a+o2+3f8Nt5ggBoBA9QIGKBGwAA1AgaoETBAjYABah53jZIeBwoz4iHhN/zJYKrYcCr9/aCLUlwj1n4Mqg1S761XNLVa1sm8W6M/TO9EZ6z4zH4a7sWP4hZxk9zSav+9ecUcvOcHvlneYIAaAQPUCBigRsAANQIGqBEwQI2AAWpWdTCzHhGpzmW0yK11v5dB05lcr3C/x6/jDtf1nVykkk4iikPoTkxv2xYNXQfqp+IO+Tz21VEHevy84Lv5hcF1/F0dDPC/RsAANQIGqBEwQI2AAWoEDFAjYIAaAQPU3BfahTqgUY3csm5rVIi3HHoWi8+u63qGArTJgLhsO+nrm+stTgx3ywP79qO+ThTBpVXiCt/Kezx/e/95XOHng+/mD9MB+6LEWH/5gX8g3mCAGgED1AgYoEbAADUCBqgRMECNgAFqQsOp2J0o7xB+QE/NcEZNfQ6c5t6+G9S639Sk4VQ+5H6PwTFHaoLSzfhGqJP5fd7isWyudWRg3xfDAX8bnMi21mywwJ/Ck//qBybEeYMBagQMUCNggBoBA9QIGKBGwAA1Agao+ej+432NynqO16S4I4nFCIOTDIekep7ZILDVx9djUIByV/f0bpG0wOBK4onuB689BnUueY9t35rB/U4rhDqX0VczPNPvXp+73+NAd50PreANBqgRMECNgAFqBAxQI2CAGgED1AgYoEbAADW3DaeuA62aZkO2bk5gVAOXhnTtm0HlSrs43W2ww7rSLu6RxEK8SQOwdKKTWr39EtnyazH7bu4ark0GD8bz+Ho44A9nxu297z96gwFqBAxQI2CAGgED1AgYoEbAADUCBqgJg9fuf6SfDfraTRM70QznQN+swRLbAXP5RFJjoVzvc2JI3ZEKlGjbkmp0lqlu6MD3Zl/Qs29qlf+hji4kHaAOBngtAQPUCBigRsAANQIGqBEwQI2AAWpCP5hQNDFoVrHuB3OiVcUL2sEcaJ0zuJ2ppuhA85ATZS4vKLuIdUWj3ji7mqBZrc1qi+sxmlx4pJ9LEOu81MEAryVggBoBA9QIGKBGwAA1AgaoETBAjYABaj66+zAONJvUda2PGE23Ch+nRk2DLZa1TKM/jw2Q0vPo36tRA6RUn3mg0i7WTo6aoaUD9k3EUqFcPof+UMBZLd/b/gF4gwFqBAxQI2CAGgED1AgYoEbAADUCBqgJDafuf+if1DPseygdaKYT6xkObHGifiRca+yhNChoyLUb+6ZWsUnSkaZWB7pBjQpAbv78wFGPZe3TdQ16iG1n7V3XpEZLwyngtQQMUCNggBoBA9QIGKBGwAA1AgaoCf1gDvTDWK6xLFU45kgtQdrjbaUG/7HAgRqVEz2ADtQ2PcP35sgcvGWTn9mfL+/3pMVPOo8vh0X+MtnjbT2VvMEANQIGqBEwQI2AAWoEDFAjYIAaAQPUCBig5rbh1GM9biwX0h3pPZQOCBVsszFg/esYnMS9wUnEGrcDDZAO9MVaF2jOmqHt1pgUmqa7kYcbvmBI3WCHwb3QcAp4LQED1AgYoEbAADUCBqgRMECNgAFqbhtOpR/QZz19dkO40jCyd2ssm/qMCkjCx/Ey93vksqS8R7xTcZ7Zvi5j8sXJTatCXcaoUdNyIF/83mX5u3dmLOD9x5MGYG87wBsMUCNggBoBA9QIGKBGwAA1AgaoETBAjYABam4L7WJDnlGlXb9x0KCLUl4jiddxb1IvmG7oujDsyg2O4nWOCh/XB8Qb9oMThY3Jvt9afCgnmnPFVT49WSPtYLIj8D9GwAA1AgaoETBAjYABagQMUCNggJrbwWvpx+8jTZTS7+svmWg2aNS0bDh1YEZXrA15HGmAtKvFGa0xOZFls7NJCUts9hQ/PtCo6UTDqc+G4W7/TFsMriOfpsFrwGsJGKBGwAA1AgaoETBAjYABagQMUHNfB7PuZnFdj20ByZF2MP1+MY9wEifKYI40D4nPY3kOA88D55l74+wH9uXSj32/mDyYcF9rc2KP9N16fuAAbzBAjYABagQMUCNggBoBA9QIGKBGwAA1AgaouR28lvsXDYqAlp2YZsPdBscsbWdwzXpBLYfUjYoSt02WJuIYuv0SR4a7pY8PFNKtG0oduI4TA+TeeBneYIAaAQPUCBigRsAANQIGqBEwQI2AAWpu62ByncCkcdDugEktQi7XCXtM6gSWg8AmzaBSc65tr6h3e6QjthPm8iYfD870k7xLMPjepJtx4nsTb+f2i3Wt/42NBsi9sZTGGwxQI2CAGgED1AgYoEbAADUCBqgRMEDNbR3Mid/oc+3GvldFPM/UR2W2yf0aB8oZ8hC6VCdzYEhXqks6UPsxu+O7njJx2Ni7g8IOaY0D1xEfyOcHe/zjfonJvUinke6VfjDAqwkYoEbAADUCBqgRMECNgAFqBAxQI2CAmsfzrjhr0okmCSu8YH7WoCHVYI9Yy7cdsJXlIV6vMCnm209vi/V86yK5wdf7wEy03FBtv0UcyLce/jYaoPjeA7zBADUCBqgRMECNgAFqBAxQI2CAGgED1Nw2nMq1CIMd1vUjky1STUT6+xMNecIBg+tcN98aFU2kwqQTg8BONIMa7HOrX68z6b21LQk60agsNQmbPI+3/hvzBgPUCBigRsAANQIGqBEwQI2AAWoEDFBz2w/m8YJ+MNGoH8yyaGIwsCzWueQVtgsMZngNriNtEWs/JvU8B2xrUA7UBOVLPdBzZtkvZnYWe7d9o25OwxsMUCNggBoBA9QIGKBGwAA1AgaoETBAjYABam4bTg3qz7LYcCcUGg3O4SWFRmGXRyrampRMLS9kUgz4vD4Jp/Dx/d8PuiydeB7puceCwCOFjeGZj57ptoHXC2pdD/T/0nAKeDkBA9QIGKBGwAA1AgaoETBAjYABau4Hr4Xf8AdNaGbDqW7PYT9AK9eoTCwHlo222O4xuJJnqnNJtR8DJ5pBLes/Rs234nmE781kgFy8Fz/b/f11rae3zQbIpXqe9y/iDQaoETBAjYABagQMUCNggBoBA9QIGKDmvh9M+AH9SN+P2NcjS7/jrwezDc4j3qtJzUSyHhR2Xeun9rJJYLteLJNnnlux7AtIlqU2kxXiIrmeLe+Qask+xBsMUCNggBoBA9QIGKBGwAA1AgaoETBAjYABah53TaMeoZJoNIQrVH/FNQbVY7EpVazme0FXn4FYHJbqvl5xDsNV7kyGom2vZHIv8rWm6/jj4ES+lo+5PYXJVLRd0eHsiafmW+9fxhsMUCNggBoBA9QIGKBGwAA1AgaoETBAzW0dDMCGNxigRsAANQIGqBEwQI2AAWoEDFDzby2AXCxW3WRGAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}