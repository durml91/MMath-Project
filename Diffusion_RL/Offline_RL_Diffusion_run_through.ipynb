{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIjXZDa0hjWXOb1IIDDN3+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durml91/MMath-Project/blob/duo-branch/Diffusion_RL/Offline_RL_Diffusion_run_through.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non exectutable run through of the offline RL + diffusion paper which can be found here: https://github.com/Zhendong-Wang/Diffusion-Policies-for-Offline-RL\n",
        "\n"
      ],
      "metadata": {
        "id": "hJ_UMOXkcN04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility functions"
      ],
      "metadata": {
        "id": "N2qRLGsGkmsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#typical positional embedding\n",
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "#-----------------------------------------------------------------------------#\n",
        "#---------------------------------- sampling ---------------------------------#\n",
        "#-----------------------------------------------------------------------------#\n",
        "\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    b, *_ = t.shape\n",
        "    out = a.gather(-1, t)\n",
        "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
        "\n",
        "#this gives a new calculation function for the betas\n",
        "def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):\n",
        "    \"\"\"\n",
        "    cosine schedule\n",
        "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    x = np.linspace(0, steps, steps)\n",
        "    alphas_cumprod = np.cos(((x / steps) + s) / (1 + s) * np.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    betas_clipped = np.clip(betas, a_min=0, a_max=0.999)\n",
        "    return torch.tensor(betas_clipped, dtype=dtype)\n",
        "\n",
        "\n",
        "#normal beta calc\n",
        "def linear_beta_schedule(timesteps, beta_start=1e-4, beta_end=2e-2, dtype=torch.float32):\n",
        "    betas = np.linspace(\n",
        "        beta_start, beta_end, timesteps\n",
        "    )\n",
        "    return torch.tensor(betas, dtype=dtype)\n",
        "\n",
        "\n",
        "#variance preserving - this is the weird beta_min beta_max stuff\n",
        "def vp_beta_schedule(timesteps, dtype=torch.float32):\n",
        "    t = np.arange(1, timesteps + 1)\n",
        "    T = timesteps\n",
        "    b_max = 10.\n",
        "    b_min = 0.1\n",
        "    alpha = np.exp(-b_min / T - 0.5 * (b_max - b_min) * (2 * t - 1) / T ** 2)\n",
        "    betas = 1 - alpha\n",
        "    return torch.tensor(betas, dtype=dtype)\n",
        "\n",
        "#-----------------------------------------------------------------------------#\n",
        "#---------------------------------- losses -----------------------------------#\n",
        "#-----------------------------------------------------------------------------#\n",
        "\n",
        "\n",
        "class WeightedLoss(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, pred, targ, weights=1.0):\n",
        "        '''\n",
        "            pred, targ : tensor [ batch_size x action_dim ]\n",
        "        '''\n",
        "        loss = self._loss(pred, targ)\n",
        "        weighted_loss = (loss * weights).mean()\n",
        "        return weighted_loss\n",
        "\n",
        "class WeightedL1(WeightedLoss):\n",
        "\n",
        "    def _loss(self, pred, targ):\n",
        "        return torch.abs(pred - targ)\n",
        "\n",
        "class WeightedL2(WeightedLoss):\n",
        "\n",
        "    def _loss(self, pred, targ):\n",
        "        return F.mse_loss(pred, targ, reduction='none')\n",
        "\n",
        "\n",
        "Losses = {\n",
        "    'l1': WeightedL1,\n",
        "    'l2': WeightedL2,\n",
        "}\n",
        "\n",
        "\n",
        "class EMA():\n",
        "    '''\n",
        "        empirical moving average\n",
        "    '''\n",
        "    def __init__(self, beta):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "\n",
        "    def update_model_average(self, ma_model, current_model):\n",
        "        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
        "            old_weight, up_weight = ma_params.data, current_params.data\n",
        "            ma_params.data = self.update_average(old_weight, up_weight)\n",
        "\n",
        "    def update_average(self, old, new):\n",
        "        if old is None:\n",
        "            return new\n",
        "        return old * self.beta + (1 - self.beta) * "
      ],
      "metadata": {
        "id": "eLwv57UJklQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main stuff"
      ],
      "metadata": {
        "id": "pdGUiQMpcYh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we proceed to go through the main parts of the agent starting with the diffusion policy below."
      ],
      "metadata": {
        "id": "KX4kFVitcap8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The policy is technically a BC policy that essentially infers the policy from a given dataset. This is DDPMs diffusion and is all pretty straightforward to that end. Given python adopts the oop paradigm, *args and **kwargs is common when defining classes; really good article here: https://realpython.com/python-kwargs-and-args/ - quick summary are that they allow for general unpacking of iterable objects, specifically * on any iterable and ** specifically dictionaries - these are arguments of functions that allow you to be more \"general\" with how you define your inputs - this basically all to do about unpacking, and allows you to use python functions with more fluidity (rather than having to specifically define a function with a list for example, that means every time you call the function, you must specifically have to input that object, although you may want to use that function more generally for all kinds of objects) - good thing to adopt!"
      ],
      "metadata": {
        "id": "2P-xry4UchKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from agents.helpers import (cosine_beta_schedule,\n",
        "                            linear_beta_schedule,\n",
        "                            vp_beta_schedule,\n",
        "                            extract,\n",
        "                            Losses)\n",
        "from utils.utils import Progress, Silent\n",
        "\n",
        "\n",
        "\n",
        "class Diffusion(nn.Module):     \n",
        "    def __init__(self, state_dim, action_dim, model, max_action, beta_schedule='linear', n_timesteps=100, loss_type='l2', clip_denoised=True, predict_epsilon=True):\n",
        "        super(Diffusion, self).__init__() #for some reason we are accessing a parent class for diffusion - not sure why\n",
        "\n",
        "\n",
        "        #these are all the input parameters, that are come from the D4RL dataset - i.e. this is the data\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.max_action = max_action\n",
        "        self.model = model\n",
        "\n",
        "        #there are three beta functions here - not really that important\n",
        "        if beta_schedule == 'linear':\n",
        "            betas = linear_beta_schedule(n_timesteps)\n",
        "        elif beta_schedule == 'cosine':\n",
        "            betas = cosine_beta_schedule(n_timesteps)\n",
        "        elif beta_schedule == 'vp':\n",
        "            betas = vp_beta_schedule(n_timesteps)\n",
        "\n",
        "        \n",
        "        #work out your parameters for the Gaussian transition kernel.\n",
        "        alphas = 1. - betas\n",
        "        alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "        alphas_cumprod_prev = torch.cat([torch.ones(1), alphas_cumprod[:-1]])\n",
        "\n",
        "        \n",
        "        #again we intiliase some inputs like the number of timesteps - don't really know what the other two are really doing?\n",
        "        self.n_timesteps = int(n_timesteps)\n",
        "        self.clip_denoised = clip_denoised\n",
        "        self.predict_epsilon = predict_epsilon\n",
        "\n",
        "\n",
        "        #we register the parameters we calculated above\n",
        "        self.register_buffer('betas', betas)\n",
        "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
        "        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
        "\n",
        "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
        "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n",
        "        self.register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n",
        "        self.register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n",
        "        self.register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n",
        "\n",
        "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
        "        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
        "        self.register_buffer('posterior_variance', posterior_variance)\n",
        "\n",
        "        ## log calculation clipped because the posterior variance\n",
        "        ## is 0 at the beginning of the diffusion chain\n",
        "        self.register_buffer('posterior_log_variance_clipped', # clamp is equivalent to jnp.clip in JAX\n",
        "                             torch.log(torch.clamp(posterior_variance, min=1e-20)))\n",
        "        self.register_buffer('posterior_mean_coef1',\n",
        "                             betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n",
        "        self.register_buffer('posterior_mean_coef2',\n",
        "                             (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod))\n",
        "\n",
        "        self.loss_fn = Losses[loss_type]()\n",
        "\n",
        "    # ------------------------------------------ sampling ------------------------------------------#\n",
        "\n",
        "    def predict_start_from_noise(self, x_t, t, noise):\n",
        "        '''\n",
        "            if self.predict_epsilon, model output is (scaled) noise;\n",
        "            otherwise, model predicts x0 directly\n",
        "        '''\n",
        "        if self.predict_epsilon:   #again this is simply the same stuff from the blog\n",
        "            return (\n",
        "                    extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
        "                    extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
        "            )\n",
        "        else:\n",
        "            return noise\n",
        "\n",
        "    def q_posterior(self, x_start, x_t, t):\n",
        "        posterior_mean = (\n",
        "                extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
        "                extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
        "        )\n",
        "        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n",
        "        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
        "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
        "\n",
        "    def p_mean_variance(self, x, t, s):\n",
        "        x_recon = self.predict_start_from_noise(x, t=t, noise=self.model(x, t, s))\n",
        "\n",
        "        if self.clip_denoised:\n",
        "            x_recon.clamp_(-self.max_action, self.max_action)\n",
        "        else:\n",
        "            assert RuntimeError()\n",
        "\n",
        "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n",
        "        return model_mean, posterior_variance, posterior_log_variance\n",
        "\n",
        "    # @torch.no_grad()\n",
        "    def p_sample(self, x, t, s):\n",
        "        b, *_, device = *x.shape, x.device\n",
        "        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, s=s)\n",
        "        noise = torch.randn_like(x)\n",
        "        # no noise when t == 0\n",
        "        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n",
        "        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
        "\n",
        "    # @torch.no_grad()\n",
        "    def p_sample_loop(self, state, shape, verbose=False, return_diffusion=False):\n",
        "        device = self.betas.device\n",
        "\n",
        "        batch_size = shape[0]\n",
        "        x = torch.randn(shape, device=device)\n",
        "\n",
        "        if return_diffusion: diffusion = [x]\n",
        "\n",
        "        progress = Progress(self.n_timesteps) if verbose else Silent()\n",
        "        for i in reversed(range(0, self.n_timesteps)):\n",
        "            timesteps = torch.full((batch_size,), i, device=device, dtype=torch.long)\n",
        "            x = self.p_sample(x, timesteps, state)\n",
        "\n",
        "            progress.update({'t': i})\n",
        "\n",
        "            if return_diffusion: diffusion.append(x)\n",
        "\n",
        "        progress.close()\n",
        "\n",
        "        if return_diffusion:\n",
        "            return x, torch.stack(diffusion, dim=1)\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    # @torch.no_grad()\n",
        "    def sample(self, state, *args, **kwargs):\n",
        "        batch_size = state.shape[0]\n",
        "        shape = (batch_size, self.action_dim)\n",
        "        action = self.p_sample_loop(state, shape, *args, **kwargs)\n",
        "        return action.clamp_(-self.max_action, self.max_action)\n",
        "\n",
        "    # ------------------------------------------ training ------------------------------------------#\n",
        "\n",
        "    #this is the getting that reparameterised value in the loss i.e. predicting the noise in the loss function\n",
        "    #we can simply get\n",
        "    def q_sample(self, x_start, t, noise=None):\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x_start)\n",
        "\n",
        "        sample = (\n",
        "                extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
        "                extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
        "        )\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "    #\n",
        "    def p_losses(self, x_start, state, t, weights=1.0):\n",
        "        noise = torch.randn_like(x_start) #sample from normal distribution\n",
        "\n",
        "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)  #sample from the above function\n",
        "\n",
        "        x_recon = self.model(x_noisy, t, state)  #plug that into the neural net\n",
        "\n",
        "        assert noise.shape == x_recon.shape\n",
        "\n",
        "        if self.predict_epsilon: #this is the lil blog loss - makes sense - we arranged the loss so the NN tries to predict the noise\n",
        "            loss = self.loss_fn(x_recon, noise, weights)\n",
        "        else:\n",
        "            loss = self.loss_fn(x_recon, x_start, weights) #not sure what bit this is\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def loss(self, x, state, weights=1.0):  #this is taking the expectation over time - we get a random vector of times\n",
        "        batch_size = len(x)\n",
        "        t = torch.randint(0, self.n_timesteps, (batch_size,), device=x.device).long()\n",
        "        return self.p_losses(x, state, t, weights)\n",
        "\n",
        "    def forward(self, state, *args, **kwargs):\n",
        "        return self.sample(state, *args, **kwargs)"
      ],
      "metadata": {
        "id": "o-4u35JCcuVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP"
      ],
      "metadata": {
        "id": "EnRQUj1Cr3eU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP Model\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 state_dim,\n",
        "                 action_dim,\n",
        "                 device,\n",
        "                 t_dim=16):\n",
        "\n",
        "        super(MLP, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPosEmb(t_dim),\n",
        "            nn.Linear(t_dim, t_dim * 2),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(t_dim * 2, t_dim),\n",
        "        )\n",
        "\n",
        "        input_dim = state_dim + action_dim + t_dim #important to note that both state and action are inputs\n",
        "        self.mid_layer = nn.Sequential(nn.Linear(input_dim, 256),\n",
        "                                       nn.Mish(),\n",
        "                                       nn.Linear(256, 256),\n",
        "                                       nn.Mish(),\n",
        "                                       nn.Linear(256, 256),\n",
        "                                       nn.Mish())\n",
        "\n",
        "        self.final_layer = nn.Linear(256, action_dim)\n",
        "\n",
        "    def forward(self, x, time, state):\n",
        "\n",
        "        t = self.time_mlp(time)\n",
        "        x = torch.cat([x, t, state], dim=1)\n",
        "        x = self.mid_layer(x)\n",
        "\n",
        "        return self.final_layer(x)"
      ],
      "metadata": {
        "id": "XLgl6O6Qr2Ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent"
      ],
      "metadata": {
        "id": "WaT9hiD6thBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from agents.diffusion import Diffusion\n",
        "from agents.model import MLP\n",
        "from agents.helpers import EMA\n",
        "\n",
        "#critic stuff here is just from the fujimoto stuff\n",
        "class Critic(nn.Module): #recall that we have two critics (Q value estimate) and two actors (the policy estimate - so the diffusion model)\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
        "        super(Critic, self).__init__()\n",
        "        self.q1_model = nn.Sequential(nn.Linear(state_dim + action_dim, hidden_dim),\n",
        "                                      nn.Mish(),\n",
        "                                      nn.Linear(hidden_dim, hidden_dim),\n",
        "                                      nn.Mish(),\n",
        "                                      nn.Linear(hidden_dim, hidden_dim),\n",
        "                                      nn.Mish(),\n",
        "                                      nn.Linear(hidden_dim, 1))\n",
        "\n",
        "        self.q2_model = nn.Sequential(nn.Linear(state_dim + action_dim, hidden_dim),\n",
        "                                      nn.Mish(),\n",
        "                                      nn.Linear(hidden_dim, hidden_dim),\n",
        "                                      nn.Mish(),\n",
        "                                      nn.Linear(hidden_dim, hidden_dim),\n",
        "                                      nn.Mish(),\n",
        "                                      nn.Linear(hidden_dim, 1))\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], dim=-1)\n",
        "        return self.q1_model(x), self.q2_model(x) #compute the q value for both critics\n",
        "\n",
        "    def q1(self, state, action):\n",
        "        x = torch.cat([state, action], dim=-1)\n",
        "        return self.q1_model(x)   \n",
        "\n",
        "    def q_min(self, state, action):\n",
        "        q1, q2 = self.forward(state, action) #here we compute the min - this is from the fujimoto paper\n",
        "        return torch.min(q1, q2)\n",
        "\n",
        "\n",
        "class Diffusion_QL(object):\n",
        "    def __init__(self,\n",
        "                 state_dim,\n",
        "                 action_dim,\n",
        "                 max_action,\n",
        "                 device,\n",
        "                 discount,\n",
        "                 tau,\n",
        "                 max_q_backup=False,\n",
        "                 eta=1.0,\n",
        "                 beta_schedule='linear',\n",
        "                 n_timesteps=100,\n",
        "                 ema_decay=0.995,\n",
        "                 step_start_ema=1000,\n",
        "                 update_ema_every=5,\n",
        "                 lr=3e-4,\n",
        "                 lr_decay=False,\n",
        "                 lr_maxt=1000,\n",
        "                 grad_norm=1.0,\n",
        "                 ):\n",
        "\n",
        "        self.model = MLP(state_dim=state_dim, action_dim=action_dim, device=device) #this is the main diffusion MLP\n",
        "\n",
        "        self.actor = Diffusion(state_dim=state_dim, action_dim=action_dim, model=self.model, max_action=max_action,\n",
        "                               beta_schedule=beta_schedule, n_timesteps=n_timesteps,).to(device) #we define the actor\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr) #initiliase the optimiser\n",
        "\n",
        "        self.lr_decay = lr_decay\n",
        "        self.grad_norm = grad_norm\n",
        "\n",
        "\n",
        "        #target network stuff\n",
        "        self.step = 0\n",
        "        self.step_start_ema = step_start_ema\n",
        "        self.ema = EMA(ema_decay)\n",
        "        self.ema_model = copy.deepcopy(self.actor)\n",
        "        self.update_ema_every = update_ema_every\n",
        "\n",
        "        self.critic = Critic(state_dim, action_dim).to(device) #define the critic network\n",
        "        self.critic_target = copy.deepcopy(self.critic) #define the target Q/critic network - identical the critic\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
        "\n",
        "        if lr_decay:\n",
        "            self.actor_lr_scheduler = CosineAnnealingLR(self.actor_optimizer, T_max=lr_maxt, eta_min=0.)\n",
        "            self.critic_lr_scheduler = CosineAnnealingLR(self.critic_optimizer, T_max=lr_maxt, eta_min=0.)\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        self.max_action = max_action\n",
        "        self.action_dim = action_dim\n",
        "        self.discount = discount\n",
        "        self.tau = tau\n",
        "        self.eta = eta  # q_learning weight\n",
        "        self.device = device\n",
        "        self.max_q_backup = max_q_backup\n",
        "\n",
        "    def step_ema(self):\n",
        "        if self.step < self.step_start_ema:\n",
        "            return\n",
        "        self.ema.update_model_average(self.ema_model, self.actor)\n",
        "\n",
        "    def train(self, replay_buffer, iterations, batch_size=100, log_writer=None):\n",
        "\n",
        "        metric = {'bc_loss': [], 'ql_loss': [], 'actor_loss': [], 'critic_loss': []}\n",
        "        for _ in range(iterations):\n",
        "            # Sample replay buffer / batch\n",
        "            state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
        "\n",
        "            \"\"\" Q Training \"\"\"\n",
        "            current_q1, current_q2 = self.critic(state, action)\n",
        "\n",
        "            if self.max_q_backup:\n",
        "                next_state_rpt = torch.repeat_interleave(next_state, repeats=10, dim=0)\n",
        "                next_action_rpt = self.ema_model(next_state_rpt)\n",
        "                target_q1, target_q2 = self.critic_target(next_state_rpt, next_action_rpt)\n",
        "                target_q1 = target_q1.view(batch_size, 10).max(dim=1, keepdim=True)[0]\n",
        "                target_q2 = target_q2.view(batch_size, 10).max(dim=1, keepdim=True)[0]\n",
        "                target_q = torch.min(target_q1, target_q2)\n",
        "            else:\n",
        "                next_action = self.ema_model(next_state)\n",
        "                target_q1, target_q2 = self.critic_target(next_state, next_action)\n",
        "                target_q = torch.min(target_q1, target_q2)\n",
        "\n",
        "            target_q = (reward + not_done * self.discount * target_q).detach()\n",
        "\n",
        "            critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
        "\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            if self.grad_norm > 0:\n",
        "                critic_grad_norms = nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=self.grad_norm, norm_type=2)\n",
        "            self.critic_optimizer.step()\n",
        "\n",
        "            \"\"\" Policy Training \"\"\"\n",
        "            bc_loss = self.actor.loss(action, state)\n",
        "            new_action = self.actor(state)\n",
        "\n",
        "            q1_new_action, q2_new_action = self.critic(state, new_action)\n",
        "            if np.random.uniform() > 0.5:\n",
        "                q_loss = - q1_new_action.mean() / q2_new_action.abs().mean().detach()\n",
        "            else:\n",
        "                q_loss = - q2_new_action.mean() / q1_new_action.abs().mean().detach()\n",
        "            actor_loss = bc_loss + self.eta * q_loss\n",
        "\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            if self.grad_norm > 0: \n",
        "                actor_grad_norms = nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=self.grad_norm, norm_type=2)\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "\n",
        "            \"\"\" Step Target network \"\"\"\n",
        "            if self.step % self.update_ema_every == 0:\n",
        "                self.step_ema()\n",
        "\n",
        "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "            self.step += 1\n",
        "\n",
        "            \"\"\" Log \"\"\"\n",
        "            if log_writer is not None:\n",
        "                if self.grad_norm > 0:\n",
        "                    log_writer.add_scalar('Actor Grad Norm', actor_grad_norms.max().item(), self.step)\n",
        "                    log_writer.add_scalar('Critic Grad Norm', critic_grad_norms.max().item(), self.step)\n",
        "                log_writer.add_scalar('BC Loss', bc_loss.item(), self.step)\n",
        "                log_writer.add_scalar('QL Loss', q_loss.item(), self.step)\n",
        "                log_writer.add_scalar('Critic Loss', critic_loss.item(), self.step)\n",
        "                log_writer.add_scalar('Target_Q Mean', target_q.mean().item(), self.step)\n",
        "\n",
        "            metric['actor_loss'].append(actor_loss.item())\n",
        "            metric['bc_loss'].append(bc_loss.item())\n",
        "            metric['ql_loss'].append(q_loss.item())\n",
        "            metric['critic_loss'].append(critic_loss.item())\n",
        "\n",
        "        if self.lr_decay: \n",
        "            self.actor_lr_scheduler.step()\n",
        "            self.critic_lr_scheduler.step()\n",
        "\n",
        "        return metric\n",
        "\n",
        "    def sample_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
        "        state_rpt = torch.repeat_interleave(state, repeats=50, dim=0)\n",
        "        with torch.no_grad():\n",
        "            action = self.actor.sample(state_rpt)\n",
        "            q_value = self.critic_target.q_min(state_rpt, action).flatten()\n",
        "            idx = torch.multinomial(F.softmax(q_value), 1)\n",
        "        return action[idx].cpu().data.numpy().flatten()\n",
        "\n",
        "    def save_model(self, dir, id=None):\n",
        "        if id is not None:\n",
        "            torch.save(self.actor.state_dict(), f'{dir}/actor_{id}.pth')\n",
        "            torch.save(self.critic.state_dict(), f'{dir}/critic_{id}.pth')\n",
        "        else:\n",
        "            torch.save(self.actor.state_dict(), f'{dir}/actor.pth')\n",
        "            torch.save(self.critic.state_dict(), f'{dir}/critic.pth')\n",
        "\n",
        "    def load_model(self, dir, id=None):\n",
        "        if id is not None:\n",
        "            self.actor.load_state_dict(torch.load(f'{dir}/actor_{id}.pth'))\n",
        "            self.critic.load_state_dict(torch.load(f'{dir}/critic_{id}.pth'))\n",
        "        else:\n",
        "            self.actor.load_state_dict(torch.load(f'{dir}/actor.pth'))\n",
        "            self.critic.load_state_dict(torch.load(f'{dir}/critic.pth'))"
      ],
      "metadata": {
        "id": "ABljXqgytf-h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}